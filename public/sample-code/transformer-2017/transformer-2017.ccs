{
  "id": "transformer-2017-sample",
  "name": "Transformer Architecture (2017)",
  "mode": "critique",
  "description": "Attention Is All You Need - multi-head attention and the foundation of modern LLMs",
  "era": "2010s",
  "codeFiles": [
    {
      "id": "file-1",
      "name": "README.md",
      "language": "markdown"
    },
    {
      "id": "file-2",
      "name": "annotated_transformer.py",
      "language": "python"
    },
    {
      "id": "file-3",
      "name": "transformer_pytorch.py",
      "language": "python"
    },
    {
      "id": "file-4",
      "name": "attention_pytorch.py",
      "language": "python"
    },
    {
      "id": "file-5",
      "name": "transformer_tensorflow.py",
      "language": "python"
    },
    {
      "id": "file-6",
      "name": "attention_tensorflow.py",
      "language": "python"
    }
  ],
  "codeContents": {
    "file-1": "# Transformer Architecture (2017)\n\n## Historical Context\n\nThe Transformer architecture was introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), a paper that fundamentally transformed natural language processing and artificial intelligence. Published by researchers at Google Brain and Google Research, it replaced recurrent neural networks (RNNs) and long short-term memory (LSTMs) with a novel attention-based architecture.\n\nThe paper's title was provocative and prescient: attention mechanisms alone, without recurrence or convolution, could achieve state-of-the-art results. This architectural innovation enabled:\n\n- **Parallel processing** of sequences (unlike sequential RNNs)\n- **Long-range dependencies** without vanishing gradients\n- **Scalability** to massive datasets and model sizes\n- **Transfer learning** across tasks and domains\n\nThe Transformer became the foundation for:\n- **GPT series** (2018-present): Generative Pre-trained Transformers\n- **BERT** (2018): Bidirectional Encoder Representations from Transformers\n- **Modern LLMs**: Claude, ChatGPT, LLaMA, and countless others\n\n## Technical Significance\n\n### Core Innovation: Self-Attention\n\nThe Transformer's key innovation is the **multi-head self-attention mechanism**, which allows the model to weigh the importance of different words in a sequence when processing each word. This mechanism computes three learned representations for each word:\n\n- **Query (Q)**: What am I looking for?\n- **Key (K)**: What do I offer?\n- **Value (V)**: What do I actually contain?\n\nAttention is computed as: `Attention(Q, K, V) = softmax(QK^T / √d_k)V`\n\nThis mathematical formula encodes:\n- Which words attend to which other words\n- How much weight to assign each relationship\n- What information to propagate through the network\n\n### Architecture Components\n\n**Encoder-Decoder Structure:**\n- **Encoder**: Processes input sequences (e.g., source language in translation)\n- **Decoder**: Generates output sequences (e.g., target language)\n- Both use stacked layers of attention and feed-forward networks\n\n**Key Mechanisms:**\n- **Multi-head attention**: Parallel attention operations with different learned weights\n- **Positional encoding**: Sinusoidal functions that encode word position\n- **Residual connections**: Skip connections around each sublayer\n- **Layer normalization**: Stabilizes training\n\n### Computational Infrastructure\n\nTransformers require massive computational resources:\n- **Training**: Hundreds of GPUs/TPUs running for weeks\n- **Data**: Billions of tokens from web scraping, books, code repositories\n- **Parameters**: Millions to trillions of learned weights\n- **Energy**: Significant carbon footprint for training\n\n## Critical Code Studies Value\n\nThis code rewards analysis through multiple lenses:\n\n**Materiality of Computation**: The Transformer makes visible the infrastructure of modern AI—GPUs, distributed systems, energy consumption, data centers. \"Attention\" is not a metaphor; it's matrix multiplication at industrial scale.\n\n**Mathematics Becoming Code**: The architecture translates differential calculus, linear algebra, and information theory into executable Python/TensorFlow/PyTorch. How do mathematical abstractions become computational operations? What's lost or gained in translation?\n\n**Opacity and Interpretability**: Despite \"attention\" mechanisms that seem explainable, large Transformers remain black boxes. We can visualize attention weights, but cannot fully explain why the model makes specific predictions. What does \"understanding\" mean for neural networks?\n\n**Labor and Automation**: Transformers automate language work—translation, summarization, writing, coding. What labor is displaced? What new forms of labor emerge (data annotation, prompt engineering, model alignment)? Who benefits from automation?\n\n**Scale and Inequality**: Training large Transformers requires resources available only to major tech companies and well-funded research labs. This creates barriers to entry and concentrates AI capabilities in corporate hands.\n\n**Data and Extraction**: Transformers are trained on scraped web data, books, code repositories, and other digital artifacts often taken without explicit permission or compensation. Whose labor and creativity is extracted to train these models?\n\n**Environmental Impact**: Training large models consumes enormous energy (comparable to the lifetime emissions of multiple cars). The environmental costs of AI are often invisible in the code.\n\n**Epistemology of Patterns**: Transformers learn statistical patterns in text without semantic understanding. They can generate fluent text that lacks grounding in reality. What does it mean to \"know\" something as a pattern in high-dimensional space?\n\n## About the Creators\n\n**Lead Authors:**\n- **Ashish Vaswani** (Google Brain): First author, led architecture design\n- **Noam Shazeer** (Google Brain): Co-architecture design, later founded Character.AI\n- **Niki Parmar** (Google Brain): Implementation and experiments\n- **Jakob Uszkoreit** (Google Research): Theory and analysis\n- **Llion Jones** (Google Research): Engineering and optimization\n- **Aidan N. Gomez** (University of Toronto/Google Brain): Student contributor, later founded Cohere\n- **Łukasz Kaiser** (Google Brain): Senior researcher, optimization\n- **Illia Polosukhin** (Google Research): Implementation, later founded NEAR Protocol\n\nThe paper emerged from Google's well-resourced research environment, with access to TPUs, massive datasets, and experienced ML researchers. The authors came from diverse backgrounds but shared access to corporate infrastructure unavailable to most researchers.\n\n## Source\n\n- **Paper**: \"Attention Is All You Need\" (Vaswani et al., NeurIPS 2017)\n- **arXiv**: https://arxiv.org/abs/1706.03762\n- **Language**: Python (TensorFlow, PyTorch)\n- **License**: Apache 2.0 (open source, but trained models often proprietary)\n- **Lines of Code**: ~2,000-5,000 (core architecture, excluding training infrastructure)\n\n## Key Files Included\n\n**Important**: This is a curated sample containing multiple implementations of the Transformer architecture for comparative Critical Code Studies analysis. Each implementation shows the same mathematical architecture expressed in different frameworks and coding styles.\n\n**Download original implementations:**\n- **Tensor2Tensor (TensorFlow)**: https://github.com/tensorflow/tensor2tensor\n- **PyTorch Official**: https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.py\n- **Annotated Transformer**: http://nlp.seas.harvard.edu/annotated-transformer/\n\nThis sample includes:\n\n### Documentation\n- **README.md**: This file - historical context and CCS analysis\n\n### Harvard NLP Annotated Transformer (PyTorch - Educational)\n- **annotated_transformer.py**: Complete implementation with extensive comments\n  - Pedagogical version designed for learning\n  - Line-by-line explanation of each component\n  - Includes training loop and example usage\n\n### TensorFlow Implementation (Original Framework)\n- **transformer_tensorflow.py**: Transformer in TensorFlow (Google's original framework)\n- **attention_tensorflow.py**: Multi-head attention mechanism (TF)\n  - Shows how Google's original team likely implemented it\n  - TensorFlow's computational graph approach\n\n### PyTorch Implementation (Modern Standard)\n- **transformer_pytorch.py**: PyTorch nn.Transformer module\n- **attention_pytorch.py**: PyTorch MultiheadAttention\n  - Industry standard for research\n  - Imperative programming style vs TensorFlow's graphs\n\n### Component Breakdown (Minimal Educational Versions)\n- **multi_head_attention.py**: Core attention mechanism isolated and explained\n- **positional_encoding.py**: Sinusoidal position embeddings\n- **encoder_layer.py**: Single encoder block (attention + feed-forward)\n- **decoder_layer.py**: Single decoder block (masked attention + cross-attention + feed-forward)\n- **full_model.py**: Complete minimal Transformer for teaching\n\n## Suggested Annotations\n\nWhen analyzing this code, consider:\n\n### Mathematics and Computation\n\n1. **Attention formula**: How does `softmax(QK^T / √d_k)V` translate to code? What operations are expensive? Where do GPUs help?\n2. **Matrix dimensions**: Trace tensor shapes through the network. How do dimensions encode sequence length, batch size, model width?\n3. **Softmax operation**: What does softmax do conceptually vs. computationally? Why is numerical stability important?\n4. **Scaling factor**: Why divide by √d_k? What happens without it?\n5. **Multi-head attention**: Why run multiple parallel attention operations? How do heads specialize?\n\n### Architecture and Design\n\n6. **Encoder-decoder split**: Why separate encoding and decoding? What tasks need both vs. just one?\n7. **Layer stacking**: Why stack 6-12 identical layers? What does depth accomplish?\n8. **Residual connections**: Why add input to output? How do skip connections help training?\n9. **Layer normalization**: Where and why normalize? What would happen without it?\n10. **Positional encoding**: Why use sinusoids vs. learned positions? What patterns do they encode?\n\n### Implementation Differences\n\n11. **TensorFlow vs PyTorch**: How do the two frameworks express the same architecture differently?\n12. **Graph vs imperative**: TensorFlow builds computation graphs; PyTorch is imperative. How does this affect code?\n13. **Abstraction levels**: Compare minimal educational code to production PyTorch. What's abstracted away?\n14. **Optimization tricks**: What implementation details are needed for efficiency but not conceptual understanding?\n\n### Scale and Infrastructure\n\n15. **Batch processing**: How does batching work? Why is it essential for GPU efficiency?\n16. **Memory usage**: Where does memory consumption come from? What limits model size?\n17. **Parallelization**: What operations parallelize across GPUs? What must be sequential?\n18. **Training vs inference**: How does the code differ for training vs. using a trained model?\n\n### Data and Training\n\n19. **Training loop**: How is the model actually trained? What does the loss function measure?\n20. **Tokenization**: How is text converted to numbers? What are tokens?\n21. **Vocabulary**: What words/tokens does the model know? Who decides?\n22. **Dataset construction**: Where does training data come from? Whose labor created it?\n\n### Opacity and Interpretability\n\n23. **Attention visualization**: Can we see what the model \"attends\" to? Does this explain behavior?\n24. **Learned weights**: What do billions of parameters represent? Can we interpret individual weights?\n25. **Emergent behavior**: Why do large Transformers develop unexpected capabilities? Where does this come from?\n\n### Labor and Political Economy\n\n26. **Data labor**: Who annotated, cleaned, and formatted training data? Were they compensated?\n27. **Computational resources**: Who can afford to train large Transformers? What does this concentration mean?\n28. **Environmental costs**: How much energy for training? Who bears the carbon cost?\n29. **Automation**: What human labor does this architecture replace? What new labor does it create?\n30. **Intellectual property**: Code is open source, but trained models are often proprietary. Why the split?\n\n### Epistemology and Meaning\n\n31. **Statistical patterns**: The model learns correlations in text. Is this \"understanding\" or pattern matching?\n32. **Grounding problem**: Transformers never interact with the physical world. What are limits of text-only training?\n33. **Hallucination**: Why do models generate plausible-sounding falsehoods? What's missing?\n34. **Context window**: Why are Transformers limited to fixed-length inputs? How does this constrain reasoning?\n35. **Transfer learning**: Why do models trained on text generation work for other tasks? What's being transferred?\n\n## References\n\n- Vaswani, A., et al. (2017). \"Attention Is All You Need.\" *NeurIPS 2017*. arXiv:1706.03762\n- Alammar, J. (2018). \"The Illustrated Transformer.\" https://jalammar.github.io/illustrated-transformer/\n- Rush, A. M. (2018). \"The Annotated Transformer.\" *Harvard NLP*. http://nlp.seas.harvard.edu/annotated-transformer/\n- Brown, T., et al. (2020). \"Language Models are Few-Shot Learners\" (GPT-3). *NeurIPS 2020*.\n- Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\"\n- Crawford, K., & Joler, V. (2018). \"Anatomy of an AI System.\" https://anatomyof.ai\n- Bender, E. M., et al. (2021). \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\"\n- Strubell, E., et al. (2019). \"Energy and Policy Considerations for Deep Learning in NLP.\"\n- Birhane, A., et al. (2021). \"The Values Encoded in Machine Learning Research.\"\n- Crawford, K. (2021). *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence*. Yale University Press.\n",
    "file-2": "\"\"\"\n================================================================================\nCRITICAL CODE STUDIES SAMPLE - TRANSFORMER ARCHITECTURE (2017)\n================================================================================\n\nImplementation: Harvard NLP Annotated Transformer (Educational)\nFramework: PyTorch\nYear: 2022 version (based on 2017 \"Attention Is All You Need\" paper)\nPurpose: Line-by-line pedagogical implementation with extensive annotations\nAuthors: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,\n         Stella Biderman (v2022); Original by Sasha Rush (2018)\nSource: https://github.com/harvardnlp/annotated-transformer/\nLicense: MIT License\n\nThis is a complete, working implementation of the Transformer architecture\ndesigned for teaching and understanding. It includes:\n- Full model implementation with detailed comments\n- Training loop and data handling\n- Visualization tools for attention weights\n- Real-world translation examples\n\nCompare this pedagogical version to production implementations in:\n- transformer_pytorch.py (PyTorch official implementation)\n- transformer_tensorflow.py (TensorFlow tensor2tensor implementation)\n\n================================================================================\n\"\"\"\n\n# -*- coding: utf-8 -*-\n# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.13.0\n#   kernelspec:\n#     display_name: Python 3 (ipykernel)\n#     language: python\n#     name: python3\n# ---\n# %% [markdown] id=\"SX7UC-8jTsp7\" tags=[]\n#\n# <center><h1>The Annotated Transformer</h1> </center>\n#\n#\n# <center>\n# <p><a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need\n# </a></p>\n# </center>\n#\n# <img src=\"images/aiayn.png\" width=\"70%\"/>\n#\n# * *v2022: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,\n#    and Stella Biderman.*\n# * *[Original](https://nlp.seas.harvard.edu/2018/04/03/attention.html):\n#    [Sasha Rush](http://rush-nlp.com/).*\n#\n#\n# The Transformer has been on a lot of\n# people's minds over the last <s>year</s> five years.\n# This post presents an annotated version of the paper in the\n# form of a line-by-line implementation. It reorders and deletes\n# some sections from the original paper and adds comments\n# throughout. This document itself is a working notebook, and should\n# be a completely usable implementation.\n# Code is available\n# [here](https://github.com/harvardnlp/annotated-transformer/).\n#\n\n\n# %% [markdown] id=\"RSntDwKhTsp-\"\n# <h3> Table of Contents </h3>\n# <ul>\n# <li><a href=\"#prelims\">Prelims</a></li>\n# <li><a href=\"#background\">Background</a></li>\n# <li><a href=\"#part-1-model-architecture\">Part 1: Model Architecture</a></li>\n# <li><a href=\"#model-architecture\">Model Architecture</a><ul>\n# <li><a href=\"#encoder-and-decoder-stacks\">Encoder and Decoder Stacks</a></li>\n# <li><a href=\"#position-wise-feed-forward-networks\">Position-wise Feed-Forward\n# Networks</a></li>\n# <li><a href=\"#embeddings-and-softmax\">Embeddings and Softmax</a></li>\n# <li><a href=\"#positional-encoding\">Positional Encoding</a></li>\n# <li><a href=\"#full-model\">Full Model</a></li>\n# <li><a href=\"#inference\">Inference:</a></li>\n# </ul></li>\n# <li><a href=\"#part-2-model-training\">Part 2: Model Training</a></li>\n# <li><a href=\"#training\">Training</a><ul>\n# <li><a href=\"#batches-and-masking\">Batches and Masking</a></li>\n# <li><a href=\"#training-loop\">Training Loop</a></li>\n# <li><a href=\"#training-data-and-batching\">Training Data and Batching</a></li>\n# <li><a href=\"#hardware-and-schedule\">Hardware and Schedule</a></li>\n# <li><a href=\"#optimizer\">Optimizer</a></li>\n# <li><a href=\"#regularization\">Regularization</a></li>\n# </ul></li>\n# <li><a href=\"#a-first-example\">A First Example</a><ul>\n# <li><a href=\"#synthetic-data\">Synthetic Data</a></li>\n# <li><a href=\"#loss-computation\">Loss Computation</a></li>\n# <li><a href=\"#greedy-decoding\">Greedy Decoding</a></li>\n# </ul></li>\n# <li><a href=\"#part-3-a-real-world-example\">Part 3: A Real World Example</a>\n# <ul>\n# <li><a href=\"#data-loading\">Data Loading</a></li>\n# <li><a href=\"#iterators\">Iterators</a></li>\n# <li><a href=\"#training-the-system\">Training the System</a></li>\n# </ul></li>\n# <li><a href=\"#additional-components-bpe-search-averaging\">Additional\n# Components: BPE, Search, Averaging</a></li>\n# <li><a href=\"#results\">Results</a><ul>\n# <li><a href=\"#attention-visualization\">Attention Visualization</a></li>\n# <li><a href=\"#encoder-self-attention\">Encoder Self Attention</a></li>\n# <li><a href=\"#decoder-self-attention\">Decoder Self Attention</a></li>\n# <li><a href=\"#decoder-src-attention\">Decoder Src Attention</a></li>\n# </ul></li>\n# <li><a href=\"#conclusion\">Conclusion</a></li>\n# </ul>\n\n\n# %% [markdown] id=\"BhmOhn9lTsp8\"\n# # Prelims\n#\n# <a href=\"#background\">Skip</a>\n\n# %% id=\"NwClcbH6Tsp8\"\n# # !pip install -r requirements.txt\n\n# %% id=\"NwClcbH6Tsp8\"\n# # Uncomment for colab\n# #\n# # !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n# # !python -m spacy download de_core_news_sm\n# # !python -m spacy download en_core_web_sm\n\n\n# %% id=\"v1-1MX6oTsp9\"\nimport os\nfrom os.path import exists\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import log_softmax, pad\nimport math\nimport copy\nimport time\nfrom torch.optim.lr_scheduler import LambdaLR\nimport pandas as pd\nimport altair as alt\nfrom torchtext.data.functional import to_map_style_dataset\nfrom torch.utils.data import DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torchtext.datasets as datasets\nimport spacy\nimport GPUtil\nimport warnings\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\n# Set to False to skip notebook execution (e.g. for debugging)\nwarnings.filterwarnings(\"ignore\")\nRUN_EXAMPLES = True\n\n\n# %%\n# Some convenience helper functions used throughout the notebook\n\n\ndef is_interactive_notebook():\n    return __name__ == \"__main__\"\n\n\ndef show_example(fn, args=[]):\n    if __name__ == \"__main__\" and RUN_EXAMPLES:\n        return fn(*args)\n\n\ndef execute_example(fn, args=[]):\n    if __name__ == \"__main__\" and RUN_EXAMPLES:\n        fn(*args)\n\n\nclass DummyOptimizer(torch.optim.Optimizer):\n    def __init__(self):\n        self.param_groups = [{\"lr\": 0}]\n        None\n\n    def step(self):\n        None\n\n    def zero_grad(self, set_to_none=False):\n        None\n\n\nclass DummyScheduler:\n    def step(self):\n        None\n\n\n# %% [markdown] id=\"jx49WRyfTsp-\"\n# > My comments are blockquoted. The main text is all from the paper itself.\n\n# %% [markdown] id=\"7phVeWghTsp_\"\n# # Background\n\n# %% [markdown] id=\"83ZDS91dTsqA\"\n#\n# The goal of reducing sequential computation also forms the\n# foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of\n# which use convolutional neural networks as basic building block,\n# computing hidden representations in parallel for all input and\n# output positions. In these models, the number of operations required\n# to relate signals from two arbitrary input or output positions grows\n# in the distance between positions, linearly for ConvS2S and\n# logarithmically for ByteNet. This makes it more difficult to learn\n# dependencies between distant positions. In the Transformer this is\n# reduced to a constant number of operations, albeit at the cost of\n# reduced effective resolution due to averaging attention-weighted\n# positions, an effect we counteract with Multi-Head Attention.\n#\n# Self-attention, sometimes called intra-attention is an attention\n# mechanism relating different positions of a single sequence in order\n# to compute a representation of the sequence. Self-attention has been\n# used successfully in a variety of tasks including reading\n# comprehension, abstractive summarization, textual entailment and\n# learning task-independent sentence representations. End-to-end\n# memory networks are based on a recurrent attention mechanism instead\n# of sequencealigned recurrence and have been shown to perform well on\n# simple-language question answering and language modeling tasks.\n#\n# To the best of our knowledge, however, the Transformer is the first\n# transduction model relying entirely on self-attention to compute\n# representations of its input and output without using sequence\n# aligned RNNs or convolution.\n\n# %% [markdown]\n# # Part 1: Model Architecture\n\n# %% [markdown] id=\"pFrPajezTsqB\"\n# # Model Architecture\n\n# %% [markdown] id=\"ReuU_h-fTsqB\"\n#\n# Most competitive neural sequence transduction models have an\n# encoder-decoder structure\n# [(cite)](https://arxiv.org/abs/1409.0473). Here, the encoder maps an\n# input sequence of symbol representations $(x_1, ..., x_n)$ to a\n# sequence of continuous representations $\\mathbf{z} = (z_1, ...,\n# z_n)$. Given $\\mathbf{z}$, the decoder then generates an output\n# sequence $(y_1,...,y_m)$ of symbols one element at a time. At each\n# step the model is auto-regressive\n# [(cite)](https://arxiv.org/abs/1308.0850), consuming the previously\n# generated symbols as additional input when generating the next.\n\n# %% id=\"k0XGXhzRTsqB\"\nclass EncoderDecoder(nn.Module):\n    \"\"\"\n    A standard Encoder-Decoder architecture. Base for this and many\n    other models.\n    \"\"\"\n\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n\n\n# %% id=\"NKGoH2RsTsqC\"\nclass Generator(nn.Module):\n    \"Define standard linear + softmax generation step.\"\n\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return log_softmax(self.proj(x), dim=-1)\n\n\n# %% [markdown] id=\"mOoEnF_jTsqC\"\n#\n# The Transformer follows this overall architecture using stacked\n# self-attention and point-wise, fully connected layers for both the\n# encoder and decoder, shown in the left and right halves of Figure 1,\n# respectively.\n\n# %% [markdown] id=\"oredWloYTsqC\"\n# ![](images/ModalNet-21.png)\n\n\n# %% [markdown] id=\"bh092NZBTsqD\"\n# ## Encoder and Decoder Stacks\n#\n# ### Encoder\n#\n# The encoder is composed of a stack of $N=6$ identical layers.\n\n# %% id=\"2gxTApUYTsqD\"\ndef clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n\n# %% id=\"xqVTz9MkTsqD\"\nclass Encoder(nn.Module):\n    \"Core encoder is a stack of N layers\"\n\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\n\n# %% [markdown] id=\"GjAKgjGwTsqD\"\n#\n# We employ a residual connection\n# [(cite)](https://arxiv.org/abs/1512.03385) around each of the two\n# sub-layers, followed by layer normalization\n# [(cite)](https://arxiv.org/abs/1607.06450).\n\n# %% id=\"3jKa_prZTsqE\"\nclass LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\n\n# %% [markdown] id=\"nXSJ3QYmTsqE\"\n#\n# That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x +\n# \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function\n# implemented by the sub-layer itself.  We apply dropout\n# [(cite)](http://jmlr.org/papers/v15/srivastava14a.html) to the\n# output of each sub-layer, before it is added to the sub-layer input\n# and normalized.\n#\n# To facilitate these residual connections, all sub-layers in the\n# model, as well as the embedding layers, produce outputs of dimension\n# $d_{\\text{model}}=512$.\n\n# %% id=\"U1P7zI0eTsqE\"\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n\n\n# %% [markdown] id=\"ML6oDlEqTsqE\"\n#\n# Each layer has two sub-layers. The first is a multi-head\n# self-attention mechanism, and the second is a simple, position-wise\n# fully connected feed-forward network.\n\n# %% id=\"qYkUFr6GTsqE\"\nclass EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n\n\n# %% [markdown] id=\"7ecOQIhkTsqF\"\n# ### Decoder\n#\n# The decoder is also composed of a stack of $N=6$ identical layers.\n#\n\n# %%\nclass Decoder(nn.Module):\n    \"Generic N layer decoder with masking.\"\n\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n\n\n# %% [markdown] id=\"dXlCB12pTsqF\"\n#\n# In addition to the two sub-layers in each encoder layer, the decoder\n# inserts a third sub-layer, which performs multi-head attention over\n# the output of the encoder stack.  Similar to the encoder, we employ\n# residual connections around each of the sub-layers, followed by\n# layer normalization.\n\n# %% id=\"M2hA1xFQTsqF\"\nclass DecoderLayer(nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"Follow Figure 1 (right) for connections.\"\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\n\n# %% [markdown] id=\"FZz5rLl4TsqF\"\n#\n# We also modify the self-attention sub-layer in the decoder stack to\n# prevent positions from attending to subsequent positions.  This\n# masking, combined with fact that the output embeddings are offset by\n# one position, ensures that the predictions for position $i$ can\n# depend only on the known outputs at positions less than $i$.\n\n# %% id=\"QN98O2l3TsqF\"\ndef subsequent_mask(size):\n    \"Mask out subsequent positions.\"\n    attn_shape = (1, size, size)\n    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n        torch.uint8\n    )\n    return subsequent_mask == 0\n\n\n# %% [markdown] id=\"Vg_f_w-PTsqG\"\n#\n# > Below the attention mask shows the position each tgt word (row) is\n# > allowed to look at (column). Words are blocked for attending to\n# > future words during training.\n\n# %% id=\"ht_FtgYAokC4\"\ndef example_mask():\n    LS_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n                    \"Window\": y,\n                    \"Masking\": x,\n                }\n            )\n            for y in range(20)\n            for x in range(20)\n        ]\n    )\n\n    return (\n        alt.Chart(LS_data)\n        .mark_rect()\n        .properties(height=250, width=250)\n        .encode(\n            alt.X(\"Window:O\"),\n            alt.Y(\"Masking:O\"),\n            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n        )\n        .interactive()\n    )\n\n\nshow_example(example_mask)\n\n# %% [markdown] id=\"Qto_yg7BTsqG\"\n# ### Attention\n#\n# An attention function can be described as mapping a query and a set\n# of key-value pairs to an output, where the query, keys, values, and\n# output are all vectors.  The output is computed as a weighted sum of\n# the values, where the weight assigned to each value is computed by a\n# compatibility function of the query with the corresponding key.\n#\n# We call our particular attention \"Scaled Dot-Product Attention\".\n# The input consists of queries and keys of dimension $d_k$, and\n# values of dimension $d_v$.  We compute the dot products of the query\n# with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax\n# function to obtain the weights on the values.\n#\n#\n#\n# ![](images/ModalNet-19.png)\n\n\n# %% [markdown] id=\"EYJLWk6cTsqG\"\n#\n# In practice, we compute the attention function on a set of queries\n# simultaneously, packed together into a matrix $Q$.  The keys and\n# values are also packed together into matrices $K$ and $V$.  We\n# compute the matrix of outputs as:\n#\n# $$\n#    \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n# $$\n\n# %% id=\"qsoVxS5yTsqG\"\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\n\n# %% [markdown] id=\"jUkpwu8kTsqG\"\n#\n# The two most commonly used attention functions are additive\n# attention [(cite)](https://arxiv.org/abs/1409.0473), and dot-product\n# (multiplicative) attention.  Dot-product attention is identical to\n# our algorithm, except for the scaling factor of\n# $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the\n# compatibility function using a feed-forward network with a single\n# hidden layer.  While the two are similar in theoretical complexity,\n# dot-product attention is much faster and more space-efficient in\n# practice, since it can be implemented using highly optimized matrix\n# multiplication code.\n#\n#\n# While for small values of $d_k$ the two mechanisms perform\n# similarly, additive attention outperforms dot product attention\n# without scaling for larger values of $d_k$\n# [(cite)](https://arxiv.org/abs/1703.03906). We suspect that for\n# large values of $d_k$, the dot products grow large in magnitude,\n# pushing the softmax function into regions where it has extremely\n# small gradients (To illustrate why the dot products get large,\n# assume that the components of $q$ and $k$ are independent random\n# variables with mean $0$ and variance $1$.  Then their dot product,\n# $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance\n# $d_k$.). To counteract this effect, we scale the dot products by\n# $\\frac{1}{\\sqrt{d_k}}$.\n#\n#\n\n# %% [markdown] id=\"bS1FszhVTsqG\"\n# ![](images/ModalNet-20.png)\n\n\n# %% [markdown] id=\"TNtVyZ-pTsqH\"\n#\n# Multi-head attention allows the model to jointly attend to\n# information from different representation subspaces at different\n# positions. With a single attention head, averaging inhibits this.\n#\n# $$\n# \\mathrm{MultiHead}(Q, K, V) =\n#     \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n#     \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n# $$\n#\n# Where the projections are parameter matrices $W^Q_i \\in\n# \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in\n# \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in\n# \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in\n# \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n#\n# In this work we employ $h=8$ parallel attention layers, or\n# heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due\n# to the reduced dimension of each head, the total computational cost\n# is similar to that of single-head attention with full\n# dimensionality.\n\n# %% id=\"D2LBMKCQTsqH\"\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = [\n            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout\n        )\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = (\n            x.transpose(1, 2)\n            .contiguous()\n            .view(nbatches, -1, self.h * self.d_k)\n        )\n        del query\n        del key\n        del value\n        return self.linears[-1](x)\n\n\n# %% [markdown] id=\"EDRba3J3TsqH\"\n# ### Applications of Attention in our Model\n#\n# The Transformer uses multi-head attention in three different ways:\n# 1) In \"encoder-decoder attention\" layers, the queries come from the\n# previous decoder layer, and the memory keys and values come from the\n# output of the encoder.  This allows every position in the decoder to\n# attend over all positions in the input sequence.  This mimics the\n# typical encoder-decoder attention mechanisms in sequence-to-sequence\n# models such as [(cite)](https://arxiv.org/abs/1609.08144).\n#\n#\n# 2) The encoder contains self-attention layers.  In a self-attention\n# layer all of the keys, values and queries come from the same place,\n# in this case, the output of the previous layer in the encoder.  Each\n# position in the encoder can attend to all positions in the previous\n# layer of the encoder.\n#\n#\n# 3) Similarly, self-attention layers in the decoder allow each\n# position in the decoder to attend to all positions in the decoder up\n# to and including that position.  We need to prevent leftward\n# information flow in the decoder to preserve the auto-regressive\n# property.  We implement this inside of scaled dot-product attention\n# by masking out (setting to $-\\infty$) all values in the input of the\n# softmax which correspond to illegal connections.\n\n# %% [markdown] id=\"M-en97_GTsqH\"\n# ## Position-wise Feed-Forward Networks\n#\n# In addition to attention sub-layers, each of the layers in our\n# encoder and decoder contains a fully connected feed-forward network,\n# which is applied to each position separately and identically.  This\n# consists of two linear transformations with a ReLU activation in\n# between.\n#\n# $$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n#\n# While the linear transformations are the same across different\n# positions, they use different parameters from layer to\n# layer. Another way of describing this is as two convolutions with\n# kernel size 1.  The dimensionality of input and output is\n# $d_{\\text{model}}=512$, and the inner-layer has dimensionality\n# $d_{ff}=2048$.\n\n# %% id=\"6HHCemCxTsqH\"\nclass PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n\n\n# %% [markdown] id=\"dR1YM520TsqH\"\n# ## Embeddings and Softmax\n#\n# Similarly to other sequence transduction models, we use learned\n# embeddings to convert the input tokens and output tokens to vectors\n# of dimension $d_{\\text{model}}$.  We also use the usual learned\n# linear transformation and softmax function to convert the decoder\n# output to predicted next-token probabilities.  In our model, we\n# share the same weight matrix between the two embedding layers and\n# the pre-softmax linear transformation, similar to\n# [(cite)](https://arxiv.org/abs/1608.05859). In the embedding layers,\n# we multiply those weights by $\\sqrt{d_{\\text{model}}}$.\n\n# %% id=\"pyrChq9qTsqH\"\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)\n\n\n# %% [markdown] id=\"vOkdui-cTsqH\"\n# ## Positional Encoding\n#\n# Since our model contains no recurrence and no convolution, in order\n# for the model to make use of the order of the sequence, we must\n# inject some information about the relative or absolute position of\n# the tokens in the sequence.  To this end, we add \"positional\n# encodings\" to the input embeddings at the bottoms of the encoder and\n# decoder stacks.  The positional encodings have the same dimension\n# $d_{\\text{model}}$ as the embeddings, so that the two can be summed.\n# There are many choices of positional encodings, learned and fixed\n# [(cite)](https://arxiv.org/pdf/1705.03122.pdf).\n#\n# In this work, we use sine and cosine functions of different frequencies:\n#\n# $$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n#\n# $$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$\n#\n# where $pos$ is the position and $i$ is the dimension.  That is, each\n# dimension of the positional encoding corresponds to a sinusoid.  The\n# wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot\n# 2\\pi$.  We chose this function because we hypothesized it would\n# allow the model to easily learn to attend by relative positions,\n# since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a\n# linear function of $PE_{pos}$.\n#\n# In addition, we apply dropout to the sums of the embeddings and the\n# positional encodings in both the encoder and decoder stacks.  For\n# the base model, we use a rate of $P_{drop}=0.1$.\n#\n#\n\n# %% id=\"zaHGD4yJTsqH\"\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n        return self.dropout(x)\n\n\n# %% [markdown] id=\"EfHacTJLTsqH\"\n#\n# > Below the positional encoding will add in a sine wave based on\n# > position. The frequency and offset of the wave is different for\n# > each dimension.\n\n# %% id=\"rnvHk_1QokC6\" type=\"example\"\ndef example_positional():\n    pe = PositionalEncoding(20, 0)\n    y = pe.forward(torch.zeros(1, 100, 20))\n\n    data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"embedding\": y[0, :, dim],\n                    \"dimension\": dim,\n                    \"position\": list(range(100)),\n                }\n            )\n            for dim in [4, 5, 6, 7]\n        ]\n    )\n\n    return (\n        alt.Chart(data)\n        .mark_line()\n        .properties(width=800)\n        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n        .interactive()\n    )\n\n\nshow_example(example_positional)\n\n\n# %% [markdown] id=\"g8rZNCrzTsqI\"\n#\n# We also experimented with using learned positional embeddings\n# [(cite)](https://arxiv.org/pdf/1705.03122.pdf) instead, and found\n# that the two versions produced nearly identical results.  We chose\n# the sinusoidal version because it may allow the model to extrapolate\n# to sequence lengths longer than the ones encountered during\n# training.\n\n# %% [markdown] id=\"iwNKCzlyTsqI\"\n# ## Full Model\n#\n# > Here we define a function from hyperparameters to a full model.\n\n# %% id=\"mPe1ES0UTsqI\"\ndef make_model(\n    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n    \"Helper: Construct a model from hyperparameters.\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab),\n    )\n\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model\n\n\n# %% [markdown]\n# ## Inference:\n#\n# > Here we make a forward step to generate a prediction of the\n# model. We try to use our transformer to memorize the input. As you\n# will see the output is randomly generated due to the fact that the\n# model is not trained yet. In the next tutorial we will build the\n# training function and try to train our model to memorize the numbers\n# from 1 to 10.\n\n# %%\ndef inference_test():\n    test_model = make_model(11, 11, 2)\n    test_model.eval()\n    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n    src_mask = torch.ones(1, 1, 10)\n\n    memory = test_model.encode(src, src_mask)\n    ys = torch.zeros(1, 1).type_as(src)\n\n    for i in range(9):\n        out = test_model.decode(\n            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n        )\n        prob = test_model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]\n        ys = torch.cat(\n            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n        )\n\n    print(\"Example Untrained Model Prediction:\", ys)\n\n\ndef run_tests():\n    for _ in range(10):\n        inference_test()\n\n\nshow_example(run_tests)\n\n\n# %% [markdown]\n# # Part 2: Model Training\n\n# %% [markdown] id=\"05s6oT9fTsqI\"\n# # Training\n#\n# This section describes the training regime for our models.\n\n# %% [markdown] id=\"fTxlofs4TsqI\"\n#\n# > We stop for a quick interlude to introduce some of the tools\n# > needed to train a standard encoder decoder model. First we define a\n# > batch object that holds the src and target sentences for training,\n# > as well as constructing the masks.\n\n# %% [markdown] id=\"G7SkCenXTsqI\"\n# ## Batches and Masking\n\n# %%\nclass Batch:\n    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n\n    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n            self.tgt = tgt[:, :-1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n            self.ntokens = (self.tgt_y != pad).data.sum()\n\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n            tgt_mask.data\n        )\n        return tgt_mask\n\n\n# %% [markdown] id=\"cKkw5GjLTsqI\"\n#\n# > Next we create a generic training and scoring function to keep\n# > track of loss. We pass in a generic loss compute function that\n# > also handles parameter updates.\n\n# %% [markdown] id=\"Q8zzeUc0TsqJ\"\n# ## Training Loop\n\n# %%\nclass TrainState:\n    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n\n    step: int = 0  # Steps in the current epoch\n    accum_step: int = 0  # Number of gradient accumulation steps\n    samples: int = 0  # total # of examples used\n    tokens: int = 0  # total # of tokens processed\n\n\n# %% id=\"2HAZD3hiTsqJ\"\ndef run_epoch(\n    data_iter,\n    model,\n    loss_compute,\n    optimizer,\n    scheduler,\n    mode=\"train\",\n    accum_iter=1,\n    train_state=TrainState(),\n):\n    \"\"\"Train a single epoch\"\"\"\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    n_accum = 0\n    for i, batch in enumerate(data_iter):\n        out = model.forward(\n            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n        )\n        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n        # loss_node = loss_node / accum_iter\n        if mode == \"train\" or mode == \"train+log\":\n            loss_node.backward()\n            train_state.step += 1\n            train_state.samples += batch.src.shape[0]\n            train_state.tokens += batch.ntokens\n            if i % accum_iter == 0:\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                n_accum += 1\n                train_state.accum_step += 1\n            scheduler.step()\n\n        total_loss += loss\n        total_tokens += batch.ntokens\n        tokens += batch.ntokens\n        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n            lr = optimizer.param_groups[0][\"lr\"]\n            elapsed = time.time() - start\n            print(\n                (\n                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n                )\n                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n            )\n            start = time.time()\n            tokens = 0\n        del loss\n        del loss_node\n    return total_loss / total_tokens, train_state\n\n\n# %% [markdown] id=\"aB1IF0foTsqJ\"\n# ## Training Data and Batching\n#\n# We trained on the standard WMT 2014 English-German dataset\n# consisting of about 4.5 million sentence pairs.  Sentences were\n# encoded using byte-pair encoding, which has a shared source-target\n# vocabulary of about 37000 tokens. For English-French, we used the\n# significantly larger WMT 2014 English-French dataset consisting of\n# 36M sentences and split tokens into a 32000 word-piece vocabulary.\n#\n#\n# Sentence pairs were batched together by approximate sequence length.\n# Each training batch contained a set of sentence pairs containing\n# approximately 25000 source tokens and 25000 target tokens.\n\n# %% [markdown] id=\"F1mTQatiTsqJ\" jp-MarkdownHeadingCollapsed=true tags=[]\n# ## Hardware and Schedule\n#\n# We trained our models on one machine with 8 NVIDIA P100 GPUs.  For\n# our base models using the hyperparameters described throughout the\n# paper, each training step took about 0.4 seconds.  We trained the\n# base models for a total of 100,000 steps or 12 hours. For our big\n# models, step time was 1.0 seconds.  The big models were trained for\n# 300,000 steps (3.5 days).\n\n# %% [markdown] id=\"-utZeuGcTsqJ\"\n# ## Optimizer\n#\n# We used the Adam optimizer [(cite)](https://arxiv.org/abs/1412.6980)\n# with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We\n# varied the learning rate over the course of training, according to\n# the formula:\n#\n# $$\n# lrate = d_{\\text{model}}^{-0.5} \\cdot\n#   \\min({step\\_num}^{-0.5},\n#     {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n# $$\n#\n# This corresponds to increasing the learning rate linearly for the\n# first $warmup\\_steps$ training steps, and decreasing it thereafter\n# proportionally to the inverse square root of the step number.  We\n# used $warmup\\_steps=4000$.\n\n# %% [markdown] id=\"39FbYnt-TsqJ\"\n#\n# > Note: This part is very important. Need to train with this setup\n# > of the model.\n\n# %% [markdown] id=\"hlbojFkjTsqJ\"\n#\n# > Example of the curves of this model for different model sizes and\n# > for optimization hyperparameters.\n\n# %% id=\"zUz3PdAnVg4o\"\ndef rate(step, model_size, factor, warmup):\n    \"\"\"\n    we have to default the step to 1 for LambdaLR function\n    to avoid zero raising to negative power.\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (\n        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n    )\n\n\n# %% id=\"l1bnrlnSV8J5\" tags=[]\ndef example_learning_schedule():\n    opts = [\n        [512, 1, 4000],  # example 1\n        [512, 1, 8000],  # example 2\n        [256, 1, 4000],  # example 3\n    ]\n\n    dummy_model = torch.nn.Linear(1, 1)\n    learning_rates = []\n\n    # we have 3 examples in opts list.\n    for idx, example in enumerate(opts):\n        # run 20000 epoch for each example\n        optimizer = torch.optim.Adam(\n            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n        )\n        lr_scheduler = LambdaLR(\n            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n        )\n        tmp = []\n        # take 20K dummy training steps, save the learning rate at each step\n        for step in range(20000):\n            tmp.append(optimizer.param_groups[0][\"lr\"])\n            optimizer.step()\n            lr_scheduler.step()\n        learning_rates.append(tmp)\n\n    learning_rates = torch.tensor(learning_rates)\n\n    # Enable altair to handle more than 5000 rows\n    alt.data_transformers.disable_max_rows()\n\n    opts_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"Learning Rate\": learning_rates[warmup_idx, :],\n                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n                        warmup_idx\n                    ],\n                    \"step\": range(20000),\n                }\n            )\n            for warmup_idx in [0, 1, 2]\n        ]\n    )\n\n    return (\n        alt.Chart(opts_data)\n        .mark_line()\n        .properties(width=600)\n        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n        .interactive()\n    )\n\n\nexample_learning_schedule()\n\n\n# %% [markdown] id=\"7T1uD15VTsqK\"\n# ## Regularization\n#\n# ### Label Smoothing\n#\n# During training, we employed label smoothing of value\n# $\\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567).\n# This hurts perplexity, as the model learns to be more unsure, but\n# improves accuracy and BLEU score.\n\n# %% [markdown] id=\"kNoAVD8bTsqK\"\n#\n# > We implement label smoothing using the KL div loss. Instead of\n# > using a one-hot target distribution, we create a distribution that\n# > has `confidence` of the correct word and the rest of the\n# > `smoothing` mass distributed throughout the vocabulary.\n\n# %% id=\"shU2GyiETsqK\"\nclass LabelSmoothing(nn.Module):\n    \"Implement label smoothing.\"\n\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n\n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, true_dist.clone().detach())\n\n\n# %% [markdown] id=\"jCxUrlUyTsqK\"\n#\n# > Here we can see an example of how the mass is distributed to the\n# > words based on confidence.\n\n# %% id=\"EZtKaaQNTsqK\"\n# Example of label smoothing.\n\n\ndef example_label_smoothing():\n    crit = LabelSmoothing(5, 0, 0.4)\n    predict = torch.FloatTensor(\n        [\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n            [0, 0.2, 0.7, 0.1, 0],\n        ]\n    )\n    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n    LS_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"target distribution\": crit.true_dist[x, y].flatten(),\n                    \"columns\": y,\n                    \"rows\": x,\n                }\n            )\n            for y in range(5)\n            for x in range(5)\n        ]\n    )\n\n    return (\n        alt.Chart(LS_data)\n        .mark_rect(color=\"Blue\", opacity=1)\n        .properties(height=200, width=200)\n        .encode(\n            alt.X(\"columns:O\", title=None),\n            alt.Y(\"rows:O\", title=None),\n            alt.Color(\n                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n            ),\n        )\n        .interactive()\n    )\n\n\nshow_example(example_label_smoothing)\n\n\n# %% [markdown] id=\"CGM8J1veTsqK\"\n#\n# > Label smoothing actually starts to penalize the model if it gets\n# > very confident about a given choice.\n\n# %% id=\"78EHzLP7TsqK\"\n\n\ndef loss(x, crit):\n    d = x + 3 * 1\n    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n    return crit(predict.log(), torch.LongTensor([1])).data\n\n\ndef penalization_visualization():\n    crit = LabelSmoothing(5, 0, 0.1)\n    loss_data = pd.DataFrame(\n        {\n            \"Loss\": [loss(x, crit) for x in range(1, 100)],\n            \"Steps\": list(range(99)),\n        }\n    ).astype(\"float\")\n\n    return (\n        alt.Chart(loss_data)\n        .mark_line()\n        .properties(width=350)\n        .encode(\n            x=\"Steps\",\n            y=\"Loss\",\n        )\n        .interactive()\n    )\n\n\nshow_example(penalization_visualization)\n\n\n# %% [markdown] id=\"67lUqeLXTsqK\"\n# # A First  Example\n#\n# > We can begin by trying out a simple copy-task. Given a random set\n# > of input symbols from a small vocabulary, the goal is to generate\n# > back those same symbols.\n\n# %% [markdown] id=\"jJa-89_pTsqK\"\n# ## Synthetic Data\n\n# %% id=\"g1aTxeqqTsqK\"\ndef data_gen(V, batch_size, nbatches):\n    \"Generate random data for a src-tgt copy task.\"\n    for i in range(nbatches):\n        data = torch.randint(1, V, size=(batch_size, 10))\n        data[:, 0] = 1\n        src = data.requires_grad_(False).clone().detach()\n        tgt = data.requires_grad_(False).clone().detach()\n        yield Batch(src, tgt, 0)\n\n\n# %% [markdown] id=\"XTXwD9hUTsqK\"\n# ## Loss Computation\n\n# %% id=\"3J8EJm87TsqK\"\nclass SimpleLossCompute:\n    \"A simple loss compute and train function.\"\n\n    def __init__(self, generator, criterion):\n        self.generator = generator\n        self.criterion = criterion\n\n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        sloss = (\n            self.criterion(\n                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n            )\n            / norm\n        )\n        return sloss.data * norm, sloss\n\n\n# %% [markdown] id=\"eDAI7ELUTsqL\"\n# ## Greedy Decoding\n\n# %% [markdown] id=\"LFkWakplTsqL\" tags=[]\n# > This code predicts a translation using greedy decoding for simplicity.\n# %% id=\"N2UOpnT3bIyU\"\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n    for i in range(max_len - 1):\n        out = model.decode(\n            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n        )\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]\n        ys = torch.cat(\n            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n        )\n    return ys\n\n\n# %% id=\"qgIZ2yEtdYwe\" tags=[]\n# Train the simple copy task.\n\n\ndef example_simple_model():\n    V = 11\n    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n    model = make_model(V, V, N=2)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda=lambda step: rate(\n            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n        ),\n    )\n\n    batch_size = 80\n    for epoch in range(20):\n        model.train()\n        run_epoch(\n            data_gen(V, batch_size, 20),\n            model,\n            SimpleLossCompute(model.generator, criterion),\n            optimizer,\n            lr_scheduler,\n            mode=\"train\",\n        )\n        model.eval()\n        run_epoch(\n            data_gen(V, batch_size, 5),\n            model,\n            SimpleLossCompute(model.generator, criterion),\n            DummyOptimizer(),\n            DummyScheduler(),\n            mode=\"eval\",\n        )[0]\n\n    model.eval()\n    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n    max_len = src.shape[1]\n    src_mask = torch.ones(1, 1, max_len)\n    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n\n\n# execute_example(example_simple_model)\n\n\n# %% [markdown] id=\"OpuQv2GsTsqL\"\n# # Part 3: A Real World Example\n#\n# > Now we consider a real-world example using the Multi30k\n# > German-English Translation task. This task is much smaller than\n# > the WMT task considered in the paper, but it illustrates the whole\n# > system. We also show how to use multi-gpu processing to make it\n# > really fast.\n\n# %% [markdown] id=\"8y9dpfolTsqL\" tags=[]\n# ## Data Loading\n#\n# > We will load the dataset using torchtext and spacy for\n# > tokenization.\n\n# %%\n# Load spacy tokenizer models, download them if they haven't been\n# downloaded already\n\n\ndef load_tokenizers():\n\n    try:\n        spacy_de = spacy.load(\"de_core_news_sm\")\n    except IOError:\n        os.system(\"python -m spacy download de_core_news_sm\")\n        spacy_de = spacy.load(\"de_core_news_sm\")\n\n    try:\n        spacy_en = spacy.load(\"en_core_web_sm\")\n    except IOError:\n        os.system(\"python -m spacy download en_core_web_sm\")\n        spacy_en = spacy.load(\"en_core_web_sm\")\n\n    return spacy_de, spacy_en\n\n\n# %% id=\"t4BszXXJTsqL\" tags=[]\ndef tokenize(text, tokenizer):\n    return [tok.text for tok in tokenizer.tokenizer(text)]\n\n\ndef yield_tokens(data_iter, tokenizer, index):\n    for from_to_tuple in data_iter:\n        yield tokenizer(from_to_tuple[index])\n\n\n# %% id=\"jU3kVlV5okC-\" tags=[]\n\n\ndef build_vocabulary(spacy_de, spacy_en):\n    def tokenize_de(text):\n        return tokenize(text, spacy_de)\n\n    def tokenize_en(text):\n        return tokenize(text, spacy_en)\n\n    print(\"Building German Vocabulary ...\")\n    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n    vocab_src = build_vocab_from_iterator(\n        yield_tokens(train + val + test, tokenize_de, index=0),\n        min_freq=2,\n        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n    )\n\n    print(\"Building English Vocabulary ...\")\n    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n    vocab_tgt = build_vocab_from_iterator(\n        yield_tokens(train + val + test, tokenize_en, index=1),\n        min_freq=2,\n        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n    )\n\n    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n\n    return vocab_src, vocab_tgt\n\n\ndef load_vocab(spacy_de, spacy_en):\n    if not exists(\"vocab.pt\"):\n        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n    else:\n        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n    print(\"Finished.\\nVocabulary sizes:\")\n    print(len(vocab_src))\n    print(len(vocab_tgt))\n    return vocab_src, vocab_tgt\n\n\nif is_interactive_notebook():\n    # global variables used later in the script\n    spacy_de, spacy_en = show_example(load_tokenizers)\n    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])\n\n\n# %% [markdown] id=\"-l-TFwzfTsqL\"\n#\n# > Batching matters a ton for speed. We want to have very evenly\n# > divided batches, with absolutely minimal padding. To do this we\n# > have to hack a bit around the default torchtext batching. This\n# > code patches their default batching to make sure we search over\n# > enough sentences to find tight batches.\n\n# %% [markdown] id=\"kDEj-hCgokC-\" tags=[] jp-MarkdownHeadingCollapsed=true\n# ## Iterators\n\n# %% id=\"wGsIHFgOokC_\" tags=[]\ndef collate_batch(\n    batch,\n    src_pipeline,\n    tgt_pipeline,\n    src_vocab,\n    tgt_vocab,\n    device,\n    max_padding=128,\n    pad_id=2,\n):\n    bs_id = torch.tensor([0], device=device)  # <s> token id\n    eos_id = torch.tensor([1], device=device)  # </s> token id\n    src_list, tgt_list = [], []\n    for (_src, _tgt) in batch:\n        processed_src = torch.cat(\n            [\n                bs_id,\n                torch.tensor(\n                    src_vocab(src_pipeline(_src)),\n                    dtype=torch.int64,\n                    device=device,\n                ),\n                eos_id,\n            ],\n            0,\n        )\n        processed_tgt = torch.cat(\n            [\n                bs_id,\n                torch.tensor(\n                    tgt_vocab(tgt_pipeline(_tgt)),\n                    dtype=torch.int64,\n                    device=device,\n                ),\n                eos_id,\n            ],\n            0,\n        )\n        src_list.append(\n            # warning - overwrites values for negative values of padding - len\n            pad(\n                processed_src,\n                (\n                    0,\n                    max_padding - len(processed_src),\n                ),\n                value=pad_id,\n            )\n        )\n        tgt_list.append(\n            pad(\n                processed_tgt,\n                (0, max_padding - len(processed_tgt)),\n                value=pad_id,\n            )\n        )\n\n    src = torch.stack(src_list)\n    tgt = torch.stack(tgt_list)\n    return (src, tgt)\n\n\n# %% id=\"ka2Ce_WIokC_\" tags=[]\ndef create_dataloaders(\n    device,\n    vocab_src,\n    vocab_tgt,\n    spacy_de,\n    spacy_en,\n    batch_size=12000,\n    max_padding=128,\n    is_distributed=True,\n):\n    # def create_dataloaders(batch_size=12000):\n    def tokenize_de(text):\n        return tokenize(text, spacy_de)\n\n    def tokenize_en(text):\n        return tokenize(text, spacy_en)\n\n    def collate_fn(batch):\n        return collate_batch(\n            batch,\n            tokenize_de,\n            tokenize_en,\n            vocab_src,\n            vocab_tgt,\n            device,\n            max_padding=max_padding,\n            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n        )\n\n    train_iter, valid_iter, test_iter = datasets.Multi30k(\n        language_pair=(\"de\", \"en\")\n    )\n\n    train_iter_map = to_map_style_dataset(\n        train_iter\n    )  # DistributedSampler needs a dataset len()\n    train_sampler = (\n        DistributedSampler(train_iter_map) if is_distributed else None\n    )\n    valid_iter_map = to_map_style_dataset(valid_iter)\n    valid_sampler = (\n        DistributedSampler(valid_iter_map) if is_distributed else None\n    )\n\n    train_dataloader = DataLoader(\n        train_iter_map,\n        batch_size=batch_size,\n        shuffle=(train_sampler is None),\n        sampler=train_sampler,\n        collate_fn=collate_fn,\n    )\n    valid_dataloader = DataLoader(\n        valid_iter_map,\n        batch_size=batch_size,\n        shuffle=(valid_sampler is None),\n        sampler=valid_sampler,\n        collate_fn=collate_fn,\n    )\n    return train_dataloader, valid_dataloader\n\n\n# %% [markdown] id=\"90qM8RzCTsqM\"\n# ## Training the System\n\n# %%\ndef train_worker(\n    gpu,\n    ngpus_per_node,\n    vocab_src,\n    vocab_tgt,\n    spacy_de,\n    spacy_en,\n    config,\n    is_distributed=False,\n):\n    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n    torch.cuda.set_device(gpu)\n\n    pad_idx = vocab_tgt[\"<blank>\"]\n    d_model = 512\n    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n    model.cuda(gpu)\n    module = model\n    is_main_process = True\n    if is_distributed:\n        dist.init_process_group(\n            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n        )\n        model = DDP(model, device_ids=[gpu])\n        module = model.module\n        is_main_process = gpu == 0\n\n    criterion = LabelSmoothing(\n        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n    )\n    criterion.cuda(gpu)\n\n    train_dataloader, valid_dataloader = create_dataloaders(\n        gpu,\n        vocab_src,\n        vocab_tgt,\n        spacy_de,\n        spacy_en,\n        batch_size=config[\"batch_size\"] // ngpus_per_node,\n        max_padding=config[\"max_padding\"],\n        is_distributed=is_distributed,\n    )\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda=lambda step: rate(\n            step, d_model, factor=1, warmup=config[\"warmup\"]\n        ),\n    )\n    train_state = TrainState()\n\n    for epoch in range(config[\"num_epochs\"]):\n        if is_distributed:\n            train_dataloader.sampler.set_epoch(epoch)\n            valid_dataloader.sampler.set_epoch(epoch)\n\n        model.train()\n        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n        _, train_state = run_epoch(\n            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            optimizer,\n            lr_scheduler,\n            mode=\"train+log\",\n            accum_iter=config[\"accum_iter\"],\n            train_state=train_state,\n        )\n\n        GPUtil.showUtilization()\n        if is_main_process:\n            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n            torch.save(module.state_dict(), file_path)\n        torch.cuda.empty_cache()\n\n        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n        model.eval()\n        sloss = run_epoch(\n            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            DummyOptimizer(),\n            DummyScheduler(),\n            mode=\"eval\",\n        )\n        print(sloss)\n        torch.cuda.empty_cache()\n\n    if is_main_process:\n        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n        torch.save(module.state_dict(), file_path)\n\n\n# %% tags=[]\ndef train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n    from the_annotated_transformer import train_worker\n\n    ngpus = torch.cuda.device_count()\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12356\"\n    print(f\"Number of GPUs detected: {ngpus}\")\n    print(\"Spawning training processes ...\")\n    mp.spawn(\n        train_worker,\n        nprocs=ngpus,\n        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n    )\n\n\ndef train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n    if config[\"distributed\"]:\n        train_distributed_model(\n            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n        )\n    else:\n        train_worker(\n            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n        )\n\n\ndef load_trained_model():\n    config = {\n        \"batch_size\": 32,\n        \"distributed\": False,\n        \"num_epochs\": 8,\n        \"accum_iter\": 10,\n        \"base_lr\": 1.0,\n        \"max_padding\": 72,\n        \"warmup\": 3000,\n        \"file_prefix\": \"multi30k_model_\",\n    }\n    model_path = \"multi30k_model_final.pt\"\n    if not exists(model_path):\n        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n\n    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n    return model\n\n\nif is_interactive_notebook():\n    model = load_trained_model()\n\n\n# %% [markdown] id=\"RZK_VjDPTsqN\"\n#\n# > Once trained we can decode the model to produce a set of\n# > translations. Here we simply translate the first sentence in the\n# > validation set. This dataset is pretty small so the translations\n# > with greedy search are reasonably accurate.\n\n# %% [markdown] id=\"L50i0iEXTsqN\"\n# # Additional Components: BPE, Search, Averaging\n\n# %% [markdown] id=\"NBx1C2_NTsqN\"\n#\n# > So this mostly covers the transformer model itself. There are four\n# > aspects that we didn't cover explicitly. We also have all these\n# > additional features implemented in\n# > [OpenNMT-py](https://github.com/opennmt/opennmt-py).\n#\n#\n\n# %% [markdown] id=\"UpqV1mWnTsqN\"\n#\n# > 1) BPE/ Word-piece: We can use a library to first preprocess the\n# > data into subword units. See Rico Sennrich's\n# > [subword-nmt](https://github.com/rsennrich/subword-nmt)\n# > implementation. These models will transform the training data to\n# > look like this:\n\n# %% [markdown] id=\"hwJ_9J0BTsqN\"\n# ▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP\n# ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .\n\n# %% [markdown] id=\"9HwejYkpTsqN\"\n#\n# > 2) Shared Embeddings: When using BPE with shared vocabulary we can\n# > share the same weight vectors between the source / target /\n# > generator. See the [(cite)](https://arxiv.org/abs/1608.05859) for\n# > details. To add this to the model simply do this:\n\n# %% id=\"tb3j3CYLTsqN\" tags=[]\nif False:\n    model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight\n    model.generator.lut.weight = model.tgt_embed[0].lut.weight\n\n\n# %% [markdown] id=\"xDKJsSwRTsqN\"\n#\n# > 3) Beam Search: This is a bit too complicated to cover here. See the\n# > [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py/)\n# > for a pytorch implementation.\n# >\n#\n\n# %% [markdown] id=\"wf3vVYGZTsqN\"\n#\n# > 4) Model Averaging: The paper averages the last k checkpoints to\n# > create an ensembling effect. We can do this after the fact if we\n# > have a bunch of models:\n\n# %% id=\"hAFEa78JokDB\"\ndef average(model, models):\n    \"Average models into model\"\n    for ps in zip(*[m.params() for m in [model] + models]):\n        ps[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))\n\n\n# %% [markdown] id=\"Kz5BYJ9sTsqO\"\n# # Results\n#\n# On the WMT 2014 English-to-German translation task, the big\n# transformer model (Transformer (big) in Table 2) outperforms the\n# best previously reported models (including ensembles) by more than\n# 2.0 BLEU, establishing a new state-of-the-art BLEU score of\n# 28.4. The configuration of this model is listed in the bottom line\n# of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base\n# model surpasses all previously published models and ensembles, at a\n# fraction of the training cost of any of the competitive models.\n#\n# On the WMT 2014 English-to-French translation task, our big model\n# achieves a BLEU score of 41.0, outperforming all of the previously\n# published single models, at less than 1/4 the training cost of the\n# previous state-of-the-art model. The Transformer (big) model trained\n# for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n#\n\n# %% [markdown]\n# ![](images/results.png)\n\n# %% [markdown] id=\"cPcnsHvQTsqO\"\n#\n#\n# > With the addtional extensions in the last section, the OpenNMT-py\n# > replication gets to 26.9 on EN-DE WMT. Here I have loaded in those\n# > parameters to our reimplemenation.\n\n# %%\n# Load data and model for output checks\n\n\n# %%\ndef check_outputs(\n    valid_dataloader,\n    model,\n    vocab_src,\n    vocab_tgt,\n    n_examples=15,\n    pad_idx=2,\n    eos_string=\"</s>\",\n):\n    results = [()] * n_examples\n    for idx in range(n_examples):\n        print(\"\\nExample %d ========\\n\" % idx)\n        b = next(iter(valid_dataloader))\n        rb = Batch(b[0], b[1], pad_idx)\n        greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\n\n        src_tokens = [\n            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n        ]\n        tgt_tokens = [\n            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n        ]\n\n        print(\n            \"Source Text (Input)        : \"\n            + \" \".join(src_tokens).replace(\"\\n\", \"\")\n        )\n        print(\n            \"Target Text (Ground Truth) : \"\n            + \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n        )\n        model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n        model_txt = (\n            \" \".join(\n                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n            ).split(eos_string, 1)[0]\n            + eos_string\n        )\n        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n    return results\n\n\ndef run_model_example(n_examples=5):\n    global vocab_src, vocab_tgt, spacy_de, spacy_en\n\n    print(\"Preparing Data ...\")\n    _, valid_dataloader = create_dataloaders(\n        torch.device(\"cpu\"),\n        vocab_src,\n        vocab_tgt,\n        spacy_de,\n        spacy_en,\n        batch_size=1,\n        is_distributed=False,\n    )\n\n    print(\"Loading Trained Model ...\")\n\n    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n    model.load_state_dict(\n        torch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\"))\n    )\n\n    print(\"Checking Model Outputs:\")\n    example_data = check_outputs(\n        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n    )\n    return model, example_data\n\n\n# execute_example(run_model_example)\n\n\n# %% [markdown] id=\"0ZkkNTKLTsqO\"\n# ## Attention Visualization\n#\n# > Even with a greedy decoder the translation looks pretty good. We\n# > can further visualize it to see what is happening at each layer of\n# > the attention\n\n# %%\ndef mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n    \"convert a dense matrix to a data frame with row and column indices\"\n    return pd.DataFrame(\n        [\n            (\n                r,\n                c,\n                float(m[r, c]),\n                \"%.3d %s\"\n                % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n                \"%.3d %s\"\n                % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n            )\n            for r in range(m.shape[0])\n            for c in range(m.shape[1])\n            if r < max_row and c < max_col\n        ],\n        # if float(m[r,c]) != 0 and r < max_row and c < max_col],\n        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n    )\n\n\ndef attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30):\n    df = mtx2df(\n        attn[0, head].data,\n        max_dim,\n        max_dim,\n        row_tokens,\n        col_tokens,\n    )\n    return (\n        alt.Chart(data=df)\n        .mark_rect()\n        .encode(\n            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n            color=\"value\",\n            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n        )\n        .properties(height=400, width=400)\n        .interactive()\n    )\n\n\n# %% tags=[]\ndef get_encoder(model, layer):\n    return model.encoder.layers[layer].self_attn.attn\n\n\ndef get_decoder_self(model, layer):\n    return model.decoder.layers[layer].self_attn.attn\n\n\ndef get_decoder_src(model, layer):\n    return model.decoder.layers[layer].src_attn.attn\n\n\ndef visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens):\n    # ntokens = last_example[0].ntokens\n    attn = getter_fn(model, layer)\n    n_heads = attn.shape[1]\n    charts = [\n        attn_map(\n            attn,\n            0,\n            h,\n            row_tokens=row_tokens,\n            col_tokens=col_tokens,\n            max_dim=ntokens,\n        )\n        for h in range(n_heads)\n    ]\n    assert n_heads == 8\n    return alt.vconcat(\n        charts[0]\n        # | charts[1]\n        | charts[2]\n        # | charts[3]\n        | charts[4]\n        # | charts[5]\n        | charts[6]\n        # | charts[7]\n        # layer + 1 due to 0-indexing\n    ).properties(title=\"Layer %d\" % (layer + 1))\n\n\n# %% [markdown]\n# ## Encoder Self Attention\n\n# %% tags=[]\ndef viz_encoder_self():\n    model, example_data = run_model_example(n_examples=1)\n    example = example_data[\n        len(example_data) - 1\n    ]  # batch object for the final example\n\n    layer_viz = [\n        visualize_layer(\n            model, layer, get_encoder, len(example[1]), example[1], example[1]\n        )\n        for layer in range(6)\n    ]\n    return alt.hconcat(\n        layer_viz[0]\n        # & layer_viz[1]\n        & layer_viz[2]\n        # & layer_viz[3]\n        & layer_viz[4]\n        # & layer_viz[5]\n    )\n\n\nshow_example(viz_encoder_self)\n\n\n# %% [markdown]\n# ## Decoder Self Attention\n\n# %% tags=[]\ndef viz_decoder_self():\n    model, example_data = run_model_example(n_examples=1)\n    example = example_data[len(example_data) - 1]\n\n    layer_viz = [\n        visualize_layer(\n            model,\n            layer,\n            get_decoder_self,\n            len(example[1]),\n            example[1],\n            example[1],\n        )\n        for layer in range(6)\n    ]\n    return alt.hconcat(\n        layer_viz[0]\n        & layer_viz[1]\n        & layer_viz[2]\n        & layer_viz[3]\n        & layer_viz[4]\n        & layer_viz[5]\n    )\n\n\nshow_example(viz_decoder_self)\n\n\n# %% [markdown]\n# ## Decoder Src Attention\n\n# %% tags=[]\ndef viz_decoder_src():\n    model, example_data = run_model_example(n_examples=1)\n    example = example_data[len(example_data) - 1]\n\n    layer_viz = [\n        visualize_layer(\n            model,\n            layer,\n            get_decoder_src,\n            max(len(example[1]), len(example[2])),\n            example[1],\n            example[2],\n        )\n        for layer in range(6)\n    ]\n    return alt.hconcat(\n        layer_viz[0]\n        & layer_viz[1]\n        & layer_viz[2]\n        & layer_viz[3]\n        & layer_viz[4]\n        & layer_viz[5]\n    )\n\n\nshow_example(viz_decoder_src)\n\n# %% [markdown] id=\"nSseuCcATsqO\"\n# # Conclusion\n#\n#  Hopefully this code is useful for future research. Please reach\n#  out if you have any issues.\n#\n#\n#  Cheers,\n#  Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,\n#  Stella Biderman\n",
    "file-3": "\"\"\"\n================================================================================\nCRITICAL CODE STUDIES SAMPLE - TRANSFORMER ARCHITECTURE (2017)\n================================================================================\n\nImplementation: PyTorch Official (Production)\nFramework: PyTorch\nYear: Current version from PyTorch repository\nPurpose: Production-ready implementation used in research and industry\nAuthors: PyTorch core team\nSource: https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.py\nLicense: BSD 3-Clause\n\nThis is PyTorch's official Transformer implementation, representing the\nindustry-standard production version. Key features:\n- Optimized fast-path execution with nested tensors\n- Comprehensive mask handling (padding, attention, causal)\n- Flexible input formats (batch_first parameter)\n- Performance-focused with torch internals integration\n\nCompare to:\n- annotated_transformer.py (Harvard NLP pedagogical version)\n- transformer_tensorflow.py (TensorFlow/Google's original framework)\n\nNote: This file imports MultiheadAttention from .activation; see\nattention_pytorch.py for the actual attention implementation.\n\n================================================================================\n\"\"\"\n\n# mypy: allow-untyped-defs\nimport copy\nimport warnings\nfrom collections.abc import Callable\nfrom typing import Any\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn.init import xavier_uniform_\n\nfrom .activation import MultiheadAttention\nfrom .container import ModuleList\nfrom .dropout import Dropout\nfrom .linear import Linear\nfrom .module import Module\nfrom .normalization import LayerNorm\n\n\n__all__ = [\n    \"Transformer\",\n    \"TransformerEncoder\",\n    \"TransformerDecoder\",\n    \"TransformerEncoderLayer\",\n    \"TransformerDecoderLayer\",\n]\n\n\ndef _generate_square_subsequent_mask(\n    sz: int,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -> Tensor:\n    r\"\"\"Generate a square causal mask for the sequence.\n\n    The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\n    \"\"\"\n    return torch.triu(\n        torch.full((sz, sz), float(\"-inf\"), dtype=dtype, device=device),\n        diagonal=1,\n    )\n\n\ndef _get_seq_len(src: Tensor, batch_first: bool) -> int | None:\n    if src.is_nested:\n        return None\n    else:\n        src_size = src.size()\n        if len(src_size) == 2:\n            # unbatched: S, E\n            return src_size[0]\n        else:\n            # batched: B, S, E if batch_first else S, B, E\n            seq_len_pos = 1 if batch_first else 0\n            return src_size[seq_len_pos]\n\n\nclass Transformer(Module):\n    r\"\"\"A basic transformer layer.\n\n\n    This Transformer layer implements the original Transformer architecture described\n    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n    intent of this layer is as a reference implementation for foundational understanding\n    and thus it contains only limited features relative to newer Transformer architectures.\n    Given the fast pace of innovation in transformer-like architectures, we recommend\n    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n    to build an efficient transformer layer from building blocks in core or using higher\n    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\n    Args:\n        d_model: the number of expected features in the encoder/decoder inputs (default=512).\n        nhead: the number of heads in the multiheadattention models (default=8).\n        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of encoder/decoder intermediate layer, can be a string\n            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n        custom_encoder: custom encoder (default=None).\n        custom_decoder: custom decoder (default=None).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n        norm_first: if ``True``, encoder and decoder layers will perform LayerNorms before\n            other attention and feedforward operations, otherwise after. Default: ``False`` (after).\n        bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n            bias. Default: ``True``.\n\n    Examples:\n        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n        >>> src = torch.rand((10, 32, 512))\n        >>> tgt = torch.rand((20, 32, 512))\n        >>> out = transformer_model(src, tgt)\n\n    Note: A full example to apply nn.Transformer module for the word language model is available in\n    https://github.com/pytorch/examples/tree/master/word_language_model\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 512,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dim_feedforward: int = 2048,\n        dropout: float = 0.1,\n        activation: str | Callable[[Tensor], Tensor] = F.relu,\n        custom_encoder: Any | None = None,\n        custom_decoder: Any | None = None,\n        layer_norm_eps: float = 1e-5,\n        batch_first: bool = False,\n        norm_first: bool = False,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n\n        if custom_encoder is not None:\n            self.encoder = custom_encoder\n        else:\n            encoder_layer = TransformerEncoderLayer(\n                d_model,\n                nhead,\n                dim_feedforward,\n                dropout,\n                activation,\n                layer_norm_eps,\n                batch_first,\n                norm_first,\n                bias,\n                **factory_kwargs,\n            )\n            encoder_norm = LayerNorm(\n                d_model,\n                eps=layer_norm_eps,\n                bias=bias,\n                # pyrefly: ignore [bad-argument-type]\n                **factory_kwargs,\n            )\n            self.encoder = TransformerEncoder(\n                encoder_layer, num_encoder_layers, encoder_norm\n            )\n\n        if custom_decoder is not None:\n            self.decoder = custom_decoder\n        else:\n            decoder_layer = TransformerDecoderLayer(\n                d_model,\n                nhead,\n                dim_feedforward,\n                dropout,\n                activation,\n                layer_norm_eps,\n                batch_first,\n                norm_first,\n                bias,\n                **factory_kwargs,\n            )\n            decoder_norm = LayerNorm(\n                d_model,\n                eps=layer_norm_eps,\n                bias=bias,\n                # pyrefly: ignore [bad-argument-type]\n                **factory_kwargs,\n            )\n            self.decoder = TransformerDecoder(\n                decoder_layer, num_decoder_layers, decoder_norm\n            )\n\n        self._reset_parameters()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n        self.batch_first = batch_first\n\n    def forward(\n        self,\n        src: Tensor,\n        tgt: Tensor,\n        src_mask: Tensor | None = None,\n        tgt_mask: Tensor | None = None,\n        memory_mask: Tensor | None = None,\n        src_key_padding_mask: Tensor | None = None,\n        tgt_key_padding_mask: Tensor | None = None,\n        memory_key_padding_mask: Tensor | None = None,\n        src_is_causal: bool | None = None,\n        tgt_is_causal: bool | None = None,\n        memory_is_causal: bool = False,\n    ) -> Tensor:\n        r\"\"\"Take in and process masked source/target sequences.\n\n        .. note::\n\n            If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a ``True`` value are\n            not allowed to participate in the attention,\n            which is the opposite of the definition for :attr:`attn_mask`\n            in :func:`torch.nn.functional.scaled_dot_product_attention`.\n\n        Args:\n            src: the sequence to the encoder (required).\n            tgt: the sequence to the decoder (required).\n            src_mask: the additive mask for the src sequence (optional).\n            tgt_mask: the additive mask for the tgt sequence (optional).\n            memory_mask: the additive mask for the encoder output (optional).\n            src_key_padding_mask: the Tensor mask for src keys per batch (optional).\n            tgt_key_padding_mask: the Tensor mask for tgt keys per batch (optional).\n            memory_key_padding_mask: the Tensor mask for memory keys per batch (optional).\n            src_is_causal: If specified, applies a causal mask as ``src_mask``.\n                Default: ``None``; try to detect a causal mask.\n                Warning:\n                ``src_is_causal`` provides a hint that ``src_mask`` is\n                the causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n            tgt_is_causal: If specified, applies a causal mask as ``tgt_mask``.\n                Default: ``None``; try to detect a causal mask.\n                Warning:\n                ``tgt_is_causal`` provides a hint that ``tgt_mask`` is\n                the causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n            memory_is_causal: If specified, applies a causal mask as\n                ``memory_mask``.\n                Default: ``False``.\n                Warning:\n                ``memory_is_causal`` provides a hint that\n                ``memory_mask`` is the causal mask. Providing incorrect\n                hints can result in incorrect execution, including\n                forward and backward compatibility.\n\n        Shape:\n            - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\n              `(N, S, E)` if `batch_first=True`.\n            - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n              `(N, T, E)` if `batch_first=True`.\n            - src_mask: :math:`(S, S)` or :math:`(N\\cdot\\text{num\\_heads}, S, S)`.\n            - tgt_mask: :math:`(T, T)` or :math:`(N\\cdot\\text{num\\_heads}, T, T)`.\n            - memory_mask: :math:`(T, S)`.\n            - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n            - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\n            - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n\n            Note: [src/tgt/memory]_mask ensures that position :math:`i` is allowed to attend the unmasked\n            positions. If a BoolTensor is provided, positions with ``True``\n            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n            the attention. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n\n            - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n              `(N, T, E)` if `batch_first=True`.\n\n            Note: Due to the multi-head attention architecture in the transformer model,\n            the output sequence length of a transformer is same as the input sequence\n            (i.e. target) length of the decoder.\n\n            where :math:`S` is the source sequence length, :math:`T` is the target sequence length, :math:`N` is the\n            batch size, :math:`E` is the feature number\n\n        Examples:\n            >>> # xdoctest: +SKIP\n            >>> output = transformer_model(\n            ...     src, tgt, src_mask=src_mask, tgt_mask=tgt_mask\n            ... )\n        \"\"\"\n        is_batched = src.dim() == 3\n        if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:\n            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n        elif self.batch_first and src.size(0) != tgt.size(0) and is_batched:\n            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n\n        if src.size(-1) != self.d_model or tgt.size(-1) != self.d_model:\n            raise RuntimeError(\n                \"the feature number of src and tgt must be equal to d_model\"\n            )\n\n        memory = self.encoder(\n            src,\n            mask=src_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            is_causal=src_is_causal,\n        )\n        output = self.decoder(\n            tgt,\n            memory,\n            tgt_mask=tgt_mask,\n            memory_mask=memory_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask,\n            memory_key_padding_mask=memory_key_padding_mask,\n            tgt_is_causal=tgt_is_causal,\n            memory_is_causal=memory_is_causal,\n        )\n        return output\n\n    @staticmethod\n    def generate_square_subsequent_mask(\n        sz: int,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -> Tensor:\n        r\"\"\"Generate a square causal mask for the sequence.\n\n        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\n        \"\"\"\n        return _generate_square_subsequent_mask(sz, dtype=dtype, device=device)\n\n    def _reset_parameters(self) -> None:\n        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                xavier_uniform_(p)\n\n\nclass TransformerEncoder(Module):\n    r\"\"\"TransformerEncoder is a stack of N encoder layers.\n\n    This TransformerEncoder layer implements the original architecture described\n    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n    intent of this layer is as a reference implementation for foundational understanding\n    and thus it contains only limited features relative to newer Transformer architectures.\n    Given the fast pace of innovation in transformer-like architectures, we recommend\n    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n    to build efficient layers from building blocks in core or using higher\n    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\n    .. warning::\n        All layers in the TransformerEncoder are initialized with the same parameters.\n        It is recommended to manually initialize the layers after creating the TransformerEncoder instance.\n\n    Args:\n        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n        num_layers: the number of sub-encoder-layers in the encoder (required).\n        norm: the layer normalization component (optional).\n        enable_nested_tensor: if True, input will automatically convert to nested tensor\n            (and convert back on output). This will improve the overall performance of\n            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).\n\n    Examples:\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n        >>> src = torch.rand(10, 32, 512)\n        >>> out = transformer_encoder(src)\n    \"\"\"\n\n    __constants__ = [\"norm\"]\n\n    def __init__(\n        self,\n        encoder_layer: \"TransformerEncoderLayer\",\n        num_layers: int,\n        norm: Module | None = None,\n        enable_nested_tensor: bool = True,\n        mask_check: bool = True,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        # this attribute saves the value providedat object construction\n        self.enable_nested_tensor = enable_nested_tensor\n        # this attribute controls whether nested tensors are used\n        self.use_nested_tensor = enable_nested_tensor\n        self.mask_check = mask_check\n\n        enc_layer = \"encoder_layer\"\n        why_not_sparsity_fast_path = \"\"\n        if not isinstance(encoder_layer, torch.nn.TransformerEncoderLayer):\n            why_not_sparsity_fast_path = f\"{enc_layer} was not TransformerEncoderLayer\"\n        elif encoder_layer.norm_first:\n            why_not_sparsity_fast_path = f\"{enc_layer}.norm_first was True\"\n        elif not encoder_layer.self_attn.batch_first:\n            why_not_sparsity_fast_path = (\n                f\"{enc_layer}.self_attn.batch_first was not True\"\n                + \"(use batch_first for better inference performance)\"\n            )\n        elif not encoder_layer.self_attn._qkv_same_embed_dim:\n            why_not_sparsity_fast_path = (\n                f\"{enc_layer}.self_attn._qkv_same_embed_dim was not True\"\n            )\n        elif encoder_layer.self_attn.in_proj_bias is None:\n            why_not_sparsity_fast_path = f\"{enc_layer}.self_attn was passed bias=False\"\n        elif not encoder_layer.activation_relu_or_gelu:\n            why_not_sparsity_fast_path = (\n                f\"{enc_layer}.activation_relu_or_gelu was not True\"\n            )\n        elif encoder_layer.norm1.eps != encoder_layer.norm2.eps:\n            why_not_sparsity_fast_path = (\n                f\"{enc_layer}.norm1.eps was not equal to {enc_layer}.norm2.eps\"\n            )\n        elif encoder_layer.self_attn.num_heads % 2 == 1:\n            why_not_sparsity_fast_path = f\"{enc_layer}.self_attn.num_heads is odd\"\n\n        if enable_nested_tensor and why_not_sparsity_fast_path:\n            warnings.warn(\n                f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\",\n                stacklevel=2,\n            )\n            self.use_nested_tensor = False\n\n    def forward(\n        self,\n        src: Tensor,\n        mask: Tensor | None = None,\n        src_key_padding_mask: Tensor | None = None,\n        is_causal: bool | None = None,\n    ) -> Tensor:\n        r\"\"\"Pass the input through the encoder layers in turn.\n\n        Args:\n            src: the sequence to the encoder (required).\n            mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n            is_causal: If specified, applies a causal mask as ``mask``.\n                Default: ``None``; try to detect a causal mask.\n                Warning:\n                ``is_causal`` provides a hint that ``mask`` is the\n                causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n\n        Shape:\n            see the docs in :class:`~torch.nn.Transformer`.\n        \"\"\"\n        src_key_padding_mask = F._canonical_mask(\n            mask=src_key_padding_mask,\n            mask_name=\"src_key_padding_mask\",\n            other_type=F._none_or_dtype(mask),\n            other_name=\"mask\",\n            target_type=src.dtype,\n        )\n\n        mask = F._canonical_mask(\n            mask=mask,\n            mask_name=\"mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=src.dtype,\n            check_other=False,\n        )\n\n        output = src\n        convert_to_nested = False\n        first_layer = self.layers[0]\n        src_key_padding_mask_for_layers = src_key_padding_mask\n        why_not_sparsity_fast_path = \"\"\n        str_first_layer = \"self.layers[0]\"\n        batch_first = first_layer.self_attn.batch_first\n        is_fastpath_enabled = torch.backends.mha.get_fastpath_enabled()\n        do_mask_check = getattr(self, \"mask_check\", True)\n\n        if not is_fastpath_enabled:\n            why_not_sparsity_fast_path = (\n                \"torch.backends.mha.get_fastpath_enabled() was not True\"\n            )\n        elif not hasattr(self, \"use_nested_tensor\"):\n            why_not_sparsity_fast_path = \"use_nested_tensor attribute not present\"\n        elif not self.use_nested_tensor:\n            why_not_sparsity_fast_path = (\n                \"self.use_nested_tensor (set in init) was not True\"\n            )\n        elif first_layer.training:\n            why_not_sparsity_fast_path = f\"{str_first_layer} was in training mode\"\n        elif src.dim() != 3:\n            why_not_sparsity_fast_path = (\n                f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n            )\n        elif src_key_padding_mask is None:\n            why_not_sparsity_fast_path = \"src_key_padding_mask was None\"\n        # This check avoids a call to torch._nested_tensor_from_mask_left_aligned() that\n        # breaks in torch.compile.\n        elif do_mask_check and torch.compiler.is_compiling():\n            why_not_sparsity_fast_path = (\n                \"mask_check enabled with torch.compile or torch.export\"\n            )\n        elif do_mask_check and not torch._nested_tensor_from_mask_left_aligned(\n            src, src_key_padding_mask.logical_not()\n        ):\n            why_not_sparsity_fast_path = \"mask_check enabled, and src and src_key_padding_mask was not left aligned\"\n        elif output.is_nested:\n            why_not_sparsity_fast_path = \"NestedTensor input is not supported\"\n        elif mask is not None:\n            why_not_sparsity_fast_path = (\n                \"src_key_padding_mask and mask were both supplied\"\n            )\n        elif torch.is_autocast_enabled():\n            why_not_sparsity_fast_path = \"autocast is enabled\"\n\n        if not why_not_sparsity_fast_path:\n            tensor_args = (\n                src,\n                first_layer.self_attn.in_proj_weight,\n                first_layer.self_attn.in_proj_bias,\n                first_layer.self_attn.out_proj.weight,\n                first_layer.self_attn.out_proj.bias,\n                first_layer.norm1.weight,\n                first_layer.norm1.bias,\n                first_layer.norm2.weight,\n                first_layer.norm2.bias,\n                first_layer.linear1.weight,\n                first_layer.linear1.bias,\n                first_layer.linear2.weight,\n                first_layer.linear2.bias,\n            )\n            _supported_device_type = [\n                \"cpu\",\n                \"cuda\",\n                \"xpu\",\n                torch.utils.backend_registration._privateuse1_backend_name,\n            ]\n            if torch.overrides.has_torch_function(tensor_args):\n                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n            elif src.device.type not in _supported_device_type:\n                why_not_sparsity_fast_path = (\n                    f\"src device is neither one of {_supported_device_type}\"\n                )\n            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n                why_not_sparsity_fast_path = (\n                    \"grad is enabled and at least one of query or the \"\n                    \"input/output projection weights or biases requires_grad\"\n                )\n\n            if (not why_not_sparsity_fast_path) and (src_key_padding_mask is not None):\n                convert_to_nested = True\n                output = torch._nested_tensor_from_mask(\n                    output, src_key_padding_mask.logical_not(), mask_check=False\n                )\n                src_key_padding_mask_for_layers = None\n\n        seq_len = _get_seq_len(src, batch_first)\n        is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\n        for mod in self.layers:\n            output = mod(\n                output,\n                src_mask=mask,\n                is_causal=is_causal,\n                src_key_padding_mask=src_key_padding_mask_for_layers,\n            )\n\n        if convert_to_nested:\n            output = output.to_padded_tensor(0.0, src.size())\n\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output\n\n\nclass TransformerDecoder(Module):\n    r\"\"\"TransformerDecoder is a stack of N decoder layers.\n\n    This TransformerDecoder layer implements the original architecture described\n    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n    intent of this layer is as a reference implementation for foundational understanding\n    and thus it contains only limited features relative to newer Transformer architectures.\n    Given the fast pace of innovation in transformer-like architectures, we recommend\n    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n    to build efficient layers from building blocks in core or using higher\n    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\n    .. warning::\n        All layers in the TransformerDecoder are initialized with the same parameters.\n        It is recommended to manually initialize the layers after creating the TransformerDecoder instance.\n\n    Args:\n        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n        num_layers: the number of sub-decoder-layers in the decoder (required).\n        norm: the layer normalization component (optional).\n\n    Examples:\n        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n        >>> memory = torch.rand(10, 32, 512)\n        >>> tgt = torch.rand(20, 32, 512)\n        >>> out = transformer_decoder(tgt, memory)\n    \"\"\"\n\n    __constants__ = [\"norm\"]\n\n    def __init__(\n        self,\n        decoder_layer: \"TransformerDecoderLayer\",\n        num_layers: int,\n        norm: Module | None = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(\n        self,\n        tgt: Tensor,\n        memory: Tensor,\n        tgt_mask: Tensor | None = None,\n        memory_mask: Tensor | None = None,\n        tgt_key_padding_mask: Tensor | None = None,\n        memory_key_padding_mask: Tensor | None = None,\n        tgt_is_causal: bool | None = None,\n        memory_is_causal: bool = False,\n    ) -> Tensor:\n        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n\n        Args:\n            tgt: the sequence to the decoder (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n            tgt_is_causal: If specified, applies a causal mask as ``tgt mask``.\n                Default: ``None``; try to detect a causal mask.\n                Warning:\n                ``tgt_is_causal`` provides a hint that ``tgt_mask`` is\n                the causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n            memory_is_causal: If specified, applies a causal mask as\n                ``memory mask``.\n                Default: ``False``.\n                Warning:\n                ``memory_is_causal`` provides a hint that\n                ``memory_mask`` is the causal mask. Providing incorrect\n                hints can result in incorrect execution, including\n                forward and backward compatibility.\n\n        Shape:\n            see the docs in :class:`~torch.nn.Transformer`.\n        \"\"\"\n        output = tgt\n\n        seq_len = _get_seq_len(tgt, self.layers[0].self_attn.batch_first)\n        tgt_is_causal = _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\n        for mod in self.layers:\n            output = mod(\n                output,\n                memory,\n                tgt_mask=tgt_mask,\n                memory_mask=memory_mask,\n                tgt_key_padding_mask=tgt_key_padding_mask,\n                memory_key_padding_mask=memory_key_padding_mask,\n                tgt_is_causal=tgt_is_causal,\n                memory_is_causal=memory_is_causal,\n            )\n\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output\n\n\nclass TransformerEncoderLayer(Module):\n    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n\n    This TransformerEncoderLayer implements the original architecture described\n    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n    intent of this layer is as a reference implementation for foundational understanding\n    and thus it contains only limited features relative to newer Transformer architectures.\n    Given the fast pace of innovation in transformer-like architectures, we recommend\n    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n    to build efficient layers from building blocks in core or using higher\n    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\n    TransformerEncoderLayer can handle either traditional torch.tensor inputs,\n    or Nested Tensor inputs.  Derived classes are expected to similarly accept\n    both input formats.  (Not all combinations of inputs are currently\n    supported by TransformerEncoderLayer while Nested Tensor is in prototype\n    state.)\n\n    If you are implementing a custom layer, you may derive it either from\n    the Module or TransformerEncoderLayer class.  If your custom layer\n    supports both torch.Tensors and Nested Tensors inputs, make its\n    implementation a derived class of TransformerEncoderLayer. If your custom\n    Layer supports only torch.Tensor inputs, derive its implementation from\n    Module.\n\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of the intermediate layer, can be a string\n            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectively. Otherwise it's done after. Default: ``False`` (after).\n        bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n            bias. Default: ``True``.\n\n    Examples:\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n        >>> src = torch.rand(10, 32, 512)\n        >>> out = encoder_layer(src)\n\n    Alternatively, when ``batch_first`` is ``True``:\n        >>> encoder_layer = nn.TransformerEncoderLayer(\n        ...     d_model=512, nhead=8, batch_first=True\n        ... )\n        >>> src = torch.rand(32, 10, 512)\n        >>> out = encoder_layer(src)\n\n    Fast path:\n        forward() will use a special optimized implementation described in\n        `FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`_ if all of the following\n        conditions are met:\n\n        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor\n          argument ``requires_grad``\n        - training is disabled (using ``.eval()``)\n        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)\n        - activation is one of: ``\"relu\"``, ``\"gelu\"``, ``torch.functional.relu``, or ``torch.functional.gelu``\n        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed\n        - if src is a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_, neither ``src_mask``\n          nor ``src_key_padding_mask`` is passed\n        - the two ``LayerNorm`` instances have a consistent ``eps`` value (this will naturally be the case\n          unless the caller has manually modified one without modifying the other)\n\n        If the optimized implementation is in use, a\n        `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be\n        passed for ``src`` to represent padding more efficiently than using a padding\n        mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ will be\n        returned, and an additional speedup proportional to the fraction of the input that\n        is padding can be expected.\n\n        .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n         https://arxiv.org/abs/2205.14135\n\n    \"\"\"\n\n    __constants__ = [\"norm_first\"]\n\n    def __init__(\n        self,\n        d_model: int,\n        nhead: int,\n        dim_feedforward: int = 2048,\n        dropout: float = 0.1,\n        activation: str | Callable[[Tensor], Tensor] = F.relu,\n        layer_norm_eps: float = 1e-5,\n        batch_first: bool = False,\n        norm_first: bool = False,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.self_attn = MultiheadAttention(\n            d_model,\n            nhead,\n            dropout=dropout,\n            bias=bias,\n            batch_first=batch_first,\n            **factory_kwargs,\n        )\n        # Implementation of Feedforward model\n        self.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n        self.dropout = Dropout(dropout)\n        self.linear2 = Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n\n        self.norm_first = norm_first\n        # pyrefly: ignore [bad-argument-type]\n        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        # pyrefly: ignore [bad-argument-type]\n        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n\n        # Legacy string support for activation function.\n        if isinstance(activation, str):\n            activation = _get_activation_fn(activation)\n\n        # We can't test self.activation in forward() in TorchScript,\n        # so stash some information about it instead.\n        if activation is F.relu or isinstance(activation, torch.nn.ReLU):\n            self.activation_relu_or_gelu = 1\n        elif activation is F.gelu or isinstance(activation, torch.nn.GELU):\n            self.activation_relu_or_gelu = 2\n        else:\n            self.activation_relu_or_gelu = 0\n        self.activation = activation\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"activation\"):\n            self.activation = F.relu\n\n    def forward(\n        self,\n        src: Tensor,\n        src_mask: Tensor | None = None,\n        src_key_padding_mask: Tensor | None = None,\n        is_causal: bool = False,\n    ) -> Tensor:\n        r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n            is_causal: If specified, applies a causal mask as ``src mask``.\n                Default: ``False``.\n                Warning:\n                ``is_causal`` provides a hint that ``src_mask`` is the\n                causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n\n        Shape:\n            see the docs in :class:`~torch.nn.Transformer`.\n        \"\"\"\n        src_key_padding_mask = F._canonical_mask(\n            mask=src_key_padding_mask,\n            mask_name=\"src_key_padding_mask\",\n            other_type=F._none_or_dtype(src_mask),\n            other_name=\"src_mask\",\n            target_type=src.dtype,\n        )\n\n        src_mask = F._canonical_mask(\n            mask=src_mask,\n            mask_name=\"src_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=src.dtype,\n            check_other=False,\n        )\n\n        is_fastpath_enabled = torch.backends.mha.get_fastpath_enabled()\n\n        why_not_sparsity_fast_path = \"\"\n        if not is_fastpath_enabled:\n            why_not_sparsity_fast_path = (\n                \"torch.backends.mha.get_fastpath_enabled() was not True\"\n            )\n        elif src.dim() != 3:\n            why_not_sparsity_fast_path = (\n                f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n            )\n        elif self.training:\n            why_not_sparsity_fast_path = \"training is enabled\"\n        elif not self.self_attn.batch_first:\n            why_not_sparsity_fast_path = \"self_attn.batch_first was not True\"\n        elif self.self_attn.in_proj_bias is None:\n            why_not_sparsity_fast_path = \"self_attn was passed bias=False\"\n        elif not self.self_attn._qkv_same_embed_dim:\n            why_not_sparsity_fast_path = \"self_attn._qkv_same_embed_dim was not True\"\n        elif not self.activation_relu_or_gelu:\n            why_not_sparsity_fast_path = \"activation_relu_or_gelu was not True\"\n        elif self.norm1.eps != self.norm2.eps:\n            why_not_sparsity_fast_path = \"norm1.eps is not equal to norm2.eps\"\n        elif src.is_nested and (\n            src_key_padding_mask is not None or src_mask is not None\n        ):\n            why_not_sparsity_fast_path = \"neither src_key_padding_mask nor src_mask are not supported with NestedTensor input\"\n        elif self.self_attn.num_heads % 2 == 1:\n            why_not_sparsity_fast_path = \"num_head is odd\"\n        elif torch.is_autocast_enabled():\n            why_not_sparsity_fast_path = \"autocast is enabled\"\n        elif any(\n            len(getattr(m, \"_forward_hooks\", {}))\n            + len(getattr(m, \"_forward_pre_hooks\", {}))\n            for m in self.modules()\n        ):\n            why_not_sparsity_fast_path = \"forward pre-/hooks are attached to the module\"\n        if not why_not_sparsity_fast_path:\n            tensor_args = (\n                src,\n                self.self_attn.in_proj_weight,\n                self.self_attn.in_proj_bias,\n                self.self_attn.out_proj.weight,\n                self.self_attn.out_proj.bias,\n                self.norm1.weight,\n                self.norm1.bias,\n                self.norm2.weight,\n                self.norm2.bias,\n                self.linear1.weight,\n                self.linear1.bias,\n                self.linear2.weight,\n                self.linear2.bias,\n            )\n\n            # We have to use list comprehensions below because TorchScript does not support\n            # generator expressions.\n            _supported_device_type = [\n                \"cpu\",\n                \"cuda\",\n                \"xpu\",\n                torch.utils.backend_registration._privateuse1_backend_name,\n            ]\n            if torch.overrides.has_torch_function(tensor_args):\n                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n            elif not all(\n                (x.device.type in _supported_device_type) for x in tensor_args\n            ):\n                why_not_sparsity_fast_path = (\n                    \"some Tensor argument's device is neither one of \"\n                    f\"{_supported_device_type}\"\n                )\n            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n                why_not_sparsity_fast_path = (\n                    \"grad is enabled and at least one of query or the \"\n                    \"input/output projection weights or biases requires_grad\"\n                )\n\n            if not why_not_sparsity_fast_path:\n                merged_mask, mask_type = self.self_attn.merge_masks(\n                    src_mask, src_key_padding_mask, src\n                )\n                return torch._transformer_encoder_layer_fwd(\n                    src,\n                    self.self_attn.embed_dim,\n                    self.self_attn.num_heads,\n                    self.self_attn.in_proj_weight,\n                    self.self_attn.in_proj_bias,\n                    self.self_attn.out_proj.weight,\n                    self.self_attn.out_proj.bias,\n                    self.activation_relu_or_gelu == 2,\n                    self.norm_first,\n                    self.norm1.eps,\n                    self.norm1.weight,\n                    self.norm1.bias,\n                    self.norm2.weight,\n                    self.norm2.bias,\n                    self.linear1.weight,\n                    self.linear1.bias,\n                    self.linear2.weight,\n                    self.linear2.bias,\n                    merged_mask,\n                    mask_type,\n                )\n\n        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n        x = src\n        if self.norm_first:\n            x = x + self._sa_block(\n                self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal\n            )\n            x = x + self._ff_block(self.norm2(x))\n        else:\n            x = self.norm1(\n                x\n                + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n            )\n            x = self.norm2(x + self._ff_block(x))\n\n        return x\n\n    # self-attention block\n    def _sa_block(\n        self,\n        x: Tensor,\n        attn_mask: Tensor | None,\n        key_padding_mask: Tensor | None,\n        is_causal: bool = False,\n    ) -> Tensor:\n        x = self.self_attn(\n            x,\n            x,\n            x,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n            need_weights=False,\n            is_causal=is_causal,\n        )[0]\n        return self.dropout1(x)\n\n    # feed forward block\n    def _ff_block(self, x: Tensor) -> Tensor:\n        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n        return self.dropout2(x)\n\n\nclass TransformerDecoderLayer(Module):\n    r\"\"\"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n\n    This TransformerDecoderLayer implements the original architecture described\n    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n    intent of this layer is as a reference implementation for foundational understanding\n    and thus it contains only limited features relative to newer Transformer architectures.\n    Given the fast pace of innovation in transformer-like architectures, we recommend\n    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n    to build efficient layers from building blocks in core or using higher\n    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of the intermediate layer, can be a string\n            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n        norm_first: if ``True``, layer norm is done prior to self attention, multihead\n            attention and feedforward operations, respectively. Otherwise it's done after.\n            Default: ``False`` (after).\n        bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n            bias. Default: ``True``.\n\n    Examples:\n        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n        >>> memory = torch.rand(10, 32, 512)\n        >>> tgt = torch.rand(20, 32, 512)\n        >>> out = decoder_layer(tgt, memory)\n\n    Alternatively, when ``batch_first`` is ``True``:\n        >>> decoder_layer = nn.TransformerDecoderLayer(\n        ...     d_model=512, nhead=8, batch_first=True\n        ... )\n        >>> memory = torch.rand(32, 10, 512)\n        >>> tgt = torch.rand(32, 20, 512)\n        >>> out = decoder_layer(tgt, memory)\n    \"\"\"\n\n    __constants__ = [\"norm_first\"]\n\n    def __init__(\n        self,\n        d_model: int,\n        nhead: int,\n        dim_feedforward: int = 2048,\n        dropout: float = 0.1,\n        activation: str | Callable[[Tensor], Tensor] = F.relu,\n        layer_norm_eps: float = 1e-5,\n        batch_first: bool = False,\n        norm_first: bool = False,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.self_attn = MultiheadAttention(\n            d_model,\n            nhead,\n            dropout=dropout,\n            batch_first=batch_first,\n            bias=bias,\n            **factory_kwargs,\n        )\n        self.multihead_attn = MultiheadAttention(\n            d_model,\n            nhead,\n            dropout=dropout,\n            batch_first=batch_first,\n            bias=bias,\n            **factory_kwargs,\n        )\n        # Implementation of Feedforward model\n        self.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n        self.dropout = Dropout(dropout)\n        self.linear2 = Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n\n        self.norm_first = norm_first\n        # pyrefly: ignore [bad-argument-type]\n        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        # pyrefly: ignore [bad-argument-type]\n        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        # pyrefly: ignore [bad-argument-type]\n        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n        self.dropout3 = Dropout(dropout)\n\n        # Legacy string support for activation function.\n        if isinstance(activation, str):\n            self.activation = _get_activation_fn(activation)\n        else:\n            self.activation = activation\n\n    def __setstate__(self, state):\n        if \"activation\" not in state:\n            state[\"activation\"] = F.relu\n        super().__setstate__(state)\n\n    def forward(\n        self,\n        tgt: Tensor,\n        memory: Tensor,\n        tgt_mask: Tensor | None = None,\n        memory_mask: Tensor | None = None,\n        tgt_key_padding_mask: Tensor | None = None,\n        memory_key_padding_mask: Tensor | None = None,\n        tgt_is_causal: bool = False,\n        memory_is_causal: bool = False,\n    ) -> Tensor:\n        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n\n        Args:\n            tgt: the sequence to the decoder layer (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n            tgt_is_causal: If specified, applies a causal mask as ``tgt mask``.\n                Default: ``False``.\n                Warning:\n                ``tgt_is_causal`` provides a hint that ``tgt_mask`` is\n                the causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n            memory_is_causal: If specified, applies a causal mask as\n                ``memory mask``.\n                Default: ``False``.\n                Warning:\n                ``memory_is_causal`` provides a hint that\n                ``memory_mask`` is the causal mask. Providing incorrect\n                hints can result in incorrect execution, including\n                forward and backward compatibility.\n\n        Shape:\n            see the docs in :class:`~torch.nn.Transformer`.\n        \"\"\"\n        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n\n        x = tgt\n        if self.norm_first:\n            x = x + self._sa_block(\n                self.norm1(x), tgt_mask, tgt_key_padding_mask, tgt_is_causal\n            )\n            x = x + self._mha_block(\n                self.norm2(x),\n                memory,\n                memory_mask,\n                memory_key_padding_mask,\n                memory_is_causal,\n            )\n            x = x + self._ff_block(self.norm3(x))\n        else:\n            x = self.norm1(\n                x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n            )\n            x = self.norm2(\n                x\n                + self._mha_block(\n                    x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n                )\n            )\n            x = self.norm3(x + self._ff_block(x))\n\n        return x\n\n    # self-attention block\n    def _sa_block(\n        self,\n        x: Tensor,\n        attn_mask: Tensor | None,\n        key_padding_mask: Tensor | None,\n        is_causal: bool = False,\n    ) -> Tensor:\n        x = self.self_attn(\n            x,\n            x,\n            x,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n            is_causal=is_causal,\n            need_weights=False,\n        )[0]\n        return self.dropout1(x)\n\n    # multihead attention block\n    def _mha_block(\n        self,\n        x: Tensor,\n        mem: Tensor,\n        attn_mask: Tensor | None,\n        key_padding_mask: Tensor | None,\n        is_causal: bool = False,\n    ) -> Tensor:\n        x = self.multihead_attn(\n            x,\n            mem,\n            mem,\n            attn_mask=attn_mask,\n            key_padding_mask=key_padding_mask,\n            is_causal=is_causal,\n            need_weights=False,\n        )[0]\n        return self.dropout2(x)\n\n    # feed forward block\n    def _ff_block(self, x: Tensor) -> Tensor:\n        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n        return self.dropout3(x)\n\n\ndef _get_clones(module, N):\n    # FIXME: copy.deepcopy() is not defined on nn.module\n    return ModuleList([copy.deepcopy(module) for i in range(N)])\n\n\ndef _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return F.gelu\n\n    raise RuntimeError(f\"activation should be relu/gelu, not {activation}\")\n\n\ndef _detect_is_causal_mask(\n    mask: Tensor | None,\n    is_causal: bool | None = None,\n    size: int | None = None,\n) -> bool:\n    \"\"\"Return whether the given attention mask is causal.\n\n    Warning:\n    If ``is_causal`` is not ``None``, its value will be returned as is.  If a\n    user supplies an incorrect ``is_causal`` hint,\n\n    ``is_causal=False`` when the mask is in fact a causal attention.mask\n       may lead to reduced performance relative to what would be achievable\n       with ``is_causal=True``;\n    ``is_causal=True`` when the mask is in fact not a causal attention.mask\n       may lead to incorrect and unpredictable execution - in some scenarios,\n       a causal mask may be applied based on the hint, in other execution\n       scenarios the specified mask may be used.  The choice may not appear\n       to be deterministic, in that a number of factors like alignment,\n       hardware SKU, etc influence the decision whether to use a mask or\n       rely on the hint.\n    ``size`` if not None, check whether the mask is a causal mask of the provided size\n       Otherwise, checks for any causal mask.\n    \"\"\"\n    # Prevent type refinement\n    make_causal = is_causal is True\n\n    if is_causal is None and mask is not None:\n        sz = size if size is not None else mask.size(-2)\n        causal_comparison = _generate_square_subsequent_mask(\n            sz, device=mask.device, dtype=mask.dtype\n        )\n\n        # Do not use `torch.equal` so we handle batched masks by\n        # broadcasting the comparison.\n        if mask.size() == causal_comparison.size():\n            make_causal = bool((mask == causal_comparison).all())\n        else:\n            make_causal = False\n\n    return make_causal\n",
    "file-4": "\"\"\"\n================================================================================\nCRITICAL CODE STUDIES SAMPLE - TRANSFORMER ARCHITECTURE (2017)\n================================================================================\n\nImplementation: PyTorch Official MultiheadAttention (Production)\nFramework: PyTorch\nYear: Current version from PyTorch repository\nPurpose: Production attention mechanism with optimizations\nAuthors: PyTorch core team\nSource: https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/activation.py\nLicense: BSD 3-Clause\n\nThis file contains PyTorch's activation functions AND the production\nMultiheadAttention implementation. Key features:\n- Fast-path using torch.nn.functional.scaled_dot_product_attention\n- Support for nested tensors (efficient padding representation)\n- Key padding masks and attention masks\n- Optimized for performance in production models\n\nThe MultiheadAttention class implements the core attention mechanism:\n    Attention(Q, K, V) = softmax(QK^T / √d_k)V\n\nThis is the actual attention code used in transformer_pytorch.py.\n\nCompare to:\n- annotated_transformer.py (pedagogical implementation with comments)\n- attention_tensorflow.py (TensorFlow's attention in tensor2tensor)\n\n================================================================================\n\"\"\"\n\n# mypy: allow-untyped-defs\nimport warnings\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn.init import constant_, xavier_normal_, xavier_uniform_\nfrom torch.nn.parameter import Parameter\n\nfrom .linear import NonDynamicallyQuantizableLinear\nfrom .module import Module\n\n\n__all__ = [\n    \"Threshold\",\n    \"ReLU\",\n    \"RReLU\",\n    \"Hardtanh\",\n    \"ReLU6\",\n    \"Sigmoid\",\n    \"Hardsigmoid\",\n    \"Tanh\",\n    \"SiLU\",\n    \"Mish\",\n    \"Hardswish\",\n    \"ELU\",\n    \"CELU\",\n    \"SELU\",\n    \"GLU\",\n    \"GELU\",\n    \"Hardshrink\",\n    \"LeakyReLU\",\n    \"LogSigmoid\",\n    \"Softplus\",\n    \"Softshrink\",\n    \"MultiheadAttention\",\n    \"PReLU\",\n    \"Softsign\",\n    \"Tanhshrink\",\n    \"Softmin\",\n    \"Softmax\",\n    \"Softmax2d\",\n    \"LogSoftmax\",\n]\n\n\nclass Threshold(Module):\n    r\"\"\"Thresholds each element of the input Tensor.\n\n    Threshold is defined as:\n\n    .. math::\n        y =\n        \\begin{cases}\n        x, &\\text{ if } x > \\text{threshold} \\\\\n        \\text{value}, &\\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        threshold: The value to threshold at\n        value: The value to replace with\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Threshold.png\n\n    Examples::\n\n        >>> m = nn.Threshold(0, 0.5)\n        >>> input = torch.arange(-3, 3)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"threshold\", \"value\", \"inplace\"]\n\n    threshold: float\n    value: float\n    inplace: bool\n\n    def __init__(self, threshold: float, value: float, inplace: bool = False) -> None:\n        super().__init__()\n        self.threshold = threshold\n        self.value = value\n        self.inplace = inplace\n        # TODO: check in THNN (if inplace == True, then assert value <= threshold)\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.threshold(input, self.threshold, self.value, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"threshold={self.threshold}, value={self.value}{inplace_str}\"\n\n\nclass ReLU(Module):\n    r\"\"\"Applies the rectified linear unit function element-wise.\n\n    :math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ReLU.png\n\n    Examples::\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n\n      An implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2).unsqueeze(0)\n        >>> output = torch.cat((m(input), m(-input)))\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.relu(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass RReLU(Module):\n    r\"\"\"Applies the randomized leaky rectified linear unit function, element-wise.\n\n    Method described in the paper:\n    `Empirical Evaluation of Rectified Activations in Convolutional Network <https://arxiv.org/abs/1505.00853>`_.\n\n    The function is defined as:\n\n    .. math::\n        \\text{RReLU}(x) =\n        \\begin{cases}\n            x & \\text{if } x \\geq 0 \\\\\n            ax & \\text{ otherwise }\n        \\end{cases}\n\n    where :math:`a` is randomly sampled from uniform distribution\n    :math:`\\mathcal{U}(\\text{lower}, \\text{upper})` during training while during\n    evaluation :math:`a` is fixed with :math:`a = \\frac{\\text{lower} + \\text{upper}}{2}`.\n\n    Args:\n        lower: lower bound of the uniform distribution. Default: :math:`\\frac{1}{8}`\n        upper: upper bound of the uniform distribution. Default: :math:`\\frac{1}{3}`\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/RReLU.png\n\n    Examples::\n\n        >>> m = nn.RReLU(0.1, 0.3)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    \"\"\"\n\n    __constants__ = [\"lower\", \"upper\", \"inplace\"]\n\n    lower: float\n    upper: float\n    inplace: bool\n\n    def __init__(\n        self, lower: float = 1.0 / 8, upper: float = 1.0 / 3, inplace: bool = False\n    ) -> None:\n        super().__init__()\n        self.lower = lower\n        self.upper = upper\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.rrelu(input, self.lower, self.upper, self.training, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"lower={self.lower}, upper={self.upper}{inplace_str}\"\n\n\nclass Hardtanh(Module):\n    r\"\"\"Applies the HardTanh function element-wise.\n\n    HardTanh is defined as:\n\n    .. math::\n        \\text{HardTanh}(x) = \\begin{cases}\n            \\text{max\\_val} & \\text{ if } x > \\text{ max\\_val } \\\\\n            \\text{min\\_val} & \\text{ if } x < \\text{ min\\_val } \\\\\n            x & \\text{ otherwise } \\\\\n        \\end{cases}\n\n    Args:\n        min_val: minimum value of the linear region range. Default: -1\n        max_val: maximum value of the linear region range. Default: 1\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Keyword arguments :attr:`min_value` and :attr:`max_value`\n    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardtanh.png\n\n    Examples::\n\n        >>> m = nn.Hardtanh(-2, 2)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"min_val\", \"max_val\", \"inplace\"]\n\n    min_val: float\n    max_val: float\n    inplace: bool\n\n    def __init__(\n        self,\n        min_val: float = -1.0,\n        max_val: float = 1.0,\n        inplace: bool = False,\n        min_value: float | None = None,\n        max_value: float | None = None,\n    ) -> None:\n        super().__init__()\n        if min_value is not None:\n            warnings.warn(\n                \"keyword argument `min_value` is deprecated and rename to `min_val`\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            min_val = min_value\n        if max_value is not None:\n            warnings.warn(\n                \"keyword argument `max_value` is deprecated and rename to `max_val`\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            max_val = max_value\n\n        self.min_val = min_val\n        self.max_val = max_val\n        self.inplace = inplace\n        if self.max_val <= self.min_val:\n            raise AssertionError(\n                f\"max_val ({self.max_val}) must be greater than min_val ({self.min_val})\"\n            )\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.hardtanh(input, self.min_val, self.max_val, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"min_val={self.min_val}, max_val={self.max_val}{inplace_str}\"\n\n\nclass ReLU6(Hardtanh):\n    r\"\"\"Applies the ReLU6 function element-wise.\n\n    .. math::\n        \\text{ReLU6}(x) = \\min(\\max(0,x), 6)\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ReLU6.png\n\n    Examples::\n\n        >>> m = nn.ReLU6()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__(0.0, 6.0, inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass Sigmoid(Module):\n    r\"\"\"Applies the Sigmoid function element-wise.\n\n    .. math::\n        \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Sigmoid.png\n\n    Examples::\n\n        >>> m = nn.Sigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return torch.sigmoid(input)\n\n\nclass Hardsigmoid(Module):\n    r\"\"\"Applies the Hardsigmoid function element-wise.\n\n    Hardsigmoid is defined as:\n\n    .. math::\n        \\text{Hardsigmoid}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            1 & \\text{if~} x \\ge +3, \\\\\n            x / 6 + 1 / 2 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardsigmoid.png\n\n    Examples::\n\n        >>> m = nn.Hardsigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.hardsigmoid(input, self.inplace)\n\n\nclass Tanh(Module):\n    r\"\"\"Applies the Hyperbolic Tangent (Tanh) function element-wise.\n\n    Tanh is defined as:\n\n    .. math::\n        \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Tanh.png\n\n    Examples::\n\n        >>> m = nn.Tanh()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return torch.tanh(input)\n\n\nclass SiLU(Module):\n    r\"\"\"Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n\n    The SiLU function is also known as the swish function.\n\n    .. math::\n        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n\n    .. note::\n        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n        where the SiLU was experimented with later.\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/SiLU.png\n\n    Examples::\n\n        >>> m = nn.SiLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.silu(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass Mish(Module):\n    r\"\"\"Applies the Mish function, element-wise.\n\n    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n    .. math::\n        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n\n    .. note::\n        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Mish.png\n\n    Examples::\n\n        >>> m = nn.Mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.mish(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass Hardswish(Module):\n    r\"\"\"Applies the Hardswish function, element-wise.\n\n    Method described in the paper: `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_.\n\n    Hardswish is defined as:\n\n    .. math::\n        \\text{Hardswish}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            x & \\text{if~} x \\ge +3, \\\\\n            x \\cdot (x + 3) /6 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardswish.png\n\n    Examples::\n\n        >>> m = nn.Hardswish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.hardswish(input, self.inplace)\n\n\nclass ELU(Module):\n    r\"\"\"Applies the Exponential Linear Unit (ELU) function, element-wise.\n\n    Method described in the paper: `Fast and Accurate Deep Network Learning by Exponential Linear\n    Units (ELUs) <https://arxiv.org/abs/1511.07289>`__.\n\n    ELU is defined as:\n\n    .. math::\n        \\text{ELU}(x) = \\begin{cases}\n        x, & \\text{ if } x > 0\\\\\n        \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n        \\end{cases}\n\n    Args:\n        alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ELU.png\n\n    Examples::\n\n        >>> m = nn.ELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"alpha\", \"inplace\"]\n    alpha: float\n    inplace: bool\n\n    def __init__(self, alpha: float = 1.0, inplace: bool = False) -> None:\n        super().__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.elu(input, self.alpha, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"alpha={self.alpha}{inplace_str}\"\n\n\nclass CELU(Module):\n    r\"\"\"Applies the CELU function element-wise.\n\n    .. math::\n        \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))\n\n    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .\n\n    Args:\n        alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/CELU.png\n\n    Examples::\n\n        >>> m = nn.CELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _`Continuously Differentiable Exponential Linear Units`:\n        https://arxiv.org/abs/1704.07483\n    \"\"\"\n\n    __constants__ = [\"alpha\", \"inplace\"]\n    alpha: float\n    inplace: bool\n\n    def __init__(self, alpha: float = 1.0, inplace: bool = False) -> None:\n        super().__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.celu(input, self.alpha, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"alpha={self.alpha}{inplace_str}\"\n\n\nclass SELU(Module):\n    r\"\"\"Applies the SELU function element-wise.\n\n    .. math::\n        \\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))\n\n    with :math:`\\alpha = 1.6732632423543772848170429916717` and\n    :math:`\\text{scale} = 1.0507009873554804934193349852946`.\n\n    .. warning::\n        When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,\n        ``nonlinearity='linear'`` should be used instead of ``nonlinearity='selu'``\n        in order to get `Self-Normalizing Neural Networks`_.\n        See :func:`torch.nn.init.calculate_gain` for more information.\n\n    More details can be found in the paper `Self-Normalizing Neural Networks`_ .\n\n    Args:\n        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/SELU.png\n\n    Examples::\n\n        >>> m = nn.SELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.selu(input, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass GLU(Module):\n    r\"\"\"Applies the gated linear unit function.\n\n    :math:`{GLU}(a, b)= a \\otimes \\sigma(b)` where :math:`a` is the first half\n    of the input matrices and :math:`b` is the second half.\n\n    Args:\n        dim (int): the dimension on which to split the input. Default: -1\n\n    Shape:\n        - Input: :math:`(\\ast_1, N, \\ast_2)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(\\ast_1, M, \\ast_2)` where :math:`M=N/2`\n\n    Examples::\n\n        >>> m = nn.GLU()\n        >>> input = torch.randn(4, 2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: int\n\n    def __init__(self, dim: int = -1) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.glu(input, self.dim)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"dim={self.dim}\"\n\n\nclass GELU(Module):\n    r\"\"\"Applies the Gaussian Error Linear Units function.\n\n    .. math:: \\text{GELU}(x) = x * \\Phi(x)\n\n    where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\n    When the approximate argument is 'tanh', Gelu is estimated with:\n\n    .. math:: \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\n    Args:\n        approximate (str, optional): the gelu approximation algorithm to use:\n            ``'none'`` | ``'tanh'``. Default: ``'none'``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/GELU.png\n\n    Examples::\n\n        >>> m = nn.GELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"approximate\"]\n    approximate: str\n\n    def __init__(self, approximate: str = \"none\") -> None:\n        super().__init__()\n        self.approximate = approximate\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.gelu(input, approximate=self.approximate)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"approximate={repr(self.approximate)}\"\n\n\nclass Hardshrink(Module):\n    r\"\"\"Applies the Hard Shrinkage (Hardshrink) function element-wise.\n\n    Hardshrink is defined as:\n\n    .. math::\n        \\text{HardShrink}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x > \\lambda \\\\\n        x, & \\text{ if } x < -\\lambda \\\\\n        0, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        lambd: the :math:`\\lambda` value for the Hardshrink formulation. Default: 0.5\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardshrink.png\n\n    Examples::\n\n        >>> m = nn.Hardshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"lambd\"]\n    lambd: float\n\n    def __init__(self, lambd: float = 0.5) -> None:\n        super().__init__()\n        self.lambd = lambd\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Run forward pass.\n        \"\"\"\n        return F.hardshrink(input, self.lambd)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"{self.lambd}\"\n\n\nclass LeakyReLU(Module):\n    r\"\"\"Applies the LeakyReLU function element-wise.\n\n    .. math::\n        \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)\n\n\n    or\n\n    .. math::\n        \\text{LeakyReLU}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x \\geq 0 \\\\\n        \\text{negative\\_slope} \\times x, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        negative_slope: Controls the angle of the negative slope (which is used for\n          negative input values). Default: 1e-2\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    .. image:: ../scripts/activation_images/LeakyReLU.png\n\n    Examples::\n\n        >>> m = nn.LeakyReLU(0.1)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\", \"negative_slope\"]\n    inplace: bool\n    negative_slope: float\n\n    def __init__(self, negative_slope: float = 1e-2, inplace: bool = False) -> None:\n        super().__init__()\n        self.negative_slope = negative_slope\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Run forward pass.\n        \"\"\"\n        return F.leaky_relu(input, self.negative_slope, self.inplace)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"negative_slope={self.negative_slope}{inplace_str}\"\n\n\nclass LogSigmoid(Module):\n    r\"\"\"Applies the Logsigmoid function element-wise.\n\n    .. math::\n        \\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/LogSigmoid.png\n\n    Examples::\n\n        >>> m = nn.LogSigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Run forward pass.\n        \"\"\"\n        return F.logsigmoid(input)\n\n\nclass Softplus(Module):\n    r\"\"\"Applies the Softplus function element-wise.\n\n    .. math::\n        \\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\n\n    SoftPlus is a smooth approximation to the ReLU function and can be used\n    to constrain the output of a machine to always be positive.\n\n    For numerical stability the implementation reverts to the linear function\n    when :math:`input \\times \\beta > threshold`.\n\n    Args:\n        beta: the :math:`\\beta` value for the Softplus formulation. Default: 1\n        threshold: values above this revert to a linear function. Default: 20\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softplus.png\n\n    Examples::\n\n        >>> m = nn.Softplus()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"beta\", \"threshold\"]\n    beta: float\n    threshold: float\n\n    def __init__(self, beta: float = 1.0, threshold: float = 20.0) -> None:\n        super().__init__()\n        self.beta = beta\n        self.threshold = threshold\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Run forward pass.\n        \"\"\"\n        return F.softplus(input, self.beta, self.threshold)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"beta={self.beta}, threshold={self.threshold}\"\n\n\nclass Softshrink(Module):\n    r\"\"\"Applies the soft shrinkage function element-wise.\n\n    .. math::\n        \\text{SoftShrinkage}(x) =\n        \\begin{cases}\n        x - \\lambda, & \\text{ if } x > \\lambda \\\\\n        x + \\lambda, & \\text{ if } x < -\\lambda \\\\\n        0, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        lambd: the :math:`\\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softshrink.png\n\n    Examples::\n\n        >>> m = nn.Softshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"lambd\"]\n    lambd: float\n\n    def __init__(self, lambd: float = 0.5) -> None:\n        super().__init__()\n        self.lambd = lambd\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Run forward pass.\n        \"\"\"\n        return F.softshrink(input, self.lambd)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return str(self.lambd)\n\n\ndef _check_arg_device(x: torch.Tensor | None) -> bool:\n    if x is not None:\n        return x.device.type in [\n            \"cpu\",\n            \"cuda\",\n            torch.utils.backend_registration._privateuse1_backend_name,\n        ]\n    return True\n\n\ndef _arg_requires_grad(x: torch.Tensor | None) -> bool:\n    if x is not None:\n        return x.requires_grad\n    return False\n\n\ndef _is_make_fx_tracing():\n    if not torch.jit.is_scripting():\n        torch_dispatch_mode_stack = (\n            torch.utils._python_dispatch._get_current_dispatch_mode_stack()\n        )\n        # this can be triggered when dynamo inlining the module too.\n        return (\n            any(\n                type(x) is torch.fx.experimental.proxy_tensor.ProxyTorchDispatchMode\n                for x in torch_dispatch_mode_stack\n            )\n            or torch.compiler.is_exporting()\n        )\n    else:\n        return False\n\n\nclass MultiheadAttention(Module):\n    r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\n\n    This MultiheadAttention layer implements the original architecture described\n    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n    intent of this layer is as a reference implementation for foundational understanding\n    and thus it contains only limited features relative to newer architectures.\n    Given the fast pace of innovation in transformer-like architectures, we recommend\n    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n    to build efficient layers from building blocks in core or using higher\n    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\n    Multi-Head Attention is defined as:\n\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n\n    where :math:`\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n\n    ``nn.MultiheadAttention`` will use the optimized implementations of\n    ``scaled_dot_product_attention()`` when possible.\n\n    In addition to support for the new ``scaled_dot_product_attention()``\n    function, for speeding up Inference, MHA will use\n    fastpath inference with support for Nested Tensors, iff:\n\n    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n    - inputs are batched (3D) with ``batch_first==True``\n    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n    - training is disabled (using ``.eval()``)\n    - ``add_bias_kv`` is ``False``\n    - ``add_zero_attn`` is ``False``\n    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n      nor ``attn_mask`` is passed\n    - autocast is disabled\n\n    If the optimized inference fastpath implementation is in use, a\n    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n    ``query``/``key``/``value`` to represent padding more efficiently than using a\n    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n    will be returned, and an additional speedup proportional to the fraction of the input\n    that is padding can be expected.\n\n    Args:\n        embed_dim: Total dimension of the model.\n        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n            Default: ``False``.\n        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n\n    Examples::\n\n        >>> # xdoctest: +SKIP\n        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n\n    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n         https://arxiv.org/abs/2205.14135\n\n    \"\"\"\n\n    __constants__ = [\"batch_first\"]\n    bias_k: torch.Tensor | None\n    bias_v: torch.Tensor | None\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        kdim=None,\n        vdim=None,\n        batch_first=False,\n        device=None,\n        dtype=None,\n    ) -> None:\n        if embed_dim <= 0 or num_heads <= 0:\n            raise ValueError(\n                f\"embed_dim and num_heads must be greater than 0,\"\n                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n            )\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.batch_first = batch_first\n        self.head_dim = embed_dim // num_heads\n        if self.head_dim * num_heads != self.embed_dim:\n            raise AssertionError(\"embed_dim must be divisible by num_heads\")\n\n        if not self._qkv_same_embed_dim:\n            self.q_proj_weight = Parameter(\n                torch.empty((embed_dim, embed_dim), **factory_kwargs)\n            )\n            self.k_proj_weight = Parameter(\n                torch.empty((embed_dim, self.kdim), **factory_kwargs)\n            )\n            self.v_proj_weight = Parameter(\n                torch.empty((embed_dim, self.vdim), **factory_kwargs)\n            )\n            self.register_parameter(\"in_proj_weight\", None)\n        else:\n            self.in_proj_weight = Parameter(\n                torch.empty((3 * embed_dim, embed_dim), **factory_kwargs)\n            )\n            self.register_parameter(\"q_proj_weight\", None)\n            self.register_parameter(\"k_proj_weight\", None)\n            self.register_parameter(\"v_proj_weight\", None)\n\n        if bias:\n            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n        else:\n            self.register_parameter(\"in_proj_bias\", None)\n        self.out_proj = NonDynamicallyQuantizableLinear(\n            embed_dim, embed_dim, bias=bias, **factory_kwargs\n        )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self) -> None:\n        if self._qkv_same_embed_dim:\n            xavier_uniform_(self.in_proj_weight)\n        else:\n            xavier_uniform_(self.q_proj_weight)\n            xavier_uniform_(self.k_proj_weight)\n            xavier_uniform_(self.v_proj_weight)\n\n        if self.in_proj_bias is not None:\n            constant_(self.in_proj_bias, 0.0)\n            constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            xavier_normal_(self.bias_v)\n\n    def __setstate__(self, state):\n        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n        if \"_qkv_same_embed_dim\" not in state:\n            state[\"_qkv_same_embed_dim\"] = True\n\n        super().__setstate__(state)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        key_padding_mask: Tensor | None = None,\n        need_weights: bool = True,\n        attn_mask: Tensor | None = None,\n        average_attn_weights: bool = True,\n        is_causal: bool = False,\n    ) -> tuple[Tensor, Tensor | None]:\n        r\"\"\"Compute attention outputs using query, key, and value embeddings.\n\n            Supports optional parameters for padding, masks and attention weights.\n\n        Args:\n            query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n                or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n                :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n                Queries are compared against key-value pairs to produce the output.\n                See \"Attention Is All You Need\" for more details.\n            key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n                or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n                :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n                See \"Attention Is All You Need\" for more details.\n            value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n                ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n                sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n                See \"Attention Is All You Need\" for more details.\n            key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n                to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n                Binary and float masks are supported.\n                For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n                the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n            need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n                Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n                and achieve the best performance for MHA.\n                Default: ``True``.\n            attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n                :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n                :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n                broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n                Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n                corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n                the attention weight.\n                If both attn_mask and key_padding_mask are supplied, their types should match.\n            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n                heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n                effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n            is_causal: If specified, applies a causal mask as attention mask.\n                Default: ``False``.\n                Warning:\n                ``is_causal`` provides a hint that ``attn_mask`` is the\n                causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n\n        Outputs:\n            - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n              :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n              where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n              embedding dimension ``embed_dim``.\n            - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n              returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n              head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n\n            .. note::\n                `batch_first` argument is ignored for unbatched inputs.\n        \"\"\"  # noqa: B950\n        why_not_fast_path = \"\"\n        if (\n            (attn_mask is not None and torch.is_floating_point(attn_mask))\n            or (key_padding_mask is not None)\n            and torch.is_floating_point(key_padding_mask)\n        ):\n            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n\n        is_batched = query.dim() == 3\n\n        key_padding_mask = F._canonical_mask(\n            mask=key_padding_mask,\n            mask_name=\"key_padding_mask\",\n            other_type=F._none_or_dtype(attn_mask),\n            other_name=\"attn_mask\",\n            target_type=query.dtype,\n        )\n\n        attn_mask = F._canonical_mask(\n            mask=attn_mask,\n            mask_name=\"attn_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=query.dtype,\n            check_other=False,\n        )\n\n        is_fastpath_enabled = torch.backends.mha.get_fastpath_enabled()\n\n        if not is_fastpath_enabled:\n            why_not_fast_path = \"torch.backends.mha.get_fastpath_enabled() was not True\"\n        elif not is_batched:\n            why_not_fast_path = (\n                f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n            )\n        elif query is not key or key is not value:\n            # When lifting this restriction, don't forget to either\n            # enforce that the dtypes all match or test cases where\n            # they don't!\n            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n        elif self.in_proj_weight is None:\n            why_not_fast_path = \"in_proj_weight was None\"\n        elif query.dtype != self.in_proj_weight.dtype:\n            # this case will fail anyway, but at least they'll get a useful error message.\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n        elif self.training:\n            why_not_fast_path = \"training is enabled\"\n        elif (self.num_heads % 2) != 0:\n            why_not_fast_path = \"self.num_heads is not even\"\n        elif not self.batch_first:\n            why_not_fast_path = \"batch_first was not True\"\n        elif self.bias_k is not None:\n            why_not_fast_path = \"self.bias_k was not None\"\n        elif self.bias_v is not None:\n            why_not_fast_path = \"self.bias_v was not None\"\n        elif self.add_zero_attn:\n            why_not_fast_path = \"add_zero_attn was enabled\"\n        elif not self._qkv_same_embed_dim:\n            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n        elif query.is_nested and (\n            key_padding_mask is not None or attn_mask is not None\n        ):\n            why_not_fast_path = (\n                \"supplying both src_key_padding_mask and src_mask at the same time \\\n                                 is not supported with NestedTensor input\"\n            )\n        elif torch.is_autocast_enabled():\n            why_not_fast_path = \"autocast is enabled\"\n\n        if not why_not_fast_path:\n            tensor_args = (\n                query,\n                key,\n                value,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.out_proj.weight,\n                self.out_proj.bias,\n            )\n            # We have to use list comprehensions below because TorchScript does not support\n            # generator expressions.\n            if torch.overrides.has_torch_function(tensor_args):\n                why_not_fast_path = \"some Tensor argument has_torch_function\"\n            elif _is_make_fx_tracing():\n                why_not_fast_path = \"we are running make_fx tracing\"\n            elif not all(_check_arg_device(x) for x in tensor_args):\n                why_not_fast_path = (\n                    \"some Tensor argument's device is neither one of \"\n                    f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\"\n                )\n            elif torch.is_grad_enabled() and any(\n                _arg_requires_grad(x) for x in tensor_args\n            ):\n                why_not_fast_path = (\n                    \"grad is enabled and at least one of query or the \"\n                    \"input/output projection weights or biases requires_grad\"\n                )\n            if not why_not_fast_path:\n                merged_mask, mask_type = self.merge_masks(\n                    attn_mask, key_padding_mask, query\n                )\n\n                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n                    return torch._native_multi_head_attention(\n                        query,\n                        key,\n                        value,\n                        self.embed_dim,\n                        self.num_heads,\n                        self.in_proj_weight,\n                        self.in_proj_bias,\n                        self.out_proj.weight,\n                        self.out_proj.bias,\n                        merged_mask,\n                        need_weights,\n                        average_attn_weights,\n                        mask_type,\n                    )\n\n        any_nested = query.is_nested or key.is_nested or value.is_nested\n        if any_nested:\n            raise AssertionError(\n                \"MultiheadAttention does not support NestedTensor outside of its fast path. \"\n                + f\"The fast path was not hit because {why_not_fast_path}\"\n            )\n\n        if self.batch_first and is_batched:\n            # make sure that the transpose op does not affect the \"is\" property\n            if key is value:\n                if query is key:\n                    query = key = value = query.transpose(1, 0)\n                else:\n                    query, key = (x.transpose(1, 0) for x in (query, key))\n                    value = key\n            else:\n                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n\n        if not self._qkv_same_embed_dim:\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                training=self.training,\n                key_padding_mask=key_padding_mask,\n                need_weights=need_weights,\n                attn_mask=attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj_weight,\n                k_proj_weight=self.k_proj_weight,\n                v_proj_weight=self.v_proj_weight,\n                average_attn_weights=average_attn_weights,\n                is_causal=is_causal,\n            )\n        else:\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                training=self.training,\n                key_padding_mask=key_padding_mask,\n                need_weights=need_weights,\n                attn_mask=attn_mask,\n                average_attn_weights=average_attn_weights,\n                is_causal=is_causal,\n            )\n        if self.batch_first and is_batched:\n            return attn_output.transpose(1, 0), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def merge_masks(\n        self,\n        attn_mask: Tensor | None,\n        key_padding_mask: Tensor | None,\n        query: Tensor,\n    ) -> tuple[Tensor | None, int | None]:\n        r\"\"\"Determine mask type and combine masks if necessary.\n\n        If only one mask is provided, that mask\n        and the corresponding mask type will be returned. If both masks are provided, they will be both\n        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``\n        and mask type 2 will be returned\n        Args:\n            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0\n            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1\n            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``\n        Returns:\n            merged_mask: merged mask\n            mask_type: merged mask type (0, 1, or 2)\n        \"\"\"\n        mask_type: int | None = None\n        merged_mask: Tensor | None = None\n\n        if key_padding_mask is not None:\n            mask_type = 1\n            merged_mask = key_padding_mask\n\n        if attn_mask is not None:\n            # In this branch query can't be a nested tensor, so it has a shape\n            batch_size, seq_len, _ = query.shape\n            mask_type = 2\n\n            # Always expands attn_mask to 4D\n            if attn_mask.dim() == 3:\n                attn_mask_expanded = attn_mask.view(batch_size, -1, seq_len, seq_len)\n            else:  # attn_mask.dim() == 2:\n                attn_mask_expanded = attn_mask.view(1, 1, seq_len, seq_len).expand(\n                    batch_size, self.num_heads, -1, -1\n                )\n            merged_mask = attn_mask_expanded\n\n            if key_padding_mask is not None:\n                key_padding_mask_expanded = key_padding_mask.view(\n                    batch_size, 1, 1, seq_len\n                ).expand(-1, self.num_heads, -1, -1)\n                merged_mask = attn_mask_expanded + key_padding_mask_expanded\n\n        # no attn_mask and no key_padding_mask, returns None, None\n        return merged_mask, mask_type\n\n\nclass PReLU(Module):\n    r\"\"\"Applies the element-wise PReLU function.\n\n    .. math::\n        \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\n    or\n\n    .. math::\n        \\text{PReLU}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x \\ge 0 \\\\\n        ax, & \\text{ otherwise }\n        \\end{cases}\n\n    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\n    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\n    a separate :math:`a` is used for each input channel.\n\n\n    .. note::\n        weight decay should not be used when learning :math:`a` for good performance.\n\n    .. note::\n        Channel dim is the 2nd dim of input. When input has dims < 2, then there is\n        no channel dim and the number of channels = 1.\n\n    Args:\n        num_parameters (int): number of :math:`a` to learn.\n            Although it takes an int as input, there is only two values are legitimate:\n            1, or the number of channels at input. Default: 1\n        init (float): the initial value of :math:`a`. Default: 0.25\n\n    Shape:\n        - Input: :math:`( *)` where `*` means, any number of additional\n          dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Attributes:\n        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\n\n    .. image:: ../scripts/activation_images/PReLU.png\n\n    Examples::\n\n        >>> m = nn.PReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"num_parameters\"]\n    num_parameters: int\n\n    def __init__(\n        self, num_parameters: int = 1, init: float = 0.25, device=None, dtype=None\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        self.num_parameters = num_parameters\n        super().__init__()\n        self.init = init\n        self.weight = Parameter(torch.empty(num_parameters, **factory_kwargs))\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"\n        Resets parameters based on their initialization used in ``__init__``.\n        \"\"\"\n        torch.nn.init.constant_(self.weight, self.init)\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.prelu(input, self.weight)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"num_parameters={self.num_parameters}\"\n\n\nclass Softsign(Module):\n    r\"\"\"Applies the element-wise Softsign function.\n\n    .. math::\n        \\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softsign.png\n\n    Examples::\n\n        >>> m = nn.Softsign()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.softsign(input)\n\n\nclass Tanhshrink(Module):\n    r\"\"\"Applies the element-wise Tanhshrink function.\n\n    .. math::\n        \\text{Tanhshrink}(x) = x - \\tanh(x)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Tanhshrink.png\n\n    Examples::\n\n        >>> m = nn.Tanhshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.tanhshrink(input)\n\n\nclass Softmin(Module):\n    r\"\"\"Applies the Softmin function to an n-dimensional input Tensor.\n\n    Rescales them so that the elements of the n-dimensional output Tensor\n    lie in the range `[0, 1]` and sum to 1.\n\n    Softmin is defined as:\n\n    .. math::\n        \\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Args:\n        dim (int): A dimension along which Softmin will be computed (so every slice\n            along dim will sum to 1).\n\n    Returns:\n        a Tensor of the same dimension and shape as the input, with\n        values in the range [0, 1]\n\n    Examples::\n\n        >>> m = nn.Softmin(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: int | None\n\n    def __init__(self, dim: int | None = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"dim\"):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.softmin(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"dim={self.dim}\"\n\n\nclass Softmax(Module):\n    r\"\"\"Applies the Softmax function to an n-dimensional input Tensor.\n\n    Rescales them so that the elements of the n-dimensional output Tensor\n    lie in the range [0,1] and sum to 1.\n\n    Softmax is defined as:\n\n    .. math::\n        \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\n    When the input Tensor is a sparse tensor then the unspecified\n    values are treated as ``-inf``.\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [0, 1]\n\n    Args:\n        dim (int): A dimension along which Softmax will be computed (so every slice\n            along dim will sum to 1).\n\n    .. note::\n        This module doesn't work directly with NLLLoss,\n        which expects the Log to be computed between the Softmax and itself.\n        Use `LogSoftmax` instead (it's faster and has better numerical properties).\n\n    Examples::\n\n        >>> m = nn.Softmax(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: int | None\n\n    def __init__(self, dim: int | None = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"dim\"):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.softmax(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"dim={self.dim}\"\n\n\nclass Softmax2d(Module):\n    r\"\"\"Applies SoftMax over features to each spatial location.\n\n    When given an image of ``Channels x Height x Width``, it will\n    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`\n\n    Shape:\n        - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`.\n        - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input)\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [0, 1]\n\n    Examples::\n\n        >>> m = nn.Softmax2d()\n        >>> # you softmax over the 2nd dimension\n        >>> input = torch.randn(2, 3, 12, 13)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        if input.dim() not in (3, 4):\n            raise ValueError(\n                f\"Softmax2d: expected input to be 3D or 4D, got {input.dim()}D instead\"\n            )\n        return F.softmax(input, -3, _stacklevel=5)\n\n\nclass LogSoftmax(Module):\n    r\"\"\"Applies the :math:`\\log(\\text{Softmax}(x))` function to an n-dimensional input Tensor.\n\n    The LogSoftmax formulation can be simplified as:\n\n    .. math::\n        \\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Args:\n        dim (int): A dimension along which LogSoftmax will be computed.\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [-inf, 0)\n\n    Examples::\n\n        >>> m = nn.LogSoftmax(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: int | None\n\n    def __init__(self, dim: int | None = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"dim\"):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n        Runs the forward pass.\n        \"\"\"\n        return F.log_softmax(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Return the extra representation of the module.\n        \"\"\"\n        return f\"dim={self.dim}\"\n",
    "file-5": "\"\"\"\n================================================================================\nCRITICAL CODE STUDIES SAMPLE - TRANSFORMER ARCHITECTURE (2017)\n================================================================================\n\nImplementation: TensorFlow tensor2tensor (Google's Original Framework)\nFramework: TensorFlow\nYear: 2023 version (tensor2tensor project)\nPurpose: Production implementation from Google Brain/Research\nAuthors: Tensor2Tensor Authors (Google)\nSource: https://github.com/tensorflow/tensor2tensor\nLicense: Apache 2.0\n\nThis is Google's tensor2tensor Transformer implementation, representing the\nframework closest to the original 2017 paper authors' environment (Google Brain).\nKey features:\n- TensorFlow computational graph approach (vs PyTorch's imperative style)\n- Extensive hyperparameter configuration system\n- Integration with Google's ML infrastructure\n- Multiple Transformer variants and extensions\n\nThe original \"Attention Is All You Need\" authors worked at Google Brain and\nGoogle Research, making this implementation historically significant as it\nreflects the institutional context where Transformers emerged.\n\nCompare to:\n- transformer_pytorch.py (PyTorch's imperative implementation)\n- annotated_transformer.py (pedagogical version with line-by-line explanation)\n\nNote: Imports common_attention from attention_tensorflow.py\n\n================================================================================\n\"\"\"\n\n# coding=utf-8\n# Copyright 2023 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Transformer model from \"Attention Is All You Need\".\n\nThe Transformer model consists of an encoder and a decoder. Both are stacks\nof self-attention layers followed by feed-forward layers. This model yields\ngood results on a number of problems, especially in NLP and machine translation.\n\nSee \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762) for the full\ndescription of the model and the results obtained with its early version.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom six.moves import range  # pylint: disable=redefined-builtin\n\nfrom tensor2tensor.data_generators import librispeech\nfrom tensor2tensor.layers import common_attention\nfrom tensor2tensor.layers import common_hparams\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.layers import modalities\nfrom tensor2tensor.layers import transformer_layers\nfrom tensor2tensor.layers import transformer_memory\nfrom tensor2tensor.utils import beam_search\nfrom tensor2tensor.utils import expert_utils\nfrom tensor2tensor.utils import mlperf_log\nfrom tensor2tensor.utils import registry\nfrom tensor2tensor.utils import t2t_model\n\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.compat.v1 import estimator as tf_estimator\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.ops import inplace_ops\nfrom tensorflow.python.util import nest\n# pylint: enable=g-direct-tensorflow-import\n\n# Alias some commonly reused layers, here and elsewhere.\ntransformer_prepare_encoder = transformer_layers.transformer_prepare_encoder\ntransformer_encoder = transformer_layers.transformer_encoder\ntransformer_ffn_layer = transformer_layers.transformer_ffn_layer\n\n\ndef transformer_encode(encoder_function, inputs, target_space, hparams,\n                       attention_weights=None, features=None, losses=None,\n                       prepare_encoder_fn=None, **kwargs):\n  \"\"\"Encode transformer inputs.\n\n  Args:\n    encoder_function: the encoder function\n    inputs: Transformer inputs [batch_size, input_length, 1, hidden_dim] which\n      will be flattened along the two spatial dimensions.\n    target_space: scalar, target space ID.\n    hparams: hyperparameters for model.\n    attention_weights: weight to store attention to.\n    features: optionally pass the entire features dictionary as well. This is\n      needed now for \"packed\" datasets.\n    losses: optional list onto which to append extra training losses\n    prepare_encoder_fn: optional, alternative to transformer_prepare_encoder.\n    **kwargs: additional arguments to pass to encoder_function\n\n  Returns:\n    Tuple of:\n        encoder_output: Encoder representation.\n            [batch_size, input_length, hidden_dim]\n        encoder_decoder_attention_bias: Bias and mask weights for\n            encoder-decoder attention. [batch_size, input_length]\n  \"\"\"\n  inputs = common_layers.flatten4d3d(inputs)\n\n  if not prepare_encoder_fn:\n    prepare_encoder_fn = transformer_prepare_encoder\n  encoder_input, self_attention_bias, encoder_decoder_attention_bias = (\n      prepare_encoder_fn(\n          inputs, target_space, hparams, features=features))\n\n  mlperf_log.transformer_print(\n      key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n      value=hparams.layer_prepostprocess_dropout,\n      hparams=hparams)\n\n  encoder_input = tf.nn.dropout(encoder_input,\n                                1.0 - hparams.layer_prepostprocess_dropout)\n\n  attn_bias_for_padding = None\n  # Otherwise the encoder will just use encoder_self_attention_bias.\n  if hparams.unidirectional_encoder:\n    attn_bias_for_padding = encoder_decoder_attention_bias\n\n  encoder_output = encoder_function(\n      encoder_input,\n      self_attention_bias,\n      hparams,\n      nonpadding=features_to_nonpadding(features, \"inputs\"),\n      save_weights_to=attention_weights,\n      make_image_summary=not common_layers.is_xla_compiled(),\n      losses=losses,\n      attn_bias_for_padding=attn_bias_for_padding,\n      **kwargs)\n\n  return encoder_output, encoder_decoder_attention_bias\n\n\ndef transformer_decode(decoder_function,\n                       decoder_input,\n                       encoder_output,\n                       encoder_decoder_attention_bias,\n                       decoder_self_attention_bias,\n                       hparams,\n                       attention_weights=None,\n                       cache=None,\n                       decode_loop_step=None,\n                       nonpadding=None,\n                       losses=None,\n                       **kwargs):\n  \"\"\"Decode Transformer outputs from encoder representation.\n\n  Args:\n    decoder_function: the decoder function\n    decoder_input: inputs to bottom of the model. [batch_size, decoder_length,\n      hidden_dim]\n    encoder_output: Encoder representation. [batch_size, input_length,\n      hidden_dim]\n    encoder_decoder_attention_bias: Bias and mask weights for encoder-decoder\n      attention. [batch_size, input_length]\n    decoder_self_attention_bias: Bias and mask weights for decoder\n      self-attention. [batch_size, decoder_length]\n    hparams: hyperparameters for model.\n    attention_weights: weight to store attention to.\n    cache: dict, containing tensors which are the results of previous\n      attentions, used for fast decoding.\n    decode_loop_step: An integer, step number of the decoding loop. Only used\n      for inference on TPU.\n    nonpadding: optional Tensor with shape [batch_size, decoder_length]\n    losses: optional list onto which to append extra training losses\n    **kwargs: additional arguments to pass to decoder_function\n\n  Returns:\n    Final decoder representation. [batch_size, decoder_length, hidden_dim]\n  \"\"\"\n  mlperf_log.transformer_print(\n      key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n      value=hparams.layer_prepostprocess_dropout,\n      hparams=hparams)\n  decoder_input = tf.nn.dropout(decoder_input,\n                                1.0 - hparams.layer_prepostprocess_dropout)\n\n  decoder_output = decoder_function(\n      decoder_input,\n      encoder_output,\n      decoder_self_attention_bias,\n      encoder_decoder_attention_bias,\n      hparams,\n      cache=cache,\n      decode_loop_step=decode_loop_step,\n      nonpadding=nonpadding,\n      save_weights_to=attention_weights,\n      losses=losses,\n      **kwargs)\n\n  if (common_layers.is_xla_compiled() and\n      hparams.mode == tf_estimator.ModeKeys.TRAIN):\n    # TPU does not react kindly to extra dimensions.\n    # TODO(noam): remove this once TPU is more forgiving of extra dims.\n    return decoder_output\n  else:\n    # Expand since t2t expects 4d tensors.\n    return tf.expand_dims(decoder_output, axis=2)\n\n\n@registry.register_model\nclass Transformer(t2t_model.T2TModel):\n  \"\"\"Attention net.  See file docstring.\"\"\"\n\n  def __init__(self, *args, **kwargs):\n    super(Transformer, self).__init__(*args, **kwargs)\n    self.attention_weights = {}  # For visualizing attention heads.\n    self.recurrent_memory_by_layer = None  # Override to enable recurrent memory\n    self._encoder_function = transformer_encoder\n    self._decoder_function = transformer_decoder\n    self._init_cache_fn = _init_transformer_cache\n    self._prepare_encoder_fn = transformer_prepare_encoder\n    self._prepare_decoder_fn = transformer_prepare_decoder\n\n  def encode(self, inputs, target_space, hparams, features=None, losses=None):\n    \"\"\"Encode transformer inputs, see transformer_encode.\"\"\"\n    return transformer_encode(\n        self._encoder_function, inputs, target_space, hparams,\n        attention_weights=self.attention_weights,\n        features=features, losses=losses,\n        prepare_encoder_fn=self._prepare_encoder_fn)\n\n  def decode(self,\n             decoder_input,\n             encoder_output,\n             encoder_decoder_attention_bias,\n             decoder_self_attention_bias,\n             hparams,\n             cache=None,\n             decode_loop_step=None,\n             nonpadding=None,\n             losses=None,\n             **kwargs):\n    \"\"\"Decode Transformer outputs, see transformer_decode.\"\"\"\n    return transformer_decode(\n        self._decoder_function, decoder_input, encoder_output,\n        encoder_decoder_attention_bias, decoder_self_attention_bias,\n        hparams, attention_weights=self.attention_weights, cache=cache,\n        decode_loop_step=decode_loop_step, nonpadding=nonpadding, losses=losses,\n        **kwargs)\n\n  def body(self, features):\n    \"\"\"Transformer main model_fn.\n\n    Args:\n      features: Map of features to the model. Should contain the following:\n          \"inputs\": Transformer inputs. [batch_size, input_length, 1,\n            hidden_dim].\n          \"targets\": Target decoder outputs. [batch_size, decoder_length, 1,\n            hidden_dim]\n          \"target_space_id\": A scalar int from data_generators.problem.SpaceID.\n\n    Returns:\n      Final decoder representation. [batch_size, decoder_length, hidden_dim]\n    \"\"\"\n    hparams = self._hparams\n\n    losses = []\n\n    if self.has_input:\n      inputs = self._prepare_inputs_for_body(features)\n      target_space = features[\"target_space_id\"]\n      encoder_output, encoder_decoder_attention_bias = self.encode(\n          inputs, target_space, hparams, features=features, losses=losses)\n    else:\n      encoder_output, encoder_decoder_attention_bias = (None, None)\n\n    targets = features[\"targets\"]\n    targets_shape = common_layers.shape_list(targets)\n    targets = common_layers.flatten4d3d(targets)\n    decoder_input, decoder_self_attention_bias = self._prepare_decoder_fn(\n        targets, hparams, features=features)\n\n    # Not all subclasses of Transformer support keyword arguments related to\n    # recurrent memory, so only pass these arguments if memory is enabled.\n    decode_kwargs = {}\n    if self.recurrent_memory_by_layer is not None:\n      # TODO(kitaev): The chunk_number feature currently has the same shape as\n      # \"targets\", but this is only for the purposes of sharing sharding code.\n      # In fact every token within an example must have the same chunk number.\n      chunk_number_each_token = tf.squeeze(features[\"chunk_number\"], (-1, -2))\n      chunk_number_each_example = chunk_number_each_token[:, 0]\n      # Uncomment the code below to verify that tokens within a batch share the\n      # same chunk number:\n      # with tf.control_dependencies([\n      #     tf.assert_equal(chunk_number_each_token,\n      #                     chunk_number_each_example[:, None])\n      # ]):\n      #   chunk_number_each_example = tf.identity(chunk_number_each_example)\n      decode_kwargs = dict(\n          recurrent_memory_by_layer=self.recurrent_memory_by_layer,\n          chunk_number=chunk_number_each_example,\n          )\n    decoder_output = self.decode(\n        decoder_input,\n        encoder_output,\n        encoder_decoder_attention_bias,\n        decoder_self_attention_bias,\n        hparams,\n        nonpadding=features_to_nonpadding(features, \"targets\"),\n        losses=losses,\n        **decode_kwargs\n        )\n    expected_attentions = features.get(\"expected_attentions\")\n    if expected_attentions is not None:\n      attention_loss = common_attention.encoder_decoder_attention_loss(\n          expected_attentions, self.attention_weights,\n          hparams.expected_attention_loss_type,\n          hparams.expected_attention_loss_multiplier)\n      return decoder_output, {\"attention_loss\": attention_loss}\n\n    ret = tf.reshape(decoder_output, targets_shape)\n    if losses:\n      return ret, {\"extra_loss\": tf.add_n(losses)}\n    else:\n      return ret\n\n  def _prepare_inputs_for_body(self, features):\n    \"\"\"Prepare inputs for body.\n\n    Args:\n      features: Map of string to model features. Should contain\n          \"inputs\": Transformer inputs. [batch_size, input_length, 1,\n            hidden_dim].\n\n    Returns:\n      Inputs which will be passed to the model. [batch_size, input_length, 1,\n          hidden_dim]\n    \"\"\"\n    return features[\"inputs\"]\n\n  def _greedy_infer(self, features, decode_length, use_tpu=False):\n    \"\"\"Fast version of greedy decoding.\n\n    Args:\n      features: an map of string to `Tensor`\n      decode_length: an integer.  How many additional timesteps to decode.\n      use_tpu: A bool. Whether to build the inference graph for TPU.\n\n    Returns:\n      A dict of decoding results {\n          \"outputs\": integer `Tensor` of decoded ids of shape\n              [batch_size, <= decode_length] if beam_size == 1 or\n              [batch_size, top_beams, <= decode_length]\n          \"scores\": decoding log probs from the beam search,\n              None if using greedy decoding (beam_size=1)\n      }\n\n    Raises:\n      NotImplementedError: If there are multiple data shards.\n    \"\"\"\n    # For real-valued modalities use the slow decode path for now.\n    if (self._target_modality_is_real or\n        self._hparams.self_attention_type != \"dot_product\"):\n      return super(Transformer, self)._greedy_infer(features, decode_length)\n    with tf.variable_scope(self.name):\n      if use_tpu:\n        return self._fast_decode_tpu(features, decode_length)\n      return self._fast_decode(features, decode_length)\n\n  def _beam_decode(self,\n                   features,\n                   decode_length,\n                   beam_size,\n                   top_beams,\n                   alpha,\n                   use_tpu=False):\n    \"\"\"Beam search decoding.\n\n    Args:\n      features: an map of string to `Tensor`\n      decode_length: an integer.  How many additional timesteps to decode.\n      beam_size: number of beams.\n      top_beams: an integer. How many of the beams to return.\n      alpha: Float that controls the length penalty. larger the alpha, stronger\n        the preference for longer translations.\n      use_tpu: A bool, whether to do beam decode on TPU.\n\n    Returns:\n      A dict of decoding results {\n          \"outputs\": integer `Tensor` of decoded ids of shape\n              [batch_size, <= decode_length] if beam_size == 1 or\n              [batch_size, top_beams, <= decode_length]\n          \"scores\": decoding log probs from the beam search,\n              None if using greedy decoding (beam_size=1)\n      }\n    \"\"\"\n    if (self._hparams.self_attention_type not in [\n        \"dot_product\", \"dot_product_relative\"\n    ]):\n      # Caching is not guaranteed to work with attention types other than\n      # dot_product and dot_product_relative.\n      return self._beam_decode_slow(features, decode_length, beam_size,\n                                    top_beams, alpha, use_tpu)\n    with tf.variable_scope(self.name):\n      if use_tpu:\n        return self._fast_decode_tpu(features, decode_length, beam_size,\n                                     top_beams, alpha)\n      return self._fast_decode(features, decode_length, beam_size, top_beams,\n                               alpha)\n\n  def _prepare_inputs_for_decode(self, features):\n    \"\"\"Prepare inputs for decoding.\n\n    Args:\n      features: A map of string to model features.\n\n    Returns:\n      Inputs after fixing shape and applying modality.\n    \"\"\"\n    dp = self._data_parallelism\n    hparams = self._hparams\n    inputs = features[\"inputs\"]\n    # TODO(llion): Clean up this reshaping logic.\n    inputs = tf.expand_dims(inputs, axis=1)\n    if len(inputs.shape) < 5:\n      inputs = tf.expand_dims(inputs, axis=4)\n    s = common_layers.shape_list(inputs)\n    inputs = tf.reshape(inputs, [s[0] * s[1], s[2], s[3], s[4]])\n    # _shard_features called to ensure that the variable names match\n    inputs = self._shard_features({\"inputs\": inputs})[\"inputs\"]\n    input_modality = self._problem_hparams.modality[\"inputs\"]\n    input_vocab_size = self._problem_hparams.vocab_size[\"inputs\"]\n    if input_vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n      input_vocab_size += (-input_vocab_size) % hparams.vocab_divisor\n    modality_name = hparams.name.get(\"inputs\",\n                                     modalities.get_name(input_modality))(\n                                         hparams, input_vocab_size)\n    with tf.variable_scope(modality_name):\n      bottom = hparams.bottom.get(\"inputs\",\n                                  modalities.get_bottom(input_modality))\n      inputs = dp(bottom, inputs, hparams, input_vocab_size)\n    return inputs\n\n  def _fast_decode_tpu(self,\n                       features,\n                       decode_length,\n                       beam_size=1,\n                       top_beams=1,\n                       alpha=1.0):\n    \"\"\"Fast decoding.\n\n    Implements both greedy and beam search decoding on TPU, uses beam search\n    iff beam_size > 1, otherwise beam search related arguments are ignored.\n\n    Args:\n      features: A map of string to model features.\n      decode_length: An integer, how many additional timesteps to decode.\n      beam_size: An integer, number of beams.\n      top_beams: An integer, how many of the beams to return.\n      alpha: A float that controls the length penalty. Larger the alpha,\n        stronger the preference for longer translations.\n\n    Returns:\n      A dict of decoding results {\n          \"outputs\": integer `Tensor` of decoded ids of shape\n              [batch_size, <= decode_length] if beam_size == 1 or\n              [batch_size, top_beams, <= decode_length]\n          \"scores\": decoding log probs from the beam search,\n              None if using greedy decoding (beam_size=1)\n      }.\n\n    Raises:\n      NotImplementedError: If there are multiple data shards.\n    \"\"\"\n    if self._num_datashards != 1:\n      raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n    if \"targets_segmentation\" in features:\n      raise NotImplementedError(\n          \"Decoding not supported on packed datasets \"\n          \" If you want to decode from a dataset, use the non-packed version\"\n          \" of the dataset when decoding.\")\n    dp = self._data_parallelism\n    hparams = self._hparams\n    target_modality = self._problem_hparams.modality[\"targets\"]\n    target_vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n    if target_vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n      target_vocab_size += (-target_vocab_size) % hparams.vocab_divisor\n\n    if self.has_input:\n      inputs_shape = common_layers.shape_list(features[\"inputs\"])\n      if (target_modality == modalities.ModalityType.CLASS_LABEL or\n          self._problem_hparams.get(\"regression_targets\")):\n        decode_length = 1\n      else:\n        decode_length = (\n            inputs_shape[1] + features.get(\"decode_length\", decode_length))\n      batch_size = inputs_shape[0]\n      inputs = self._prepare_inputs_for_decode(features)\n      with tf.variable_scope(\"body\"):\n        encoder_output, encoder_decoder_attention_bias = dp(\n            self.encode,\n            inputs,\n            features[\"target_space_id\"],\n            hparams,\n            features=features)\n      encoder_output = encoder_output[0]\n      encoder_decoder_attention_bias = encoder_decoder_attention_bias[0]\n      partial_targets = None\n    else:\n      # The problem has no inputs.\n      encoder_output = None\n      encoder_decoder_attention_bias = None\n\n      # Prepare partial targets.\n      # In either features[\"inputs\"] or features[\"targets\"].\n      # We force the outputs to begin with these sequences.\n      partial_targets = features.get(\"inputs\")\n      if partial_targets is None:\n        partial_targets = features[\"targets\"]\n      assert partial_targets is not None\n      partial_targets = common_layers.expand_squeeze_to_nd(partial_targets, 2)\n      partial_targets = tf.to_int64(partial_targets)\n      partial_targets_shape = common_layers.shape_list(partial_targets)\n      partial_targets_length = partial_targets_shape[1]\n      decode_length = (\n          partial_targets_length + features.get(\"decode_length\", decode_length))\n      batch_size = partial_targets_shape[0]\n\n    if hparams.pos == \"timing\":\n      positional_encoding = common_attention.get_timing_signal_1d(\n          decode_length + 1, hparams.hidden_size)\n    elif hparams.pos == \"timing_from_features\":\n      positional_encoding = common_attention.add_timing_signals_from_features(\n          tf.zeros([1, decode_length + 1, hparams.hidden_size]), features,\n          hparams.position_features)\n    elif hparams.pos == \"emb\":\n      positional_encoding = common_attention.add_positional_embedding(\n          tf.zeros([1, decode_length + 1, hparams.hidden_size]),\n          hparams.max_length, \"body/targets_positional_embedding\", None)\n    else:\n      positional_encoding = None\n\n    def preprocess_targets(targets, i):\n      \"\"\"Performs preprocessing steps on the targets to prepare for the decoder.\n\n      This includes:\n        - Embedding the ids.\n        - Flattening to 3D tensor.\n        - Optionally adding timing signals.\n\n      Args:\n        targets: A tensor, inputs ids to the decoder. [batch_size, 1].\n        i: An integer, Step number of the decoding loop.\n\n      Returns:\n        A tensor, processed targets [batch_size, 1, hidden_dim].\n      \"\"\"\n      # _shard_features called to ensure that the variable names match\n      targets = self._shard_features({\"targets\": targets})[\"targets\"]\n      modality_name = hparams.name.get(\n          \"targets\",\n          modalities.get_name(target_modality))(hparams, target_vocab_size)\n      with tf.variable_scope(modality_name):\n        bottom = hparams.bottom.get(\n            \"targets\", modalities.get_targets_bottom(target_modality))\n        targets = dp(bottom, targets, hparams, target_vocab_size)[0]\n      targets = common_layers.flatten4d3d(targets)\n\n      # GO embeddings are all zero, this is because transformer_prepare_decoder\n      # Shifts the targets along by one for the input which pads with zeros.\n      # If the modality already maps GO to the zero embeddings this is not\n      # needed.\n      targets = tf.cond(\n          tf.equal(i, 0), lambda: tf.zeros_like(targets), lambda: targets)\n\n      if positional_encoding is not None:\n        positional_encoding_shape = positional_encoding.shape.as_list()\n        targets += tf.slice(\n            positional_encoding, [0, i, 0],\n            [positional_encoding_shape[0], 1, positional_encoding_shape[2]])\n      return targets\n\n    decoder_self_attention_bias = (\n        common_attention.attention_bias_lower_triangle(decode_length))\n    if hparams.proximity_bias:\n      decoder_self_attention_bias += common_attention.attention_bias_proximal(\n          decode_length)\n\n    def symbols_to_logits_tpu_fn(ids, i, cache):\n      \"\"\"Go from ids to logits for next symbol on TPU.\n\n      Args:\n        ids: A tensor, symbol IDs.\n        i: An integer, step number of the decoding loop. Only used for inference\n          on TPU.\n        cache: A dict, containing tensors which are the results of previous\n          attentions, used for fast decoding.\n\n      Returns:\n        ret: A tensor, computed logits.\n        cache: A dict, containing tensors which are the results of previous\n            attentions, used for fast decoding.\n      \"\"\"\n      ids = ids[:, -1:]\n      targets = tf.expand_dims(tf.expand_dims(ids, axis=2), axis=3)\n      targets = preprocess_targets(targets, i)\n\n      bias_shape = decoder_self_attention_bias.shape.as_list()\n      bias = tf.slice(decoder_self_attention_bias, [0, 0, i, 0],\n                      [bias_shape[0], bias_shape[1], 1, bias_shape[3]])\n\n      with tf.variable_scope(\"body\"):\n        body_outputs = dp(\n            self.decode,\n            targets,\n            cache.get(\"encoder_output\"),\n            cache.get(\"encoder_decoder_attention_bias\"),\n            bias,\n            hparams,\n            cache,\n            i,\n            nonpadding=features_to_nonpadding(features, \"targets\"))\n      modality_name = hparams.name.get(\n          \"targets\",\n          modalities.get_name(target_modality))(hparams, target_vocab_size)\n      with tf.variable_scope(modality_name):\n        top = hparams.top.get(\"targets\",\n                              modalities.get_top(target_modality))\n        logits = dp(top, body_outputs, None, hparams, target_vocab_size)[0]\n\n      ret = tf.squeeze(logits, axis=[1, 2, 3])\n      if partial_targets is not None:\n        # If the position is within the given partial targets, we alter the\n        # logits to always return those values.\n        # A faster approach would be to process the partial targets in one\n        # iteration in order to fill the corresponding parts of the cache.\n        # This would require broader changes, though.\n        vocab_size = tf.shape(ret)[1]\n\n        def forced_logits():\n          return tf.one_hot(\n              tf.tile(\n                  tf.slice(partial_targets, [0, i],\n                           [partial_targets.shape.as_list()[0], 1]),\n                  [beam_size]), vocab_size, 0.0, -1e9)\n\n        ret = tf.cond(\n            tf.less(i, partial_targets_length), forced_logits, lambda: ret)\n      return ret, cache\n\n    eos_id = self.get_decode_end_id() or beam_search.EOS_ID\n    temperature = features.get(\"sampling_temp\",\n                               getattr(hparams, \"sampling_temp\", 0.0))\n    top_k = features.get(\"sampling_keep_top_k\",\n                         getattr(hparams, \"sampling_keep_top_k\", -1))\n\n    ret = fast_decode_tpu(\n        encoder_output=encoder_output,\n        encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n        symbols_to_logits_fn=symbols_to_logits_tpu_fn,\n        hparams=hparams,\n        decode_length=decode_length,\n        vocab_size=target_vocab_size,\n        init_cache_fn=self._init_cache_fn,\n        beam_size=beam_size,\n        top_beams=top_beams,\n        alpha=alpha,\n        batch_size=batch_size,\n        force_decode_length=self._decode_hparams.force_decode_length,\n        eos_id=eos_id,\n        sampling_temperature=temperature,\n        top_k=top_k)\n    if partial_targets is not None:\n      if beam_size <= 1 or top_beams <= 1:\n        ret[\"outputs\"] = ret[\"outputs\"][:, partial_targets_length:]\n      else:\n        ret[\"outputs\"] = ret[\"outputs\"][:, :, partial_targets_length:]\n    return ret\n\n  def get_decode_start_id(self):\n    \"\"\"Returns the id of the first decoder input symbol.\n\n    The default case maps None to a vector of 0's for transformer. This method\n    can be overridden to return a different id by a model wanting to use a\n    different decoder start symbol. The id returned by this method is used to\n    index the embedding matrix, and retrieve the vector that will be used as the\n    first input to the decoder\n    \"\"\"\n    return None\n\n  def get_decode_end_id(self):\n    \"\"\"Returns the id of the output symbol that terminates decoding.\n\n    This method can be overridden by a different model. The id returned by this\n    method is used to check if the generation is complete during decoding.\n    \"\"\"\n    return None\n\n  def _fast_decode(self,\n                   features,\n                   decode_length,\n                   beam_size=1,\n                   top_beams=1,\n                   alpha=1.0,\n                   preprocess_targets_method=None):\n    \"\"\"Fast decoding.\n\n    Implements both greedy and beam search decoding, uses beam search iff\n    beam_size > 1, otherwise beam search related arguments are ignored.\n\n    Args:\n      features: a map of string to model  features.\n      decode_length: an integer.  How many additional timesteps to decode.\n      beam_size: number of beams.\n      top_beams: an integer. How many of the beams to return.\n      alpha: Float that controls the length penalty. larger the alpha, stronger\n        the preference for longer translations.\n      preprocess_targets_method: method used to preprocess targets. If None,\n      uses method \"preprocess_targets\" defined inside this method.\n\n    Returns:\n      A dict of decoding results {\n          \"outputs\": integer `Tensor` of decoded ids of shape\n              [batch_size, <= decode_length] if beam_size == 1 or\n              [batch_size, top_beams, <= decode_length]\n          \"scores\": decoding log probs from the beam search,\n              None if using greedy decoding (beam_size=1)\n      }\n\n    Raises:\n      NotImplementedError: If there are multiple data shards.\n    \"\"\"\n    if self._num_datashards != 1:\n      raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n    dp = self._data_parallelism\n    hparams = self._hparams\n    target_modality = self._problem_hparams.modality[\"targets\"]\n    target_vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n    if target_vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n      target_vocab_size += (-target_vocab_size) % hparams.vocab_divisor\n    if \"targets_segmentation\" in features:\n      raise NotImplementedError(\n          \"Decoding not supported on packed datasets \"\n          \" If you want to decode from a dataset, use the non-packed version\"\n          \" of the dataset when decoding.\")\n    if self.has_input:\n      inputs_shape = common_layers.shape_list(features[\"inputs\"])\n      if (target_modality == modalities.ModalityType.CLASS_LABEL or\n          self._problem_hparams.get(\"regression_targets\")):\n        decode_length = 1\n      else:\n        decode_length = (\n            inputs_shape[1] + features.get(\"decode_length\", decode_length))\n      batch_size = inputs_shape[0]\n      inputs = self._prepare_inputs_for_decode(features)\n      with tf.variable_scope(\"body\"):\n        encoder_output, encoder_decoder_attention_bias = dp(\n            self.encode,\n            inputs,\n            features[\"target_space_id\"],\n            hparams,\n            features=features)\n      encoder_output = encoder_output[0]\n      encoder_decoder_attention_bias = encoder_decoder_attention_bias[0]\n      partial_targets = features.get(\"partial_targets\")\n    else:\n      # The problem has no inputs.\n      encoder_output = None\n      encoder_decoder_attention_bias = None\n\n      # Prepare partial targets.\n      # In either features[\"inputs\"] or features[\"targets\"].\n      # We force the outputs to begin with these sequences.\n      partial_targets = features.get(\"inputs\")\n      if partial_targets is None:\n        partial_targets = features[\"targets\"]\n      assert partial_targets is not None\n\n    if partial_targets is not None:\n      partial_targets = common_layers.expand_squeeze_to_nd(partial_targets, 2)\n      partial_targets = tf.to_int64(partial_targets)\n      partial_targets_shape = common_layers.shape_list(partial_targets)\n      partial_targets_length = partial_targets_shape[1]\n      decode_length = (\n          partial_targets_length + features.get(\"decode_length\", decode_length))\n      batch_size = partial_targets_shape[0]\n\n    if hparams.pos == \"timing\":\n      positional_encoding = common_attention.get_timing_signal_1d(\n          decode_length + 1, hparams.hidden_size)\n    elif hparams.pos == \"timing_from_features\":\n      positional_encoding = common_attention.add_timing_signals_from_features(\n          tf.zeros([1, decode_length, hparams.hidden_size]), features,\n          hparams.position_features)\n    elif hparams.pos == \"emb\":\n      positional_encoding = common_attention.add_positional_embedding(\n          tf.zeros([1, decode_length, hparams.hidden_size]), hparams.max_length,\n          \"body/targets_positional_embedding\", None)\n    else:\n      positional_encoding = None\n\n    def preprocess_targets(targets, i):\n      \"\"\"Performs preprocessing steps on the targets to prepare for the decoder.\n\n      This includes:\n        - Embedding the ids.\n        - Flattening to 3D tensor.\n        - Optionally adding timing signals.\n\n      Args:\n        targets: inputs ids to the decoder. [batch_size, 1]\n        i: scalar, Step number of the decoding loop.\n\n      Returns:\n        Processed targets [batch_size, 1, hidden_dim]\n      \"\"\"\n      # _shard_features called to ensure that the variable names match\n      targets = self._shard_features({\"targets\": targets})[\"targets\"]\n      modality_name = hparams.name.get(\n          \"targets\",\n          modalities.get_name(target_modality))(hparams, target_vocab_size)\n      with tf.variable_scope(modality_name):\n        bottom = hparams.bottom.get(\n            \"targets\", modalities.get_targets_bottom(target_modality))\n        targets = dp(bottom, targets, hparams, target_vocab_size)[0]\n      targets = common_layers.flatten4d3d(targets)\n\n      # GO embeddings are all zero, this is because transformer_prepare_decoder\n      # Shifts the targets along by one for the input which pads with zeros.\n      # If the modality already maps GO to the zero embeddings this is not\n      # needed.\n      if not self.get_decode_start_id():\n        targets = tf.cond(\n            tf.equal(i, 0), lambda: tf.zeros_like(targets), lambda: targets)\n\n      if positional_encoding is not None:\n        targets += positional_encoding[:, i:i + 1]\n      return targets\n\n    decoder_self_attention_bias = (\n        common_attention.attention_bias_lower_triangle(decode_length))\n    if hparams.proximity_bias:\n      decoder_self_attention_bias += common_attention.attention_bias_proximal(\n          decode_length)\n\n    # Create tensors for encoder-decoder attention history\n    att_cache = {\"attention_history\": {}}\n    num_layers = hparams.num_decoder_layers or hparams.num_hidden_layers\n    if encoder_output is not None:\n      att_batch_size, enc_seq_length = common_layers.shape_list(\n          encoder_output)[0:2]\n      for layer in range(num_layers):\n        att_cache[\"attention_history\"][\"layer_%d\" % layer] = tf.zeros(\n            [att_batch_size, hparams.num_heads, 0, enc_seq_length])\n\n    def update_decoder_attention_history(cache):\n      \"\"\"Save attention weights in cache, e.g., for vizualization.\"\"\"\n      for k in [x for x in self.attention_weights\n                if \"decoder\" in x and \"self\" not in x and \"logits\" not in x]:\n        idx = k.find(\"layer_\")\n        if idx < 0:\n          continue\n        # Get layer number from the string name.\n        layer_nbr = k[idx + 6:]\n        idx = 0\n        while idx + 1 < len(layer_nbr) and layer_nbr[:idx + 1].isdigit():\n          idx += 1\n        layer_nbr = \"layer_%d\" % int(layer_nbr[:idx])\n        if layer_nbr in cache[\"attention_history\"]:\n          cache[\"attention_history\"][layer_nbr] = tf.concat(\n              [cache[\"attention_history\"][layer_nbr],\n               self.attention_weights[k]],\n              axis=2)\n    if not preprocess_targets_method:\n      preprocess_targets_method = preprocess_targets\n\n    def symbols_to_logits_fn(ids, i, cache):\n      \"\"\"Go from ids to logits for next symbol.\"\"\"\n      ids = ids[:, -1:]\n      targets = tf.expand_dims(tf.expand_dims(ids, axis=2), axis=3)\n      targets = preprocess_targets_method(targets, i)\n\n      bias = decoder_self_attention_bias[:, :, i:i + 1, :i + 1]\n      with tf.variable_scope(\"body\"):\n        body_outputs = dp(\n            self.decode,\n            targets,\n            cache.get(\"encoder_output\"),\n            cache.get(\"encoder_decoder_attention_bias\"),\n            bias,\n            hparams,\n            cache,\n            nonpadding=features_to_nonpadding(features, \"targets\"))\n\n      update_decoder_attention_history(cache)\n\n      modality_name = hparams.name.get(\n          \"targets\",\n          modalities.get_name(target_modality))(hparams, target_vocab_size)\n      with tf.variable_scope(modality_name):\n        top = hparams.top.get(\"targets\", modalities.get_top(target_modality))\n        logits = dp(top, body_outputs, None, hparams, target_vocab_size)[0]\n\n      ret = tf.squeeze(logits, axis=[1, 2, 3])\n      if partial_targets is not None:\n        # If the position is within the given partial targets, we alter the\n        # logits to always return those values.\n        # A faster approach would be to process the partial targets in one\n        # iteration in order to fill the corresponding parts of the cache.\n        # This would require broader changes, though.\n        vocab_size = tf.shape(ret)[1]\n\n        def forced_logits():\n          return tf.one_hot(\n              tf.tile(partial_targets[:, i], [beam_size]), vocab_size, 0.0,\n              -1e9)\n\n        ret = tf.cond(\n            tf.less(i, partial_targets_length), forced_logits, lambda: ret)\n      return ret, cache\n\n    sos_id = self.get_decode_start_id() or 0\n    eos_id = self.get_decode_end_id() or beam_search.EOS_ID\n    temperature = features.get(\"sampling_temp\",\n                               getattr(hparams, \"sampling_temp\", 0.0))\n    top_k = features.get(\"sampling_keep_top_k\",\n                         getattr(hparams, \"sampling_keep_top_k\", -1))\n\n    ret = fast_decode(\n        encoder_output=encoder_output,\n        encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n        symbols_to_logits_fn=symbols_to_logits_fn,\n        hparams=hparams,\n        decode_length=decode_length,\n        vocab_size=target_vocab_size,\n        init_cache_fn=self._init_cache_fn,\n        beam_size=beam_size,\n        top_beams=top_beams,\n        alpha=alpha,\n        batch_size=batch_size,\n        force_decode_length=self._decode_hparams.force_decode_length,\n        sos_id=sos_id,\n        eos_id=eos_id,\n        sampling_temperature=temperature,\n        top_k=top_k,\n        cache=att_cache)\n    if partial_targets is not None:\n      if beam_size <= 1 or top_beams <= 1:\n        ret[\"outputs\"] = ret[\"outputs\"][:, partial_targets_length:]\n      else:\n        ret[\"outputs\"] = ret[\"outputs\"][:, :, partial_targets_length:]\n    return ret\n\n\ndef _init_transformer_cache(cache, hparams, batch_size, attention_init_length,\n                            encoder_output, encoder_decoder_attention_bias,\n                            scope_prefix):\n  \"\"\"Create the initial cache for Transformer fast decoding.\"\"\"\n  key_channels = hparams.attention_key_channels or hparams.hidden_size\n  value_channels = hparams.attention_value_channels or hparams.hidden_size\n  num_layers = hparams.num_decoder_layers or hparams.num_hidden_layers\n  vars_3d_num_heads = (\n      hparams.num_heads if hparams.get(\"attention_variables_3d\") else 0)\n\n  if cache is None:\n    cache = {}\n  cache.update({\n      \"layer_%d\" % layer: {  # pylint: disable=g-complex-comprehension\n          \"k\":\n              common_attention.split_heads(\n                  tf.zeros([batch_size,\n                            attention_init_length,\n                            key_channels]), hparams.num_heads),\n          \"v\":\n              common_attention.split_heads(\n                  tf.zeros([batch_size,\n                            attention_init_length,\n                            value_channels]), hparams.num_heads),\n      } for layer in range(num_layers)\n  })\n\n  # If `ffn_layer` is in `[\"dense_relu_dense\" or \"conv_hidden_relu\"]`, then the\n  # cache key \"f\" won't be used, which means that the` shape of cache[\"f\"]`\n  # won't be changed to\n  # `[beamsize*batch_size, decode_length, hparams.hidden_size]` and may cause\n  # error when applying `nest.map reshape function` on it.\n  if hparams.ffn_layer not in [\"dense_relu_dense\", \"conv_hidden_relu\"]:\n    for layer in range(num_layers):\n      cache[\"layer_%d\" % layer][\"f\"] = tf.zeros(\n          [batch_size, 0, hparams.hidden_size])\n\n  if encoder_output is not None:\n    for layer in range(num_layers):\n      layer_name = \"layer_%d\" % layer\n      with tf.variable_scope(\n          \"%sdecoder/%s/encdec_attention/multihead_attention\" %\n          (scope_prefix, layer_name)):\n        k_encdec = common_attention.compute_attention_component(\n            encoder_output,\n            key_channels,\n            name=\"k\",\n            vars_3d_num_heads=vars_3d_num_heads)\n        k_encdec = common_attention.split_heads(k_encdec, hparams.num_heads)\n        v_encdec = common_attention.compute_attention_component(\n            encoder_output,\n            value_channels,\n            name=\"v\",\n            vars_3d_num_heads=vars_3d_num_heads)\n        v_encdec = common_attention.split_heads(v_encdec, hparams.num_heads)\n      cache[layer_name][\"k_encdec\"] = k_encdec\n      cache[layer_name][\"v_encdec\"] = v_encdec\n\n    cache[\"encoder_output\"] = encoder_output\n    cache[\"encoder_decoder_attention_bias\"] = encoder_decoder_attention_bias\n  return cache\n\n\ndef fast_decode_tpu(encoder_output,\n                    encoder_decoder_attention_bias,\n                    symbols_to_logits_fn,\n                    hparams,\n                    decode_length,\n                    vocab_size,\n                    init_cache_fn=_init_transformer_cache,\n                    beam_size=1,\n                    top_beams=1,\n                    alpha=1.0,\n                    sos_id=0,\n                    eos_id=beam_search.EOS_ID,\n                    batch_size=None,\n                    force_decode_length=False,\n                    scope_prefix=\"body/\",\n                    use_top_k_with_unique=True,\n                    sampling_temperature=0.0,\n                    top_k=-1):\n  \"\"\"Given encoder output and a symbols to logits function, does fast decoding.\n\n  Implements both greedy and beam search decoding for TPU, uses beam search iff\n  beam_size > 1, otherwise beam search related arguments are ignored.\n\n  Args:\n    encoder_output: A tensor, output from encoder.\n    encoder_decoder_attention_bias: A tensor, bias for use in encoder-decoder\n      attention.\n    symbols_to_logits_fn: Incremental decoding, function mapping triple `(ids,\n      step, cache)` to symbol logits.\n    hparams: Run hyperparameters.\n    decode_length: An integer, how many additional timesteps to decode.\n    vocab_size: Output vocabulary size.\n    init_cache_fn: Function that returns the initial cache dict.\n    beam_size: An integer, number of beams.\n    top_beams: An integer, how many of the beams to return.\n    alpha: A float that controls the length penalty. Larger the alpha, stronger\n      the preference for longer translations.\n    sos_id: Start-of-sequence symbol.\n    eos_id: End-of-sequence symbol.\n    batch_size: An integer, must be passed if there is no input.\n    force_decode_length: A bool, whether to force the full decode length, or if\n      False, stop when all beams hit eos_id.\n    scope_prefix: str, prefix for decoder layer variable scopes.\n    use_top_k_with_unique: bool, whether to use a fast (but decreased precision)\n      top_k during beam search.\n    sampling_temperature: scalar, temperature with which to sample.\n    top_k: scalar, sample only top k.\n\n  Returns:\n    A dict of decoding results {\n        \"outputs\": integer `Tensor` of decoded ids of shape\n            [batch_size, <= decode_length] if top_beams == 1 or\n            [batch_size, top_beams, <= decode_length] otherwise\n        \"scores\": decoding log probs from the beam search,\n            None if using greedy decoding (beam_size=1)\n    }.\n\n  Raises:\n    NotImplementedError: If beam size > 1 with partial targets.\n  \"\"\"\n  if encoder_output is not None:\n    batch_size = common_layers.shape_list(encoder_output)[0]\n\n  cache = init_cache_fn(None, hparams, batch_size, decode_length,\n                        encoder_output, encoder_decoder_attention_bias,\n                        scope_prefix)\n\n  mlperf_log.transformer_print(\n      key=mlperf_log.MODEL_HP_SEQ_BEAM_SEARCH,\n      value={\n          \"vocab_size\": vocab_size,\n          \"batch_size\": batch_size,\n          \"beam_size\": beam_size,\n          \"alpha\": alpha,\n          \"max_decode_length\": decode_length\n      },\n      hparams=hparams)\n  if beam_size > 1:  # Beam Search\n    initial_ids = sos_id * tf.ones([batch_size], dtype=tf.int32)\n    decoded_ids, scores, _ = beam_search.beam_search(\n        symbols_to_logits_fn,\n        initial_ids,\n        beam_size,\n        decode_length,\n        vocab_size,\n        alpha,\n        states=cache,\n        eos_id=eos_id,\n        stop_early=(top_beams == 1),\n        use_tpu=True,\n        use_top_k_with_unique=use_top_k_with_unique)\n\n    if top_beams == 1:\n      decoded_ids = decoded_ids[:, 0, 1:]\n      scores = scores[:, 0]\n    else:\n      decoded_ids = decoded_ids[:, :top_beams, 1:]\n      scores = scores[:, :top_beams]\n  else:  # Greedy\n\n    def inner_loop(i, hit_eos, next_id, decoded_ids, cache, log_prob):\n      \"\"\"One step of greedy decoding.\"\"\"\n      logits, cache = symbols_to_logits_fn(next_id, i, cache)\n      log_probs = common_layers.log_prob_from_logits(logits)\n      temperature = sampling_temperature\n      if hparams.sampling_method == \"random_per_example\":\n        next_id = common_layers.sample_temperature_per_example(\n            logits, temperature, top_k)\n      else:\n        if hparams.sampling_method == \"argmax\":\n          temperature = 0.0\n        next_id = common_layers.sample_with_temperature(logits, temperature,\n                                                        top_k)\n\n      log_prob_indices = tf.stack([tf.range(tf.to_int64(batch_size)), next_id],\n                                  axis=1)\n      log_prob += tf.gather_nd(\n          log_probs, log_prob_indices) * (1 - tf.to_float(hit_eos))\n      # Note(thangluong): we purposely update hit_eos after aggregating log_prob\n      # There is a subtle detail here that we want to include log_probs up to\n      # (and inclusive of) the first eos generated, but not subsequent tokens.\n      hit_eos |= tf.equal(next_id, eos_id)\n\n      next_id = tf.expand_dims(next_id, axis=1)\n      decoded_ids = tf.transpose(decoded_ids)\n      decoded_ids = inplace_ops.alias_inplace_update(\n          decoded_ids, i, tf.squeeze(next_id, axis=1))\n      decoded_ids = tf.transpose(decoded_ids)\n      return i + 1, hit_eos, next_id, decoded_ids, cache, log_prob\n\n    def is_not_finished(i, hit_eos, *_):\n      finished = i >= decode_length\n      if not force_decode_length:\n        finished |= tf.reduce_all(hit_eos)\n      return tf.logical_not(finished)\n\n    decoded_ids = tf.zeros([batch_size, decode_length], dtype=tf.int64)\n    hit_eos = tf.fill([batch_size], False)\n    next_id = sos_id * tf.ones([batch_size, 1], dtype=tf.int64)\n    initial_log_prob = tf.zeros([batch_size], dtype=tf.float32)\n\n    def compute_cache_shape_invariants(tensor):\n      return tf.TensorShape(tensor.shape.as_list())\n\n    _, _, _, decoded_ids, _, log_prob = tf.while_loop(\n        is_not_finished,\n        inner_loop, [\n            tf.constant(0), hit_eos, next_id, decoded_ids, cache,\n            initial_log_prob\n        ],\n        shape_invariants=[\n            tf.TensorShape([]),\n            tf.TensorShape([batch_size]),\n            tf.TensorShape([batch_size, 1]),\n            tf.TensorShape([batch_size, decode_length]),\n            nest.map_structure(compute_cache_shape_invariants, cache),\n            tf.TensorShape([batch_size]),\n        ])\n    scores = log_prob\n\n  return {\"outputs\": decoded_ids, \"scores\": scores}\n\n\ndef fast_decode(encoder_output,\n                encoder_decoder_attention_bias,\n                symbols_to_logits_fn,\n                hparams,\n                decode_length,\n                vocab_size,\n                init_cache_fn=_init_transformer_cache,\n                beam_size=1,\n                top_beams=1,\n                alpha=1.0,\n                sos_id=0,\n                eos_id=beam_search.EOS_ID,\n                batch_size=None,\n                force_decode_length=False,\n                scope_prefix=\"body/\",\n                sampling_temperature=0.0,\n                top_k=-1,\n                cache=None):\n  \"\"\"Given encoder output and a symbols to logits function, does fast decoding.\n\n  Implements both greedy and beam search decoding, uses beam search iff\n  beam_size > 1, otherwise beam search related arguments are ignored.\n\n  Args:\n    encoder_output: Output from encoder.\n    encoder_decoder_attention_bias: a bias tensor for use in encoder-decoder\n      attention\n    symbols_to_logits_fn: Incremental decoding; function mapping triple `(ids,\n      step, cache)` to symbol logits.\n    hparams: run hyperparameters\n    decode_length: an integer.  How many additional timesteps to decode.\n    vocab_size: Output vocabulary size.\n    init_cache_fn: Function that returns the initial cache dict.\n    beam_size: number of beams.\n    top_beams: an integer. How many of the beams to return.\n    alpha: Float that controls the length penalty. larger the alpha, stronger\n      the preference for longer translations.\n    sos_id: End-of-sequence symbol in beam search.\n    eos_id: End-of-sequence symbol in beam search.\n    batch_size: an integer scalar - must be passed if there is no input\n    force_decode_length: bool, whether to force the full decode length, or if\n      False, stop when all beams hit eos_id.\n    scope_prefix: str, prefix for decoder layer variable scopes.\n    sampling_temperature: scalar, temperature with which to sample.\n    top_k: scalar, sample only top k.\n    cache: cache dictionary for additional predictions.\n\n  Returns:\n      A dict of decoding results {\n          \"outputs\": integer `Tensor` of decoded ids of shape\n              [batch_size, <= decode_length] if top_beams == 1 or\n              [batch_size, top_beams, <= decode_length] otherwise\n          \"scores\": decoding log probs from the beam search,\n              None if using greedy decoding (beam_size=1)\n      }\n  \"\"\"\n  if encoder_output is not None:\n    batch_size = common_layers.shape_list(encoder_output)[0]\n\n  cache = init_cache_fn(\n      cache=cache,\n      hparams=hparams,\n      batch_size=batch_size,\n      attention_init_length=0,\n      encoder_output=encoder_output,\n      encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n      scope_prefix=scope_prefix)\n\n  if beam_size > 1:  # Beam Search\n    initial_ids = sos_id * tf.ones([batch_size], dtype=tf.int32)\n    decoded_ids, scores, cache = beam_search.beam_search(\n        symbols_to_logits_fn,\n        initial_ids,\n        beam_size,\n        decode_length,\n        vocab_size,\n        alpha,\n        states=cache,\n        eos_id=eos_id,\n        stop_early=(top_beams == 1))\n\n    if top_beams == 1:\n      decoded_ids = decoded_ids[:, 0, 1:]\n      scores = scores[:, 0]\n    else:\n      decoded_ids = decoded_ids[:, :top_beams, 1:]\n      scores = scores[:, :top_beams]\n  else:  # Greedy\n\n    def inner_loop(i, hit_eos, next_id, decoded_ids, cache, log_prob):\n      \"\"\"One step of greedy decoding.\"\"\"\n      logits, cache = symbols_to_logits_fn(next_id, i, cache)\n      log_probs = common_layers.log_prob_from_logits(logits)\n      temperature = sampling_temperature\n      if hparams.sampling_method == \"random_per_example\":\n        next_id = common_layers.sample_temperature_per_example(\n            logits, temperature, top_k)\n      else:\n        if hparams.sampling_method == \"argmax\":\n          temperature = 0.0\n        next_id = common_layers.sample_with_temperature(logits, temperature,\n                                                        top_k)\n\n      log_prob_indices = tf.stack([tf.range(tf.to_int64(batch_size)), next_id],\n                                  axis=1)\n      log_prob += tf.gather_nd(\n          log_probs, log_prob_indices) * (1 - tf.to_float(hit_eos))\n      # Note(thangluong): we purposely update hit_eos after aggregating log_prob\n      # There is a subtle detail here that we want to include log_probs up to\n      # (and inclusive of) the first eos generated, but not subsequent tokens.\n      hit_eos |= tf.equal(next_id, eos_id)\n\n      next_id = tf.expand_dims(next_id, axis=1)\n      decoded_ids = tf.concat([decoded_ids, next_id], axis=1)\n\n      return i + 1, hit_eos, next_id, decoded_ids, cache, log_prob\n\n    def is_not_finished(i, hit_eos, *_):\n      finished = i >= decode_length\n      if not force_decode_length:\n        finished |= tf.reduce_all(hit_eos)\n      return tf.logical_not(finished)\n\n    decoded_ids = tf.zeros([batch_size, 0], dtype=tf.int64)\n    hit_eos = tf.fill([batch_size], False)\n    next_id = sos_id * tf.ones([batch_size, 1], dtype=tf.int64)\n    initial_log_prob = tf.zeros([batch_size], dtype=tf.float32)\n    _, _, _, decoded_ids, cache, log_prob = tf.while_loop(\n        is_not_finished,\n        inner_loop, [\n            tf.constant(0), hit_eos, next_id, decoded_ids, cache,\n            initial_log_prob\n        ],\n        shape_invariants=[\n            tf.TensorShape([]),\n            tf.TensorShape([None]),\n            tf.TensorShape([None, None]),\n            tf.TensorShape([None, None]),\n            nest.map_structure(beam_search.get_state_shape_invariants, cache),\n            tf.TensorShape([None]),\n        ])\n    scores = log_prob\n\n  return {\"outputs\": decoded_ids, \"scores\": scores, \"cache\": cache}\n\n\n@registry.register_model\nclass TransformerScorer(Transformer):\n  \"\"\"Transformer model, but only scores in PREDICT mode.\n\n  Checkpoints between Transformer and TransformerScorer are interchangeable.\n  \"\"\"\n\n  def __init__(self, *args, **kwargs):\n    super(TransformerScorer, self).__init__(*args, **kwargs)\n    self._name = \"transformer\"\n    self._base_name = \"transformer\"\n\n  def infer(self,\n            features=None,\n            decode_length=50,\n            beam_size=1,\n            top_beams=1,\n            alpha=0.0,\n            use_tpu=False):\n    \"\"\"Returns the targets and their log probabilities.\"\"\"\n    del decode_length, beam_size, top_beams, alpha, use_tpu\n    assert features is not None\n\n    # Run the model\n    self.hparams.force_full_predict = True\n    with tf.variable_scope(self.name):\n      logits, _ = self.model_fn(features)\n    assert len(logits.shape) == 5  # [batch, time, 1, 1, vocab]\n    logits = tf.squeeze(logits, [2, 3])\n\n    # Compute the log probabilities\n    log_probs = common_layers.log_prob_from_logits(logits)\n\n    targets = features[\"targets\"]\n    assert len(targets.shape) == 4  # [batch, time, 1, 1]\n    targets = tf.squeeze(targets, [2, 3])\n\n    # Slice out the log_probs of the targets\n    log_probs = common_layers.index_last_dim_with_indices(log_probs, targets)\n\n    # Sum over time to get the log_prob of the sequence\n    scores = tf.reduce_sum(log_probs, axis=1)\n\n    return {\"outputs\": targets, \"scores\": scores}\n\n\n@registry.register_model\nclass TransformerEncoder(t2t_model.T2TModel):\n  \"\"\"Transformer, encoder only.\"\"\"\n\n  def body(self, features):\n    hparams = self._hparams\n    inputs = features[\"inputs\"]\n    target_space = features[\"target_space_id\"]\n\n    inputs = common_layers.flatten4d3d(inputs)\n\n    (encoder_input, encoder_self_attention_bias, _) = (\n        transformer_prepare_encoder(inputs, target_space, hparams))\n\n    encoder_input = tf.nn.dropout(encoder_input,\n                                  1.0 - hparams.layer_prepostprocess_dropout)\n    encoder_output = transformer_encoder(\n        encoder_input,\n        encoder_self_attention_bias,\n        hparams,\n        nonpadding=features_to_nonpadding(features, \"inputs\"))\n    encoder_output = tf.expand_dims(encoder_output, 2)\n\n    return encoder_output\n\n\n@registry.register_model\nclass TransformerRegressor(TransformerEncoder):\n  \"\"\"Transformer inheriting from Encoder, for the regression problem.\n\n  Final result is a tensor that has a shape of (?, 1, 1, 1).\n  \"\"\"\n\n  def top(self, body_output, features):\n    \"\"\"Computes single scalar value from body_output.\"\"\"\n\n    with tf.variable_scope(\"reg_top_ffn\"):\n      x = body_output\n      x = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n      res = tf.layers.dense(x, 1, name=\"model_top\")\n      return res\n\n\ndef features_to_nonpadding(features, inputs_or_targets=\"inputs\"):\n  key = inputs_or_targets + \"_segmentation\"\n  if features and key in features:\n    return tf.minimum(tf.to_float(features[key]), 1.0)\n  return None\n\n\ndef transformer_prepare_decoder(targets, hparams, features=None, pad=None):\n  \"\"\"Prepare one shard of the model for the decoder.\n\n  Args:\n    targets: a Tensor.\n    hparams: run hyperparameters\n    features: optionally pass the entire features dictionary as well. This is\n      needed now for \"packed\" datasets.\n    pad: vector to use for padding when shifting targets right\n\n  Returns:\n    decoder_input: a Tensor, bottom of decoder stack\n    decoder_self_attention_bias: a bias tensor for use in decoder self-attention\n  \"\"\"\n  if hparams.causal_decoder_self_attention:\n    # Causal attention.\n    if hparams.prepend_mode == \"prepend_inputs_full_attention\":\n      decoder_self_attention_bias = (\n          common_attention.attention_bias_prepend_inputs_full_attention(\n              common_attention.embedding_to_padding(targets)))\n    else:\n      decoder_self_attention_bias = (\n          common_attention.attention_bias_lower_triangle(\n              common_layers.shape_list(targets)[1]))\n  else:\n    # Full attention.\n    decoder_padding = common_attention.embedding_to_padding(targets)\n    decoder_self_attention_bias = (\n        common_attention.attention_bias_ignore_padding(decoder_padding))\n\n  if features and \"targets_segmentation\" in features:\n    # \"Packed\" dataset - keep the examples from seeing each other.\n    targets_segmentation = features[\"targets_segmentation\"]\n    targets_position = features[\"targets_position\"]\n    decoder_self_attention_bias += common_attention.attention_bias_same_segment(\n        targets_segmentation, targets_segmentation)\n  else:\n    targets_position = None\n  if hparams.proximity_bias:\n    decoder_self_attention_bias += common_attention.attention_bias_proximal(\n        common_layers.shape_list(targets)[1])\n  decoder_input = common_layers.shift_right_3d(targets, pad)\n  if hparams.pos == \"timing\":\n    if targets_position is not None:\n      decoder_input = common_attention.add_timing_signal_1d_given_position(\n          decoder_input, targets_position)\n    else:\n      decoder_input = common_attention.add_timing_signal_1d(decoder_input)\n  elif hparams.pos == \"timing_from_features\":\n    decoder_input = common_attention.add_timing_signals_from_features(\n        decoder_input, features, hparams.position_features)\n  elif hparams.pos == \"emb\":\n    decoder_input = common_attention.add_positional_embedding(\n        decoder_input, hparams.max_length, \"targets_positional_embedding\",\n        targets_position)\n\n  if hparams.activation_dtype == \"bfloat16\":\n    decoder_self_attention_bias = tf.cast(decoder_self_attention_bias,\n                                          tf.bfloat16)\n  return (decoder_input, decoder_self_attention_bias)\n\n\ndef transformer_self_attention_layer(decoder_input,\n                                     decoder_self_attention_bias,\n                                     layer_idx,\n                                     hparams,\n                                     encoder_output=None,\n                                     encoder_decoder_attention_bias=None,\n                                     cache=None,\n                                     decode_loop_step=None,\n                                     save_weights_to=None,\n                                     make_image_summary=False,\n                                     layer_collection=None,\n                                     recurrent_memory_by_layer=None,\n                                     chunk_number=None):\n  \"\"\"A single transformer self-attention layer.\"\"\"\n  x = decoder_input\n  layer = layer_idx\n  layer_name = \"layer_%d\" % layer\n  layer_cache = cache[layer_name] if cache is not None else None\n\n  attention_dropout_broadcast_dims = (\n      common_layers.comma_separated_string_to_integer_list(\n          getattr(hparams, \"attention_dropout_broadcast_dims\", \"\")))\n\n  if recurrent_memory_by_layer is not None:\n    recurrent_memory = recurrent_memory_by_layer[layer_name]\n  else:\n    recurrent_memory = None\n\n  if layer < hparams.get(\"num_area_layers\", 0):\n    max_area_width = hparams.get(\"max_area_width\", 1)\n    max_area_height = hparams.get(\"max_area_height\", 1)\n    memory_height = hparams.get(\"max_area_height\", 1)\n  else:\n    max_area_width = 1\n    max_area_height = 1\n    memory_height = 1\n  with tf.variable_scope(layer_name):\n    with tf.variable_scope(\"self_attention\"):\n      y = common_attention.multihead_attention(\n          common_layers.layer_preprocess(\n              x, hparams, layer_collection=layer_collection),\n          None,\n          decoder_self_attention_bias,\n          hparams.attention_key_channels or hparams.hidden_size,\n          hparams.attention_value_channels or hparams.hidden_size,\n          hparams.hidden_size,\n          hparams.num_heads,\n          hparams.attention_dropout,\n          attention_type=hparams.self_attention_type,\n          max_relative_position=hparams.max_relative_position,\n          heads_share_relative_embedding=(\n              hparams.heads_share_relative_embedding),\n          add_relative_to_values=hparams.add_relative_to_values,\n          save_weights_to=save_weights_to,\n          cache=layer_cache,\n          make_image_summary=make_image_summary,\n          dropout_broadcast_dims=attention_dropout_broadcast_dims,\n          max_length=hparams.get(\"max_length\"),\n          decode_loop_step=decode_loop_step,\n          vars_3d=hparams.get(\"attention_variables_3d\"),\n          activation_dtype=hparams.get(\"activation_dtype\", \"float32\"),\n          weight_dtype=hparams.get(\"weight_dtype\", \"float32\"),\n          layer_collection=layer_collection,\n          recurrent_memory=recurrent_memory,\n          chunk_number=chunk_number,\n          hard_attention_k=hparams.get(\"hard_attention_k\", 0),\n          gumbel_noise_weight=hparams.get(\"gumbel_noise_weight\", 0.0),\n          max_area_width=max_area_width,\n          max_area_height=max_area_height,\n          memory_height=memory_height,\n          area_key_mode=hparams.get(\"area_key_mode\", \"none\"),\n          area_value_mode=hparams.get(\"area_value_mode\", \"none\"),\n          training=(hparams.get(\n              \"mode\",\n              tf_estimator.ModeKeys.TRAIN) == tf_estimator.ModeKeys.TRAIN))\n      x = common_layers.layer_postprocess(x, y, hparams)\n    if encoder_output is not None:\n      if not isinstance(encoder_output, (list,)):\n        encoder_output = [encoder_output]\n      with tf.variable_scope(\"encdec_attention\"):\n        for enc_output in encoder_output:\n          y = common_attention.multihead_attention(\n              common_layers.layer_preprocess(\n                  x, hparams, layer_collection=layer_collection),\n              enc_output,\n              encoder_decoder_attention_bias,\n              hparams.attention_key_channels or hparams.hidden_size,\n              hparams.attention_value_channels or hparams.hidden_size,\n              hparams.hidden_size,\n              hparams.num_heads,\n              hparams.attention_dropout,\n              max_relative_position=hparams.max_relative_position,\n              heads_share_relative_embedding=(\n                  hparams.heads_share_relative_embedding),\n              add_relative_to_values=hparams.add_relative_to_values,\n              save_weights_to=save_weights_to,\n              cache=layer_cache,\n              make_image_summary=make_image_summary,\n              dropout_broadcast_dims=attention_dropout_broadcast_dims,\n              max_length=hparams.get(\"max_length\"),\n              vars_3d=hparams.get(\"attention_variables_3d\"),\n              activation_dtype=hparams.get(\"activation_dtype\", \"float32\"),\n              weight_dtype=hparams.get(\"weight_dtype\", \"float32\"),\n              layer_collection=layer_collection,\n              hard_attention_k=hparams.get(\"hard_attention_k\", 0),\n              gumbel_noise_weight=hparams.get(\"gumbel_noise_weight\", 0.0),\n              max_area_width=max_area_width,\n              max_area_height=max_area_height,\n              memory_height=memory_height,\n              area_key_mode=hparams.get(\"area_key_mode\", \"none\"),\n              area_value_mode=hparams.get(\"area_value_mode\", \"none\"),\n              training=(hparams.get(\n                  \"mode\",\n                  tf_estimator.ModeKeys.TRAIN) == tf_estimator.ModeKeys.TRAIN))\n          x = common_layers.layer_postprocess(x, y, hparams)\n    return x, layer_cache\n\n\ndef transformer_decoder_layer(decoder_input,\n                              decoder_self_attention_bias,\n                              layer_idx,\n                              hparams,\n                              encoder_output=None,\n                              encoder_decoder_attention_bias=None,\n                              cache=None,\n                              decode_loop_step=None,\n                              nonpadding=None,\n                              save_weights_to=None,\n                              make_image_summary=False,\n                              losses=None,\n                              layer_collection=None,\n                              recurrent_memory_by_layer=None,\n                              chunk_number=None):\n  \"\"\"A single transformer decoder layer.\"\"\"\n  x, layer_cache = transformer_self_attention_layer(\n      decoder_input=decoder_input,\n      decoder_self_attention_bias=decoder_self_attention_bias,\n      layer_idx=layer_idx,\n      hparams=hparams,\n      encoder_output=encoder_output,\n      encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n      cache=cache,\n      decode_loop_step=decode_loop_step,\n      save_weights_to=save_weights_to,\n      make_image_summary=make_image_summary,\n      layer_collection=layer_collection,\n      recurrent_memory_by_layer=recurrent_memory_by_layer,\n      chunk_number=chunk_number)\n\n  layer = layer_idx\n  layer_name = \"layer_%d\" % layer\n  with tf.variable_scope(layer_name):\n    with tf.variable_scope(\"ffn\"):\n      y = transformer_ffn_layer(\n          common_layers.layer_preprocess(\n              x, hparams, layer_collection=layer_collection),\n          hparams,\n          conv_padding=\"LEFT\",\n          nonpadding_mask=nonpadding,\n          losses=losses,\n          cache=layer_cache,\n          decode_loop_step=decode_loop_step,\n          layer_collection=layer_collection)\n      x = common_layers.layer_postprocess(x, y, hparams)\n      return x\n\n\ndef transformer_decoder(decoder_input,\n                        encoder_output,\n                        decoder_self_attention_bias,\n                        encoder_decoder_attention_bias,\n                        hparams,\n                        cache=None,\n                        decode_loop_step=None,\n                        name=\"decoder\",\n                        nonpadding=None,\n                        save_weights_to=None,\n                        make_image_summary=True,\n                        losses=None,\n                        layer_collection=None,\n                        recurrent_memory_by_layer=None,\n                        chunk_number=None):\n  \"\"\"A stack of transformer layers.\n\n  Args:\n    decoder_input: a Tensor\n    encoder_output: a Tensor\n    decoder_self_attention_bias: bias Tensor for self-attention (see\n      common_attention.attention_bias())\n    encoder_decoder_attention_bias: bias Tensor for encoder-decoder attention\n      (see common_attention.attention_bias())\n    hparams: hyperparameters for model\n    cache: dict, containing tensors which are the results of previous\n      attentions, used for fast decoding.\n    decode_loop_step: An integer, step number of the decoding loop. Only used\n      for inference on TPU.\n    name: a string\n    nonpadding: optional Tensor with shape [batch_size, encoder_length]\n      indicating what positions are not padding.  This is used to mask out\n      padding in convolutional layers.  We generally only need this mask for\n      \"packed\" datasets, because for ordinary datasets, no padding is ever\n      followed by nonpadding.\n    save_weights_to: an optional dictionary to capture attention weights for\n      visualization; the weights tensor will be appended there under a string\n      key created from the variable scope (including name).\n    make_image_summary: Whether to make an attention image summary.\n    losses: optional list onto which to append extra training losses\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the KFAC\n      optimizer. Default is None.\n    recurrent_memory_by_layer: Optional dict, mapping layer names to instances\n      of transformer_memory.RecurrentMemory. Default is None.\n    chunk_number: an optional integer Tensor with shape [batch] used to operate\n      the recurrent_memory.\n\n  Returns:\n    y: a Tensors\n  \"\"\"\n  x = decoder_input\n\n  mlperf_log.transformer_print(\n      key=mlperf_log.MODEL_HP_NUM_HIDDEN_LAYERS,\n      value=hparams.num_decoder_layers or hparams.num_hidden_layers,\n      hparams=hparams)\n  mlperf_log.transformer_print(\n      key=mlperf_log.MODEL_HP_ATTENTION_DROPOUT,\n      value=hparams.attention_dropout,\n      hparams=hparams)\n  mlperf_log.transformer_print(\n      key=mlperf_log.MODEL_HP_ATTENTION_DENSE,\n      value={\n          \"use_bias\": \"false\",\n          \"num_heads\": hparams.num_heads,\n          \"hidden_size\": hparams.hidden_size\n      },\n      hparams=hparams)\n\n  with tf.variable_scope(name):\n    for layer_idx in range(hparams.num_decoder_layers or\n                           hparams.num_hidden_layers):\n      x = transformer_decoder_layer(\n          x,\n          decoder_self_attention_bias,\n          layer_idx,\n          hparams,\n          encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n          encoder_output=encoder_output,\n          cache=cache,\n          decode_loop_step=decode_loop_step,\n          nonpadding=nonpadding,\n          save_weights_to=save_weights_to,\n          make_image_summary=make_image_summary,\n          losses=losses,\n          layer_collection=layer_collection,\n          recurrent_memory_by_layer=recurrent_memory_by_layer,\n          chunk_number=chunk_number\n          )\n\n    # if normalization is done in layer_preprocess, then it should also be done\n    # on the output, since the output can grow very large, being the sum of\n    # a whole stack of unnormalized layer outputs.\n    mlperf_log.transformer_print(\n        key=mlperf_log.MODEL_HP_NORM,\n        value={\"hidden_size\": hparams.hidden_size})\n    return common_layers.layer_preprocess(\n        x, hparams, layer_collection=layer_collection)\n\n\n@registry.register_model\nclass TransformerMemory(Transformer):\n  \"\"\"Transformer language model with memory across chunks.\"\"\"\n\n  # TODO(kitaev): consider overriding set_mode to swap out recurrent memory when\n  # switching between training and evaluation.\n\n  def __init__(self, *args, **kwargs):\n    super(TransformerMemory, self).__init__(*args, **kwargs)\n\n    hparams = self._hparams\n    self.recurrent_memory_by_layer = {}\n    for layer in range(hparams.num_decoder_layers or hparams.num_hidden_layers):\n      layer_name = \"layer_%d\" % layer\n      if hparams.memory_type == \"neural_memory\":\n        memory = transformer_memory.TransformerMemory(\n            batch_size=int(hparams.batch_size / hparams.max_length),\n            key_depth=hparams.hidden_size,\n            val_depth=hparams.hidden_size,\n            memory_size=hparams.split_targets_chunk_length,\n            sharpen_factor=1.,\n            name=layer_name + \"/recurrent_memory\")\n      elif hparams.memory_type == \"transformer_xl\":\n        memory = transformer_memory.RecentTokensMemory(\n            layer_name + \"/recurrent_memory\", hparams)\n      else:\n        raise ValueError(\"Unsupported memory type: %s\" % hparams.memory_type)\n      self.recurrent_memory_by_layer[layer_name] = memory\n\n  @property\n  def has_input(self):\n    if hasattr(self._hparams, \"unconditional\") and self._hparams.unconditional:\n      return False\n    return super(TransformerMemory, self).has_input\n\n  def _beam_decode(self, features, decode_length, beam_size, top_beams, alpha,\n                   use_tpu=False):\n    \"\"\"Overriding beam search because for now only the slow version works with\n    memory\n    \"\"\"\n    return self._beam_decode_slow(features, decode_length, beam_size,\n                                  top_beams, alpha, use_tpu)\n\n\n@registry.register_hparams\ndef transformer_base_v1():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.norm_type = \"layer\"\n  hparams.hidden_size = 512\n  hparams.batch_size = 4096\n  hparams.max_length = 256\n  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping\n  hparams.optimizer_adam_epsilon = 1e-9\n  hparams.learning_rate_schedule = \"legacy\"\n  hparams.learning_rate_decay_scheme = \"noam\"\n  hparams.learning_rate = 0.1\n  hparams.learning_rate_warmup_steps = 4000\n  hparams.initializer_gain = 1.0\n  hparams.num_hidden_layers = 6\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.weight_decay = 0.0\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.98\n  hparams.num_sampled_classes = 0\n  hparams.label_smoothing = 0.1\n  hparams.shared_embedding_and_softmax_weights = True\n  hparams.symbol_modality_num_shards = 16\n\n  # Add new ones like this.\n  hparams.add_hparam(\"filter_size\", 2048)\n  # Layer-related flags. If zero, these fall back on hparams.num_hidden_layers.\n  hparams.add_hparam(\"num_encoder_layers\", 0)\n  hparams.add_hparam(\"num_decoder_layers\", 0)\n  # Attention-related flags.\n  hparams.add_hparam(\"num_heads\", 8)\n  hparams.add_hparam(\"attention_key_channels\", 0)\n  hparams.add_hparam(\"attention_value_channels\", 0)\n  hparams.add_hparam(\"ffn_layer\", \"dense_relu_dense\")\n  hparams.add_hparam(\"parameter_attention_key_channels\", 0)\n  hparams.add_hparam(\"parameter_attention_value_channels\", 0)\n  # All hyperparameters ending in \"dropout\" are automatically set to 0.0\n  # when not in training mode.\n  hparams.add_hparam(\"attention_dropout\", 0.0)\n  hparams.add_hparam(\"attention_dropout_broadcast_dims\", \"\")\n  hparams.add_hparam(\"relu_dropout\", 0.0)\n  hparams.add_hparam(\"relu_dropout_broadcast_dims\", \"\")\n  hparams.add_hparam(\"pos\", \"timing\")  # timing, none\n  hparams.add_hparam(\"position_features\", \"\")\n  hparams.add_hparam(\"nbr_decoder_problems\", 1)\n  hparams.add_hparam(\"proximity_bias\", False)\n  hparams.add_hparam(\"causal_decoder_self_attention\", True)\n  hparams.add_hparam(\"use_pad_remover\", True)\n  hparams.add_hparam(\"self_attention_type\", \"dot_product\")\n  hparams.add_hparam(\"conv_first_kernel\", 3)\n  hparams.add_hparam(\"attention_variables_3d\", False)\n  hparams.add_hparam(\"use_target_space_embedding\", True)\n  # These parameters are only used when ffn_layer==\"local_moe_tpu\"\n  hparams.add_hparam(\"moe_overhead_train\", 1.0)\n  hparams.add_hparam(\"moe_overhead_eval\", 2.0)\n  hparams.moe_num_experts = 16\n  hparams.moe_loss_coef = 1e-3\n  # If specified, use this value instead of problem name in metrics.py.\n  # This is useful for programs that can automatically compare experiments side\n  #   by side based on the same metric names.\n  hparams.add_hparam(\"overload_eval_metric_name\", \"\")\n  # For making a transformer encoder unidirectional by using masked\n  # attention.\n  hparams.add_hparam(\"unidirectional_encoder\", False)\n  # For hard attention.\n  hparams.add_hparam(\"hard_attention_k\", 0)\n  hparams.add_hparam(\"gumbel_noise_weight\", 0.0)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_v2():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_v1()\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.attention_dropout = 0.1\n  hparams.relu_dropout = 0.1\n  hparams.learning_rate_warmup_steps = 8000\n  hparams.learning_rate = 0.2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_vq_ada_32ex_packed():\n  \"\"\"Set of hyperparameters for lm1b packed following tpu params.\"\"\"\n  hparams = transformer_base_v2()\n  expert_utils.update_hparams_for_vq_gating(hparams)\n  hparams.moe_num_experts = 32\n  hparams.gating_type = \"vq\"\n  # this gives us a batch size of 16 because each seq is len 256\n  hparams.batch_size = 5072\n  hparams.ffn_layer = \"local_moe\"\n  hparams.shared_embedding_and_softmax_weights = False\n  hparams.learning_rate_warmup_steps = 10000\n  # one epoch for languagemodel_lm1b32k_packed = 27200 steps w/ bsize 128\n  hparams.learning_rate_decay_steps = 27200\n  hparams.num_heads = 4\n  hparams.num_blocks = 1\n  hparams.moe_k = 1\n  hparams.num_decoder_layers = 6\n  hparams.label_smoothing = 0.\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.weight_decay = 1e-06\n  hparams.attention_dropout = 0.1\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate_schedule = \"linear_warmup*rsqrt_decay*linear_decay\"\n  hparams.activation_dtype = \"float32\"\n  hparams.learning_rate = 0.1\n  hparams.learning_rate_constant = 1.0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_topk_16_packed():\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.gating_type = \"topk\"\n  hparams.moe_num_experts = 16\n  hparams.moe_k = 2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_vq1_16_nb1_packed_nda_b01_scales():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.use_scales = int(True)\n  hparams.moe_num_experts = 16\n  hparams.moe_k = 1\n  hparams.beta = 0.1\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.ema = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_vq1_16_nb1_packed_dan_b01_scales():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.use_scales = int(True)\n  hparams.moe_num_experts = 16\n  hparams.moe_k = 1\n  hparams.beta = 0.1\n  hparams.ema = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_vq1_16_nb1_packed_nda_b01_scales_dialog():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_vq1_16_nb1_packed_nda_b01_scales()\n  hparams.batch_size = 2048\n  hparams.max_length = 1024\n  hparams.filter_size = 3072\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ada_lmpackedbase():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.ffn_layer = \"dense_relu_dense\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ada_lmpackedbase_dialog():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.max_length = 1024\n  hparams.ffn_layer = \"dense_relu_dense\"\n  hparams.batch_size = 4096\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ada_lmpackedbase_relative():\n  \"\"\"Set of hyperparameters.\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.ffn_layer = \"dense_relu_dense\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_v3():\n  \"\"\"Base parameters for Transformer model.\"\"\"\n  # Update parameters here, then occasionally cut a versioned set, e.g.\n  # transformer_base_v2.\n  hparams = transformer_base_v2()\n  hparams.optimizer_adam_beta2 = 0.997\n  # New way of specifying learning rate schedule.\n  # Equivalent to previous version.\n  hparams.learning_rate_schedule = (\n      \"constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size\")\n  hparams.learning_rate_constant = 2.0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base():\n  \"\"\"Base parameters for Transformer model.\"\"\"\n  hparams = transformer_base_v3()\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_big():\n  \"\"\"HParams for transformer big model on WMT.\"\"\"\n  hparams = transformer_base()\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  # Reduce batch size to 2048 from 4096 to be able to train the model on a GPU\n  # with 12 GB memory. For example, NVIDIA TITAN V GPU.\n  hparams.batch_size = 2048\n  hparams.num_heads = 16\n  hparams.layer_prepostprocess_dropout = 0.3\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall():\n  \"\"\"Hparams for transformer on LM for pretraining/finetuning/mixing.\"\"\"\n  hparams = transformer_base()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 768\n  hparams.filter_size = 3072\n  hparams.num_hidden_layers = 12\n  hparams.num_heads = 12\n  hparams.label_smoothing = 0.0\n  hparams.max_length = 1024\n  hparams.eval_drop_long_sequences = True\n  hparams.multiproblem_mixing_schedule = \"pretrain\"\n  hparams.multiproblem_vocab_size = 65536\n  hparams.clip_grad_norm = 1.0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_finetune_tied():\n  \"\"\"Tied means fine-tune CNN/DM summarization as LM.\"\"\"\n  hparams = transformer_tall()\n  hparams.multiproblem_max_input_length = 750\n  hparams.multiproblem_max_target_length = 100\n  hparams.multiproblem_schedule_max_examples = 0\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.learning_rate_constant = 5e-5\n  hparams.learning_rate_warmup_steps = 100\n  # Set train steps to learning_rate_decay_steps or less\n  hparams.learning_rate_decay_steps = 80000\n  hparams.multiproblem_target_eval_only = True\n  hparams.multiproblem_reweight_label_loss = True\n  hparams.multiproblem_label_weight = 1.0\n  hparams.optimizer = \"true_adam\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_train_tied():\n  \"\"\"Tied means train CNN/DM summarization as LM.\"\"\"\n  hparams = transformer_tall()\n  hparams.multiproblem_max_input_length = 750\n  hparams.multiproblem_max_target_length = 100\n  hparams.multiproblem_schedule_max_examples = 0\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.learning_rate_constant = 2e-4\n  hparams.learning_rate_warmup_steps = 8000\n  # Set train steps to learning_rate_decay_steps or less\n  hparams.learning_rate_decay_steps = 150000\n  hparams.multiproblem_target_eval_only = True\n  hparams.multiproblem_reweight_label_loss = True\n  hparams.multiproblem_label_weight = 1.0\n  hparams.optimizer = \"true_adam\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_finetune_uniencdec():\n  \"\"\"Fine-tune CNN/DM with a unidirectional encoder and decoder.\"\"\"\n  hparams = transformer_tall()\n  hparams.max_input_seq_length = 750\n  hparams.max_target_seq_length = 100\n  hparams.optimizer = \"true_adam\"\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.learning_rate_decay_steps = 80000\n  hparams.learning_rate_constant = 5e-5\n  hparams.learning_rate_warmup_steps = 100\n  hparams.unidirectional_encoder = True\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_train_uniencdec():\n  \"\"\"Train CNN/DM with a unidirectional encoder and decoder.\"\"\"\n  hparams = transformer_tall()\n  hparams.max_input_seq_length = 750\n  hparams.max_target_seq_length = 100\n  hparams.optimizer = \"true_adam\"\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.learning_rate_decay_steps = 150000\n  hparams.learning_rate_constant = 2e-4\n  hparams.unidirectional_encoder = True\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_finetune_textclass():\n  \"\"\"Hparams for transformer on LM for finetuning on text class problems.\"\"\"\n  hparams = transformer_tall()\n  hparams.learning_rate_constant = 6.25e-5\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*linear_decay\")\n  hparams.multiproblem_schedule_max_examples = 0\n  hparams.multiproblem_target_eval_only = True\n  hparams.learning_rate_warmup_steps = 50\n  # Set train steps to learning_rate_decay_steps or less\n  hparams.learning_rate_decay_steps = 25000\n  hparams.multiproblem_reweight_label_loss = True\n  hparams.multiproblem_label_weight = 0.95\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_pretrain_lm():\n  \"\"\"Hparams for transformer on LM pretraining (with 64k vocab).\"\"\"\n  hparams = transformer_tall()\n  hparams.learning_rate_constant = 2e-4\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.optimizer = \"adam_w\"\n  hparams.weight_decay = 0.01 * hparams.learning_rate_constant\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.999\n  hparams.optimizer_adam_epsilon = 1e-8\n  # Set max examples to something big when pretraining only the LM, definitely\n  # something an order of magnitude bigger than number of train steps.\n  hparams.multiproblem_schedule_max_examples = 5e8\n  # Set train steps to learning_rate_decay_steps or less\n  hparams.learning_rate_decay_steps = 5000000\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_pretrain_lm_tpu_adafactor():\n  \"\"\"Hparams for transformer on LM pretraining (with 64k vocab) on TPU.\"\"\"\n  hparams = transformer_tall_pretrain_lm()\n  update_hparams_for_tpu(hparams)\n  hparams.max_length = 1024\n  # For multi-problem on TPU we need it in absolute examples.\n  hparams.batch_size = 8\n  hparams.multiproblem_vocab_size = 2**16\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_pretrain_lm_tpu_adafactor_large():\n  \"\"\"Hparams for transformer on LM pretraining on TPU, large model.\"\"\"\n  hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n  hparams.hidden_size = 1024\n  hparams.num_heads = 16\n  hparams.filter_size = 32768  # max fitting in 16G memory is 49152, batch 2\n  hparams.batch_size = 4\n  hparams.multiproblem_mixing_schedule = \"constant\"\n  # Task order: lm/en-de/en-fr/en-ro/de-en/fr-en/ro-en/cnndm/mnli/squad.\n  hparams.multiproblem_per_task_threshold = \"320,80,160,1,80,160,2,20,10,5\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_pretrain_lm_tpu():\n  \"\"\"Hparams for transformer on LM pretraining on TPU with AdamW.\"\"\"\n  hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n  # Optimizer gets reset in update_hparams_for_tpu so we set it again here.\n  hparams.learning_rate_constant = 2e-4\n  hparams.learning_rate_schedule = (\"linear_warmup * constant * cosdecay\")\n  hparams.optimizer = \"adam_w\"\n  hparams.weight_decay = 0.01 * hparams.learning_rate_constant\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tall_big():\n  \"\"\"Hparams for transformer on LM+MNLI.\"\"\"\n  hparams = transformer_tall()\n  hparams.num_hidden_layers = 18\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_big_single_gpu():\n  \"\"\"HParams for transformer big model for single GPU.\"\"\"\n  hparams = transformer_big()\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.learning_rate_warmup_steps = 16000\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_single_gpu():\n  \"\"\"HParams for transformer base model for single GPU.\"\"\"\n  hparams = transformer_base()\n  hparams.batch_size = 1024\n  hparams.learning_rate_schedule = \"constant*linear_warmup*rsqrt_decay\"\n  hparams.learning_rate_constant = 0.1\n  hparams.learning_rate_warmup_steps = 16000\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_base_multistep8():\n  \"\"\"HParams for simulating 8 GPUs with MultistepAdam optimizer.\"\"\"\n  hparams = transformer_base()\n  hparams.optimizer = \"multistep_adam\"\n  hparams.optimizer_multistep_accumulate_steps = 8\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_cubbitt():\n  \"\"\"Transformer hyperparameters used in CUBBITT experiments.\"\"\"\n  hparams = transformer_big_single_gpu()\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.batch_size = 2900\n  hparams.learning_rate_warmup_steps = 8000\n  hparams.max_length = 150\n  hparams.layer_prepostprocess_dropout = 0\n  hparams.optimizer = \"Adafactor\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_parsing_base():\n  \"\"\"HParams for parsing on WSJ only.\"\"\"\n  hparams = transformer_base()\n  hparams.attention_dropout = 0.2\n  hparams.layer_prepostprocess_dropout = 0.2\n  hparams.max_length = 512\n  hparams.learning_rate_warmup_steps = 16000\n  hparams.hidden_size = 1024\n  hparams.learning_rate = 0.05\n  hparams.shared_embedding_and_softmax_weights = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_parsing_big():\n  \"\"\"HParams for parsing on WSJ semi-supervised.\"\"\"\n  hparams = transformer_big()\n  hparams.max_length = 512\n  hparams.shared_source_target_embedding = False\n  hparams.learning_rate_warmup_steps = 4000\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.batch_size = 2048\n  hparams.learning_rate = 0.05\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_parsing_ice():\n  \"\"\"HParams for parsing and tagging Icelandic text.\"\"\"\n  hparams = transformer_base_single_gpu()\n  hparams.batch_size = 4096\n  hparams.shared_embedding_and_softmax_weights = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tiny():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 128\n  hparams.filter_size = 512\n  hparams.num_heads = 4\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_test():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 16\n  hparams.filter_size = 8\n  hparams.num_heads = 2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_small():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 256\n  hparams.filter_size = 1024\n  hparams.num_heads = 4\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_l2():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_l4():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 4\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_l8():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 8\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_l10():\n  hparams = transformer_base()\n  hparams.num_hidden_layers = 10\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_h1():\n  hparams = transformer_base()\n  hparams.num_heads = 1\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_h4():\n  hparams = transformer_base()\n  hparams.num_heads = 4\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_h16():\n  hparams = transformer_base()\n  hparams.num_heads = 16\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_h32():\n  hparams = transformer_base()\n  hparams.num_heads = 32\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_k128():\n  hparams = transformer_base()\n  hparams.attention_key_channels = 128\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_k256():\n  hparams = transformer_base()\n  hparams.attention_key_channels = 256\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ff1024():\n  hparams = transformer_base()\n  hparams.filter_size = 1024\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ff4096():\n  hparams = transformer_base()\n  hparams.filter_size = 4096\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_dr0():\n  hparams = transformer_base()\n  hparams.layer_prepostprocess_dropout = 0.0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_dr2():\n  hparams = transformer_base()\n  hparams.layer_prepostprocess_dropout = 0.2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ls0():\n  hparams = transformer_base()\n  hparams.label_smoothing = 0.0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_ls2():\n  hparams = transformer_base()\n  hparams.label_smoothing = 0.2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_hs256():\n  hparams = transformer_base()\n  hparams.hidden_size = 256\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_hs1024():\n  hparams = transformer_base()\n  hparams.hidden_size = 1024\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_big_dr1():\n  hparams = transformer_base()\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  hparams.num_heads = 16\n  hparams.layer_prepostprocess_dropout = 0.1\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_big_enfr():\n  hparams = transformer_big_dr1()\n  hparams.shared_embedding_and_softmax_weights = False\n  hparams.filter_size = 8192\n  hparams.layer_prepostprocess_dropout = 0.1\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_big_enfr_tpu():\n  hparams = transformer_big_enfr()\n  # For performance, use fewer heads so that matrix dimensions are at least 128\n  hparams.num_heads = 8\n  update_hparams_for_tpu(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_big_dr2():\n  hparams = transformer_big_dr1()\n  hparams.layer_prepostprocess_dropout = 0.2\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_parameter_attention_a():\n  hparams = transformer_base()\n  hparams.ffn_layer = \"parameter_attention\"\n  hparams.filter_size = 1536\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_parameter_attention_b():\n  hparams = transformer_base()\n  hparams.ffn_layer = \"parameter_attention\"\n  hparams.filter_size = 512\n  hparams.parameter_attention_key_channels = 1024\n  hparams.parameter_attention_value_channels = 1024\n  hparams.num_heads = 16\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_prepend_v2():\n  hparams = transformer_base_v2()\n  hparams.prepend_mode = \"prepend_inputs_masked_attention\"\n  hparams.max_length = 0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_prepend_v1():\n  hparams = transformer_base_v1()\n  hparams.prepend_mode = \"prepend_inputs_masked_attention\"\n  hparams.max_length = 0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_prepend():\n  return transformer_prepend_v2()\n\n\n@registry.register_ranged_hparams\ndef transformer_base_range(rhp):\n  \"\"\"Small range of hyperparameters.\"\"\"\n  # After starting from base, set intervals for some parameters.\n  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n  rhp.set_discrete(\"learning_rate_warmup_steps\",\n                   [1000, 2000, 4000, 8000, 16000])\n  rhp.set_float(\"initializer_gain\", 0.5, 2.0)\n  rhp.set_float(\"optimizer_adam_beta1\", 0.85, 0.95)\n  rhp.set_float(\"optimizer_adam_beta2\", 0.97, 0.99)\n  rhp.set_float(\"weight_decay\", 0.0, 1e-4)\n\n\n@registry.register_hparams\ndef transformer_relative():\n  \"\"\"Use relative position embeddings instead of absolute position encodings.\"\"\"\n  hparams = transformer_base()\n  hparams.pos = None\n  hparams.self_attention_type = \"dot_product_relative\"\n  hparams.max_relative_position = 20\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_relative_tiny():\n  hparams = transformer_relative()\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 128\n  hparams.filter_size = 512\n  hparams.num_heads = 4\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_relative_big():\n  hparams = transformer_big()\n  hparams.pos = None\n  hparams.self_attention_type = \"dot_product_relative\"\n  hparams.max_relative_position = 20\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_timeseries():\n  hparams = transformer_small()\n  hparams.batch_size = 256\n  hparams.learning_rate_warmup_steps = 2000\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_mlperf_tpu():\n  \"\"\"HParams for Transformer model on TPU for MLPerf on TPU 2x2.\"\"\"\n  hparams = transformer_base_v3()\n  hparams.mlperf_mode = True\n  hparams.symbol_modality_num_shards = 1\n  hparams.max_length = 256  # ignored when using \"_packed\" problems\n  hparams.batch_size = 2048  # per-chip batch size matches the reference model\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  hparams.num_heads = 16\n  hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n  hparams.relu_dropout_broadcast_dims = \"1\"  # length\n  hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n  return hparams\n\n\ndef update_hparams_for_tpu(hparams):\n  \"\"\"Change hparams to be compatible with TPU training.\"\"\"\n\n  # Adafactor uses less memory than Adam.\n  # switch to Adafactor with its recommended learning rate scheme.\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.learning_rate_warmup_steps = 10000\n\n  # Avoid an expensive concat on TPU.\n  # >1 shards helps with faster parameter distribution on multi-GPU machines\n  hparams.symbol_modality_num_shards = 1\n\n  # Adaptive batch sizes and sequence lengths are not supported on TPU.\n  # Instead, every batch has the same sequence length and the same batch size.\n  # Longer sequences are dropped and shorter ones are padded.\n  #\n  # It is therefore suggested to use a problem where examples have been combined\n  # to a longer length, e.g. the \"_packed\" problems.\n  #\n  # For problems with variable sequence lengths, this parameter controls the\n  # maximum sequence length. Longer sequences are dropped and shorter ones\n  # are padded.\n  #\n  # For problems with fixed sequence lengths - e.g. the \"_packed\" problems,\n  # this hyperparameter is ignored.\n  hparams.max_length = 64\n\n  # TPUs have less memory than GPUs, so decrease the batch size if it's too high\n  if hparams.batch_size > 2048:\n    hparams.batch_size = 2048\n\n  # Using noise broadcast in the dropout layers saves memory during training.\n  hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n  hparams.relu_dropout_broadcast_dims = \"1\"  # length\n  hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tpu():\n  \"\"\"HParams for Transformer model on TPU.\"\"\"\n  hparams = transformer_base()\n  update_hparams_for_tpu(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_timeseries_tpu():\n  \"\"\"HParams for running Transformer model on timeseries on TPU.\"\"\"\n  hparams = transformer_timeseries()\n  update_hparams_for_tpu(hparams)\n  hparams.batch_size = 256  # revert to value set in transformer_timeseries\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tpu_bf16_activation():\n  \"\"\"HParams for Transformer model with BF16 activation on TPU.\"\"\"\n  hparams = transformer_tpu()\n  hparams.activation_dtype = \"bfloat16\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_fairseq_fp16_activation_big():\n  \"\"\"Hparams intended to mirror those used in arxiv.org/pdf/1806.00187.pdf.\"\"\"\n  hparams = transformer_big()\n  hparams.activation_dtype = \"float16\"\n  hparams.batch_size = 3584\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_packed_tpu():\n  \"\"\"Deprecated alias for transformer_tpu().\"\"\"\n  return transformer_tpu()\n\n\n@registry.register_hparams\ndef transformer_big_tpu():\n  hparams = transformer_big()\n  update_hparams_for_tpu(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tiny_tpu():\n  hparams = transformer_tiny()\n  update_hparams_for_tpu(hparams)\n  return hparams\n\n\n@registry.register_ranged_hparams\ndef transformer_tiny_tpu_range(rhp):\n  \"\"\"Small range of hyperparameters.\"\"\"\n  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"weight_decay\", 0.0, 2.0)\n\n\n@registry.register_ranged_hparams\ndef transformer_tpu_range(rhp):\n  \"\"\"Small range of hyperparameters.\"\"\"\n  # After starting from base, set intervals for some parameters.\n  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n  rhp.set_discrete(\"learning_rate_warmup_steps\",\n                   [1000, 2000, 4000, 8000, 16000])\n  rhp.set_float(\"initializer_gain\", 0.5, 2.0)\n  rhp.set_float(\"optimizer_adam_beta1\", 0.85, 0.95)\n  rhp.set_float(\"optimizer_adam_beta2\", 0.97, 0.99)\n  rhp.set_float(\"weight_decay\", 0.0, 2.0)\n\n\n@registry.register_hparams\ndef transformer_small_tpu():\n  \"\"\"TPU-friendly version of transformer_small.\n\n  Returns:\n    an hparams object.\n  \"\"\"\n  hparams = transformer_small()\n  update_hparams_for_tpu(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_clean():\n  \"\"\"No dropout, label smoothing, max_length.\"\"\"\n  hparams = transformer_base_v2()\n  hparams.label_smoothing = 0.0\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.attention_dropout = 0.0\n  hparams.relu_dropout = 0.0\n  hparams.max_length = 0\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_clean_big():\n  hparams = transformer_clean()\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_clean_big_tpu():\n  hparams = transformer_clean_big()\n  update_hparams_for_tpu(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tpu_with_conv():\n  \"\"\"Cut down on the number of heads, and use convs instead.\"\"\"\n  hparams = transformer_tpu()\n  hparams.num_heads = 4  # Heads are expensive on TPUs.\n  hparams.ffn_layer = \"conv_relu_conv\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_lm_tpu_0():\n  \"\"\"HParams for training languagemodel_lm1b8k on tpu.  92M Params.\"\"\"\n  hparams = transformer_clean_big()\n  update_hparams_for_tpu(hparams)\n  hparams.num_heads = 4  # Heads are expensive on TPUs.\n  hparams.batch_size = 4096\n  hparams.shared_embedding_and_softmax_weights = False\n  hparams.layer_prepostprocess_dropout = 0.1\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_lm_tpu_1():\n  \"\"\"HParams for training languagemodel_lm1b8k on tpu.  335M Params.\"\"\"\n  hparams = transformer_lm_tpu_0()\n  hparams.hidden_size = 2048\n  hparams.filter_size = 8192\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_librispeech_v1():\n  \"\"\"HParams for training ASR model on LibriSpeech V1.\"\"\"\n  hparams = transformer_base()\n\n  hparams.num_heads = 4\n  hparams.filter_size = 1024\n  hparams.hidden_size = 256\n  hparams.num_encoder_layers = 5\n  hparams.num_decoder_layers = 3\n  hparams.learning_rate = 0.15\n  hparams.batch_size = 6000000\n\n  librispeech.set_librispeech_length_hparams(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_librispeech_v2():\n  \"\"\"HParams for training ASR model on LibriSpeech V2.\"\"\"\n  hparams = transformer_base()\n\n  hparams.max_length = 1240000\n  hparams.max_input_seq_length = 1550\n  hparams.max_target_seq_length = 350\n  hparams.batch_size = 16\n  hparams.num_decoder_layers = 4\n  hparams.num_encoder_layers = 6\n  hparams.hidden_size = 384\n  hparams.learning_rate = 0.15\n  hparams.daisy_chain_variables = False\n  hparams.filter_size = 1536\n  hparams.num_heads = 2\n  hparams.ffn_layer = \"conv_relu_conv\"\n  hparams.conv_first_kernel = 9\n  hparams.weight_decay = 0\n  hparams.layer_prepostprocess_dropout = 0.2\n  hparams.relu_dropout = 0.2\n\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_librispeech_tpu_v1():\n  \"\"\"HParams for training ASR model on Librispeech on TPU v1.\"\"\"\n  hparams = transformer_librispeech_v1()\n  update_hparams_for_tpu(hparams)\n\n  hparams.batch_size = 16\n  librispeech.set_librispeech_length_hparams(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_librispeech_tpu_v2():\n  \"\"\"HParams for training ASR model on Librispeech on TPU v2.\"\"\"\n  hparams = transformer_librispeech_v2()\n  update_hparams_for_tpu(hparams)\n\n  hparams.batch_size = 16\n  librispeech.set_librispeech_length_hparams(hparams)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_librispeech_with_area_attention():\n  \"\"\"HParams for training ASR model on Librispeech on TPU v2.\"\"\"\n  hparams = transformer_librispeech_tpu_v2()\n  hparams.num_area_layers = 3  # area attn on first 3 encoder and decoder layers\n  hparams.max_area_width = 5\n  hparams.area_key_mode = \"concat\"\n  hparams.area_value_mode = \"sum\"\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_librispeech():\n  \"\"\"HParams for training ASR model on Librispeech.\"\"\"\n  return transformer_librispeech_v2()\n\n\n@registry.register_hparams\ndef transformer_librispeech_tpu():\n  \"\"\"HParams for training ASR model on Librispeech on TPU.\"\"\"\n  return transformer_librispeech_tpu_v2()\n\n\n@registry.register_hparams\ndef transformer_common_voice():\n  \"\"\"HParams for training ASR model on Mozilla Common Voice.\"\"\"\n  return transformer_librispeech()\n\n\n@registry.register_hparams\ndef transformer_common_voice_tpu():\n  \"\"\"HParams for training ASR model on Mozilla Common Voice on TPU.\"\"\"\n  hparams = transformer_librispeech_tpu()\n  hparams.batch_size = 8\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_supervised_attention():\n  \"\"\"HParams for supervised attention problems.\"\"\"\n  hparams = transformer_base()\n  # Attention loss type (KL-divergence or MSE).\n  hparams.add_hparam(\"expected_attention_loss_type\", \"kl_divergence\")\n  # Multiplier to the encoder-decoder expected attention loss.\n  hparams.add_hparam(\"expected_attention_loss_multiplier\", 1.0)\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_tpu_1b():\n  \"\"\"Hparams for machine translation with ~1.1B parameters.\"\"\"\n  hparams = transformer_tpu()\n  hparams.hidden_size = 2048\n  hparams.filter_size = 8192\n  hparams.num_hidden_layers = 8\n  # smaller batch size to avoid OOM\n  hparams.batch_size = 1024\n  hparams.activation_dtype = \"bfloat16\"\n  hparams.weight_dtype = \"bfloat16\"\n  # maximize number of parameters relative to computation by not sharing.\n  hparams.shared_embedding_and_softmax_weights = False\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_wikitext103_l4k_v0():\n  \"\"\"HParams for training languagemodel_wikitext103_l4k.\"\"\"\n  hparams = transformer_big()\n\n  # Adafactor uses less memory than Adam.\n  # switch to Adafactor with its recommended learning rate scheme.\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.learning_rate_warmup_steps = 10000\n\n  hparams.num_heads = 4\n  hparams.max_length = 4096\n  hparams.batch_size = 4096\n  hparams.shared_embedding_and_softmax_weights = False\n\n  hparams.num_hidden_layers = 8\n  hparams.attention_dropout = 0.1\n  hparams.layer_prepostprocess_dropout = 0.2\n  hparams.relu_dropout = 0.1\n  hparams.label_smoothing = 0.0\n\n  # Using noise broadcast in the dropout layers saves memory during training.\n  hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n  hparams.relu_dropout_broadcast_dims = \"1\"  # length\n  hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n\n  # Avoid an expensive concat on TPU.\n  # >1 shards helps with faster parameter distribution on multi-GPU machines\n  hparams.symbol_modality_num_shards = 1\n\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_wikitext103_l4k_memory_v0():\n  \"\"\"HParams for training languagemodel_wikitext103_l4k with memory.\"\"\"\n  hparams = transformer_wikitext103_l4k_v0()\n\n  hparams.split_targets_chunk_length = 64\n  hparams.split_targets_max_chunks = 64\n  hparams.split_targets_strided_training = True\n  hparams.add_hparam(\"memory_type\", \"transformer_xl\")\n\n  # The hparams specify batch size *before* chunking, but we want to have a\n  # consistent 4K batch size *after* chunking to fully utilize the hardware.\n  target_tokens_per_batch = 4096\n  hparams.batch_size = int(target_tokens_per_batch * (\n      hparams.max_length / hparams.split_targets_chunk_length))  # 262144\n\n  hparams.pos = None\n  hparams.self_attention_type = \"dot_product_relative\"\n  hparams.max_relative_position = 2 * hparams.split_targets_chunk_length\n\n  hparams.add_hparam(\"unconditional\", True)\n  hparams.add_hparam(\"recurrent_memory_batch_size\", 0)  # 0 = try to guess\n  # By default, cache one chunk only (like Transformer-XL)\n  hparams.add_hparam(\"num_memory_items\", hparams.split_targets_chunk_length)\n\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_wikitext103_l16k_memory_v0():\n  \"\"\"HParams for training languagemodel_wikitext103_l16k with memory.\"\"\"\n  hparams = transformer_wikitext103_l4k_memory_v0()\n\n  hparams.max_length = 16384\n  hparams.split_targets_chunk_length = 64\n  hparams.split_targets_max_chunks = int(\n      hparams.max_length / hparams.split_targets_chunk_length)\n\n  # The hparams specify batch size *before* chunking, but we want to have a\n  # consistent 4K batch size *after* chunking to fully utilize the hardware.\n  target_tokens_per_batch = 4096\n  hparams.batch_size = int(target_tokens_per_batch * (\n      hparams.max_length / hparams.split_targets_chunk_length))\n\n  hparams.max_relative_position = 2 * hparams.split_targets_chunk_length\n\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_cifar10_memory_v0():\n  \"\"\"HParams for training image_cifar10_plain_gen_flat_rev with memory.\"\"\"\n  hparams = transformer_wikitext103_l4k_memory_v0()\n\n  hparams.num_hidden_layers = 6\n\n  hparams.max_length = 32 * 32 * 3\n  hparams.split_targets_chunk_length = 64 * 3\n  hparams.split_targets_max_chunks = int(\n      hparams.max_length / hparams.split_targets_chunk_length)\n  hparams.num_memory_items = 128 * 3\n\n  # Since this is an image problem, batch size refers to examples (not tokens)\n  target_images_per_batch = 4\n  hparams.batch_size = int(target_images_per_batch * (\n      hparams.max_length / hparams.split_targets_chunk_length))\n\n  # The recurrent memory needs to know the actual batch size (in sequences)\n  hparams.recurrent_memory_batch_size = hparams.batch_size\n\n  hparams.max_relative_position = (\n      hparams.num_memory_items + hparams.split_targets_chunk_length)\n\n  return hparams\n\n\n@registry.register_hparams\ndef transformer_imagenet64_memory_v0():\n  \"\"\"HParams for training image_imagenet64_gen_flat_rev with memory.\"\"\"\n  hparams = transformer_cifar10_memory_v0()\n\n  hparams.max_length = 64 * 64 * 3\n  hparams.split_targets_chunk_length = 64 * 3\n  hparams.split_targets_max_chunks = int(\n      hparams.max_length / hparams.split_targets_chunk_length)\n  hparams.num_memory_items = 128 * 3\n\n  # Since this is an image problem, batch size refers to examples (not tokens)\n  target_images_per_batch = 2\n  hparams.batch_size = int(target_images_per_batch * (\n      hparams.max_length / hparams.split_targets_chunk_length))\n\n  # The recurrent memory needs to know the actual batch size (in sequences)\n  hparams.recurrent_memory_batch_size = hparams.batch_size\n\n  hparams.max_relative_position = 3072\n\n  return hparams\n",
    "file-6": "\"\"\"\n================================================================================\nCRITICAL CODE STUDIES SAMPLE - TRANSFORMER ARCHITECTURE (2017)\n================================================================================\n\nImplementation: TensorFlow tensor2tensor Attention Utilities\nFramework: TensorFlow\nYear: 2023 version (tensor2tensor project)\nPurpose: Attention mechanisms for tensor2tensor Transformers\nAuthors: Tensor2Tensor Authors (Google)\nSource: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\nLicense: Apache 2.0\n\nThis file contains the attention mechanism implementations used by\ntransformer_tensorflow.py. It represents Google's TensorFlow approach to\nattention, including:\n- Multihead attention with TensorFlow computational graphs\n- Various attention variants (local, sparse, etc.)\n- Positional encodings and embeddings\n- TensorFlow-specific optimizations\n\nKey difference from PyTorch: TensorFlow builds static computation graphs,\nwhile PyTorch uses imperative define-by-run. This architectural choice shapes\nhow attention is implemented and optimized.\n\nCompare to:\n- attention_pytorch.py (PyTorch's imperative attention)\n- annotated_transformer.py (pedagogical explanation of attention)\n\n================================================================================\n\"\"\"\n\n# coding=utf-8\n# Copyright 2023 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for attention.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport itertools\nimport math\nimport operator\n\nimport numpy as np\n\nfrom six.moves import range  # pylint: disable=redefined-builtin\nfrom six.moves import zip  # pylint: disable=redefined-builtin\n\nfrom tensor2tensor.layers import area_attention\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.utils import contrib\nfrom tensor2tensor.utils import expert_utils\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_probability as tfp\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.ops import inplace_ops\n# pylint: enable=g-direct-tensorflow-import\n\n\n# TODO(lukaszkaiser): remove this function when not needed any more.\ndef layers():\n  return common_layers.layers()\n\n\ndef large_compatible_negative(tensor_type):\n  \"\"\"Large negative number as Tensor.\n\n  This function is necessary because the standard value for epsilon\n  in this module (-1e9) cannot be represented using tf.float16\n\n  Args:\n    tensor_type: a dtype to determine the type.\n\n  Returns:\n    a large negative number.\n  \"\"\"\n  if tensor_type == tf.float16:\n    return tf.float16.min\n  return -1e9\n\n\ndef mixed_precision_is_enabled(\n    activation_dtype=None, weight_dtype=None, hparams=None):\n  assert not (hparams and (activation_dtype or weight_dtype)), (\n      \"Provide only hparams or activation_dtype and weight_dtype\")\n  if (hparams and hasattr(hparams, \"activation_dtype\") and\n      hasattr(hparams, \"weight_dtype\")):\n    activation_dtype = hparams.activation_dtype\n    weight_dtype = hparams.weight_dtype\n  return activation_dtype == tf.float16 and weight_dtype == tf.float32\n\n\ndef maybe_upcast(logits,\n                 activation_dtype=None, weight_dtype=None, hparams=None):\n  if mixed_precision_is_enabled(activation_dtype, weight_dtype, hparams):\n    return tf.cast(logits, tf.float32)\n  return logits\n\n\n# Struct containing the sequences ids and order on a batch (are send to the\n# expert to allow them to compute the bias mask)\nBatchInfo = collections.namedtuple(\"BatchInfo\", \"coordinates, order\")\n\n_expert_count = 0\n\n\ndef get_standardized_layers(hparams, dp=None):\n  \"\"\"Get the common attention and feed-forward layers.\n\n  The returned layer functions will have the following signature:\n\n    y, extra_loss = fct(x)\n\n  extra_loss is set to 0.0 if the layer doesn't have extra loss.\n  If dp is provided, the layers will be distributed within the devices.\n  If moe wants to be used, both dp and model need to be set.\n\n  Args:\n    hparams (tf.HParams): the model hparameters\n    dp (expert_utils.Parallelism): A data parallelism object. If not given,\n      the dp calls are simply ignored.\n\n  Returns:\n    dict[str:fct]: A dictionary containing the standardized functions\n  \"\"\"\n\n  def partial(fct, *args, **kwargs):\n    \"\"\"Same as functools.partial but with functools.wraps.\"\"\"\n    return functools.wraps(fct)(functools.partial(fct, *args, **kwargs))\n\n  def register_layer(\n      fct_in,\n      default_args=None,\n      default_kwargs=None,\n      use_dp=True,\n      recompute_grad=False,\n  ):\n    \"\"\"Turn a function into its standardized version.\n\n    Args:\n      fct_in (fct): The function to register\n      default_args (list): The default parameters to add to the function.\n      default_kwargs (dict): The default parameters to add to the function.\n        Those arguments can be overwritten when calling the function.\n      use_dp (bool): Wrap the function call within a dataparallelism object if\n        dp is available. Some layers (like MOE) must be called without dp.\n      recompute_grad (bool): If True, recompute the function during the\n        backward pass to save memory\n\n    Returns:\n      fct: the standardized layer function.\n    \"\"\"\n    # The kwargs given when calling the function overwrite the default ones\n    fct_in = partial(fct_in, *(default_args or []), **(default_kwargs or {}))\n\n    @functools.wraps(fct_in)\n    def decorator(x, *args, **kwargs):\n      \"\"\"Call the layer function.\"\"\"\n      fct = fct_in  # For closure. Could use nonlocal with Python 3\n      # Eventually create the memory optimized version of the function\n      if recompute_grad:\n        fct = partial(fct, **kwargs)  # recompute_grad only accept args\n        fct = common_layers.recompute_grad(fct)\n        kwargs = {}\n\n      # Eventually use dp (if given and not MoE)\n      if use_dp and dp is not None:\n        y = dp(fct, x, *args, **kwargs)\n      else:\n        y = fct(x, *args, **kwargs)\n\n      # Eventually capture the extra loss\n      extra_loss = 0.0\n      if isinstance(y, tuple):\n        y, extra_loss = y\n\n      return y, extra_loss\n\n    return decorator\n\n  total_key_depth = hparams.attention_key_channels or hparams.hidden_size\n  total_value_depth = hparams.attention_value_channels or hparams.hidden_size\n\n  # Attention layers:\n\n  # === Multi-head full attention layer ===\n  multihead_attention_fn = register_layer(\n      multihead_attention,\n      default_kwargs=dict(\n          memory_antecedent=None,  # Self-attention by default\n          bias=None,\n          total_key_depth=total_key_depth,\n          total_value_depth=total_value_depth,\n          output_depth=hparams.hidden_size,\n          num_heads=hparams.num_heads,\n          dropout_rate=hparams.attention_dropout,\n      ))\n\n  # === Memory efficient full-attention layer ===\n  # Save memory by not storing the activations and\n  # recomputing them during the backward pass\n  memeff_attention_base_fn = register_layer(\n      multihead_attention,\n      default_kwargs=dict(\n          total_key_depth=total_key_depth,\n          total_value_depth=total_value_depth,\n          output_depth=hparams.hidden_size,\n          num_heads=hparams.num_heads,\n          dropout_rate=hparams.attention_dropout,\n      ),\n      recompute_grad=True,\n  )\n\n  def memeff_attention_fn(*args, **kwargs):\n    \"\"\"Modify args/kwargs for compatibility with recompute_grad.\"\"\"\n    kwargs = kwargs.copy()\n    assert len(args) == 1\n    x = args[0]\n    memory_antecedent = kwargs.pop(\"memory_antecedent\", x)  # Same as x if None\n    if kwargs.get(\"bias\", None) is not None:  # Case where bias has been set\n      args = (x, memory_antecedent, kwargs.pop(\"bias\"))\n    else:\n      # Otherwise, only 2 args. This is necessary as recompute_grad does not\n      # support None values.\n      args = (x, memory_antecedent)\n    return memeff_attention_base_fn(*args, **kwargs)\n\n  # === Local attention (unmasked) layer ===\n  # Reuse same parameters as multihead_attention\n  # Don't mask the future\n  local_attention_fn = partial(\n      multihead_attention_fn,\n      block_length=hparams.attention_loc_block_length,\n      block_width=hparams.attention_loc_block_width,\n      attention_type=\"local_unmasked\",\n  )\n\n  # === Local attention (masked) layer ===\n  # Reuse same parameters as multihead_attention\n  # Only works for self attention. Always mask the future.\n  local_attention_masked_fn = partial(\n      multihead_attention_fn,\n      block_length=hparams.attention_loc_block_length,\n      attention_type=\"local_mask_right\",\n  )\n\n  # === Masked memory-compressed multihead self attention layer ===\n  # Only works for self attention. Always mask the future.\n  compressed_attention_masked_fn = register_layer(\n      multihead_self_attention_reduced,\n      default_kwargs=dict(\n          factor=hparams.attention_red_factor,\n          nonlinearity=hparams.attention_red_nonlinearity,\n          reduction_type=hparams.attention_red_type,\n          multihead_params=dict(\n              total_key_depth=total_key_depth,\n              total_value_depth=total_value_depth,\n              num_heads=hparams.num_heads,\n              dropout_rate=hparams.attention_dropout,\n          ),\n      ),\n  )\n\n  # === Unmasked memory-compressed multihead self attention layer ===\n  # Only works for self attention. Never mask the future. Bias never added\n  compressed_attention_fn = partial(\n      compressed_attention_masked_fn,\n      add_mask=False,\n  )\n\n  # Feed-forwards layers:\n\n  # === FC layer ===\n  conv_hidden_relu = register_layer(\n      common_layers.conv_hidden_relu,\n      default_kwargs=dict(\n          hidden_size=hparams.filter_size,\n          output_size=hparams.hidden_size,\n          dropout=hparams.relu_dropout,\n      ),\n  )\n\n  # === Separable convolution layer ===\n  # No mask applied\n  sep_conv_relu = partial(\n      conv_hidden_relu,\n      padding=\"SAME\",\n      # Parameters copied from the transformer model, could add hparams\n      kernel_size=(3, 1),\n      second_kernel_size=(31, 1),\n  )\n\n  # === Separable convolution layer (masked version) ===\n  # Mask the future\n  sep_conv_relu_masked = partial(\n      sep_conv_relu,\n      padding=\"LEFT\",  # Mask future for decoder\n  )\n\n  # Define all available layers\n\n  cur_layers = dict(\n      # Attention layers:\n      a=multihead_attention_fn,  # Multihead full attention\n      loc=local_attention_fn,  # Local attention\n      locm=local_attention_masked_fn,  # Local attention (masked)\n      red=compressed_attention_fn,  # Memory-compressed attention\n      redm=compressed_attention_masked_fn,  # Memory-compressed att (masked)\n      mem=memeff_attention_fn,  # Memory efficient\n      # Feed-forward layers:\n      fc=conv_hidden_relu,  # Fully connected\n      sep=sep_conv_relu,  # Separable convolution (unmasked)\n      sepm=sep_conv_relu_masked,  # Separable convolution (masked)\n  )\n  return cur_layers\n\n\ndef add_standard_attention_hparams(hparams):\n  \"\"\"Adds the hparams used by get_standardized_layers.\"\"\"\n  # All hyperparameters ending in \"dropout\" are automatically set to 0.0\n  # when not in training mode.\n\n  # hparams used and which should have been defined outside (in\n  # common_hparams):\n  # Global flags\n  # hparams.mode\n  # hparams.hidden_size\n  # Pre-post processing flags\n  # hparams.layer_preprocess_sequence\n  # hparams.layer_postprocess_sequence\n  # hparams.layer_prepostprocess_dropout\n  # hparams.norm_type\n  # hparams.norm_epsilon\n  # Mixture-of-Expert flags\n  # hparams.moe_hidden_sizes\n  # hparams.moe_num_experts\n  # hparams.moe_k\n  # hparams.moe_loss_coef\n\n  # Attention layers flags\n  hparams.add_hparam(\"num_heads\", 8)\n  hparams.add_hparam(\"attention_key_channels\", 0)\n  hparams.add_hparam(\"attention_value_channels\", 0)\n  hparams.add_hparam(\"attention_dropout\", 0.0)\n  # Attention: Local\n  hparams.add_hparam(\"attention_loc_block_length\", 256)\n  # Attention: Local (unmasked only): How much to look left.\n  hparams.add_hparam(\"attention_loc_block_width\", 128)\n  # Attention: Memory-compressed\n  hparams.add_hparam(\"attention_red_factor\", 3)\n  hparams.add_hparam(\"attention_red_type\", \"conv\")\n  hparams.add_hparam(\"attention_red_nonlinearity\", \"none\")\n\n  # Fully connected layers flags\n  # To be more consistent, should use filter_size to also control the MOE\n  # size if moe_hidden_sizes not set.\n  hparams.add_hparam(\"filter_size\", 2048)\n  hparams.add_hparam(\"relu_dropout\", 0.0)\n\n  return hparams\n\n\ndef encoder_decoder_attention_loss(expected_attention_logits,\n                                   actual_attentions,\n                                   loss_type=\"kl_divergence\",\n                                   loss_multiplier=1.0):\n  \"\"\"Computes encdec attention loss between expected and actual attentions.\n\n  Args:\n    expected_attention_logits: Tensor storing the expected encoder-decoder\n      attention logits with shape [batch_size, target_length, input_length].\n    actual_attentions: Dictionary with actual attention logits for different\n      attention types and hidden layers.\n    loss_type: type of the loss function.\n    loss_multiplier: multiplier for the attention loss.\n\n  Returns:\n    KL_divergence loss between the actual and expected attention logits.\n  \"\"\"\n\n  def combine_attentions(attention_list):\n    \"\"\"Combine different layer attentions and then average over layers/heads.\"\"\"\n    # Stack all hidden layer attention tensors to get a tensor with shape\n    # [num_hidden_layers, batch_size, num_heads, target_length, input_length].\n    attentions = tf.stack(attention_list)\n    # Reduce mean across all layers (axis=0) and all heads (axis=2) to get a\n    # tensor with shape [batch_size, target_length, input_length].\n    return tf.reduce_mean(attentions, [0, 2])\n\n  def kl_divergence_loss(expected_logits, actual_logits):\n    p = tfp.distributions.Categorical(logits=expected_logits)\n    q = tfp.distributions.Categorical(logits=actual_logits)\n    return tfp.distributions.kl_divergence(p, q)\n\n  def mse_loss(expected_logits, actual_weights):\n    expected_weights = tf.nn.softmax(expected_logits)\n    return tf.losses.mean_squared_error(expected_weights, actual_weights)\n\n  # For each hidden layer, we have attention-logit and attention-weight tensors\n  # with shape [batch_size, num_heads, target_length, input_length].\n  loss = 0.0\n  if loss_type == \"mse\":\n    actual_encdec_attention_weights = [\n        t for layer_key, t in actual_attentions.items()\n        if \"encdec_attention\" in layer_key and not layer_key.endswith(\"/logits\")\n    ]\n    actual_attention_weights = combine_attentions(\n        actual_encdec_attention_weights)\n    loss = mse_loss(expected_attention_logits, actual_attention_weights)\n  else:\n    actual_encdec_attention_logits = [\n        t for layer_key, t in actual_attentions.items()\n        if \"encdec_attention\" in layer_key and layer_key.endswith(\"/logits\")\n    ]\n    actual_attention_logits = combine_attentions(actual_encdec_attention_logits)\n    loss = kl_divergence_loss(expected_attention_logits,\n                              actual_attention_logits)\n  return loss * loss_multiplier\n\n\n@expert_utils.add_name_scope()\ndef get_timing_signal_1d(length,\n                         channels,\n                         min_timescale=1.0,\n                         max_timescale=1.0e4,\n                         start_index=0):\n  \"\"\"Gets a bunch of sinusoids of different frequencies.\n\n  Each channel of the input Tensor is incremented by a sinusoid of a different\n  frequency and phase.\n\n  This allows attention to learn to use absolute and relative positions.\n  Timing signals should be added to some precursors of both the query and the\n  memory inputs to attention.\n\n  The use of relative position is possible because sin(x+y) and cos(x+y) can be\n  expressed in terms of y, sin(x) and cos(x).\n\n  In particular, we use a geometric sequence of timescales starting with\n  min_timescale and ending with max_timescale.  The number of different\n  timescales is equal to channels / 2. For each timescale, we\n  generate the two sinusoidal signals sin(timestep/timescale) and\n  cos(timestep/timescale).  All of these sinusoids are concatenated in\n  the channels dimension.\n\n  Args:\n    length: scalar, length of timing signal sequence.\n    channels: scalar, size of timing embeddings to create. The number of\n        different timescales is equal to channels / 2.\n    min_timescale: a float\n    max_timescale: a float\n    start_index: index of first position\n\n  Returns:\n    a Tensor of timing signals [1, length, channels]\n  \"\"\"\n  position = tf.to_float(tf.range(length) + start_index)\n  num_timescales = channels // 2\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      tf.maximum(tf.to_float(num_timescales) - 1, 1))\n  inv_timescales = min_timescale * tf.exp(\n      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n  scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n  # Please note that this slightly differs from the published paper.\n  # See a discussion here: https://github.com/tensorflow/tensor2tensor/pull/177\n  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n  signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n  signal = tf.reshape(signal, [1, length, channels])\n  return signal\n\n\n@expert_utils.add_name_scope()\ndef add_timing_signal_1d(x,\n                         min_timescale=1.0,\n                         max_timescale=1.0e4,\n                         start_index=0):\n  \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n\n  Each channel of the input Tensor is incremented by a sinusoid of a different\n  frequency and phase.\n\n  This allows attention to learn to use absolute and relative positions.\n  Timing signals should be added to some precursors of both the query and the\n  memory inputs to attention.\n\n  The use of relative position is possible because sin(x+y) and cos(x+y) can be\n  expressed in terms of y, sin(x) and cos(x).\n\n  In particular, we use a geometric sequence of timescales starting with\n  min_timescale and ending with max_timescale.  The number of different\n  timescales is equal to channels / 2. For each timescale, we\n  generate the two sinusoidal signals sin(timestep/timescale) and\n  cos(timestep/timescale).  All of these sinusoids are concatenated in\n  the channels dimension.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    min_timescale: a float\n    max_timescale: a float\n    start_index: index of first position\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n  length = common_layers.shape_list(x)[1]\n  channels = common_layers.shape_list(x)[2]\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale,\n                                start_index)\n  return x + common_layers.cast_like(signal, x)\n\n\n@expert_utils.add_name_scope()\ndef get_layer_timing_signal_learned_1d(channels, layer, num_layers):\n  \"\"\"get n-dimensional embedding as the layer (vertical) timing signal.\n\n  Adds embeddings to represent the position of the layer in the tower.\n\n  Args:\n    channels: dimension of the timing signal\n    layer: layer num\n    num_layers: total number of layers\n\n  Returns:\n    a Tensor of timing signals [1, 1, channels].\n  \"\"\"\n  shape = [num_layers, 1, 1, channels]\n  layer_embedding = (\n      tf.get_variable(\n          \"layer_embedding\",\n          shape,\n          initializer=tf.random_normal_initializer(0, channels**-0.5)) *\n      (channels**0.5))\n  return layer_embedding[layer, :, :, :]\n\n\n@expert_utils.add_name_scope()\ndef add_layer_timing_signal_learned_1d(x, layer, num_layers):\n  \"\"\"Add n-dimensional embedding as the layer (vertical) timing signal.\n\n  Adds embeddings to represent the position of the layer in the tower.\n\n  Args:\n    x: a tensor with shape [batch, length, depth]\n    layer: layer num\n    num_layers: total number of layers\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n  channels = common_layers.shape_list(x)[-1]\n  signal = get_layer_timing_signal_learned_1d(channels, layer, num_layers)\n  x += signal\n  return x\n\n\n@expert_utils.add_name_scope()\ndef get_layer_timing_signal_sinusoid_1d(channels, layer, num_layers):\n  \"\"\"Add sinusoids of different frequencies as layer (vertical) timing signal.\n\n  Args:\n    channels: dimension of the timing signal\n    layer: layer num\n    num_layers: total number of layers\n\n  Returns:\n    a Tensor of timing signals [1, 1, channels].\n  \"\"\"\n\n  signal = get_timing_signal_1d(num_layers, channels)\n  layer_signal = tf.expand_dims(signal[:, layer, :], axis=1)\n\n  return layer_signal\n\n\n@expert_utils.add_name_scope()\ndef add_layer_timing_signal_sinusoid_1d(x, layer, num_layers):\n  \"\"\"Add sinusoids of different frequencies as layer (vertical) timing signal.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    layer: layer num\n    num_layers: total number of layers\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n\n  channels = common_layers.shape_list(x)[-1]\n  signal = get_layer_timing_signal_sinusoid_1d(channels, layer, num_layers)\n\n  return x + signal\n\n\n@expert_utils.add_name_scope()\ndef add_timing_signals_given_positions(x,\n                                       positions,\n                                       min_timescale=1.0,\n                                       max_timescale=1.0e4):\n  \"\"\"Adds sinusoids of diff frequencies to a Tensor, with timing positions given.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    positions: a list of positions, each of which can either be a Tensor of\n      shape [batch, length] or None for a default of (0..length]\n    min_timescale: a float\n    max_timescale: a float\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n  shape = common_layers.shape_list(x)\n  batch = shape[0]\n  length = shape[1]\n  channels = shape[2]\n  num_dims = len(positions)\n  num_timescales = channels // (num_dims * 2)\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      (tf.to_float(num_timescales) - 1))\n  inv_timescales = min_timescale * tf.exp(\n      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n  for dim, position in enumerate(positions):\n    if position is None:\n      # Create a [batch, length] Tensor of incrementing positions 0..length-1.\n      position = tf.tile(\n          tf.transpose(tf.expand_dims(tf.range(0, length), axis=1)), [batch, 1])\n    scaled_time = (\n        tf.expand_dims(tf.to_float(position), 2) *\n        tf.expand_dims(tf.expand_dims(inv_timescales, 0), 0))\n    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=2)\n    prepad = dim * 2 * num_timescales\n    postpad = channels - (dim + 1) * 2 * num_timescales\n    signal = tf.pad(signal, [[0, 0], [0, 0], [prepad, postpad]])\n    signal = common_layers.cast_like(signal, x)\n    x += signal\n  return x\n\n\n@expert_utils.add_name_scope()\ndef add_timing_signals_from_features(x,\n                                     features,\n                                     position_features,\n                                     min_timescale=1.0,\n                                     max_timescale=1.0e4):\n  \"\"\"Adds timing signals from features named in `position_features`.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    features: a features dictionary\n    position_features: a comma-delimited string where each item is either a\n      feature key or the empty string (which denotes a default position tensor\n      of [0..length])\n    min_timescale: a float\n    max_timescale: a float\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n  return add_timing_signals_given_positions(x, [\n      features.get(position_feature)\n      for position_feature in position_features.split(\",\")\n  ], min_timescale, max_timescale)\n\n\n@expert_utils.add_name_scope()\ndef add_timing_signal_1d_given_position(x,\n                                        position,\n                                        min_timescale=1.0,\n                                        max_timescale=1.0e4):\n  \"\"\"Adds sinusoids of diff frequencies to a Tensor, with timing position given.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    position: a Tensor with shape [batch, length]\n    min_timescale: a float\n    max_timescale: a float\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n  channels = common_layers.shape_list(x)[2]\n  num_timescales = channels // 2\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      (tf.to_float(num_timescales) - 1))\n  inv_timescales = min_timescale * tf.exp(\n      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n  scaled_time = (\n      tf.expand_dims(tf.to_float(position), 2) * tf.expand_dims(\n          tf.expand_dims(inv_timescales, 0), 0))\n  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=2)\n  signal = tf.pad(signal, [[0, 0], [0, 0], [0, tf.mod(channels, 2)]])\n  signal = common_layers.cast_like(signal, x)\n  return x + signal\n\n\n@expert_utils.add_name_scope()\ndef add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n  \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n\n  Each channel of the input Tensor is incremented by a sinusoid of a different\n  frequency and phase in one of the positional dimensions.\n\n  This allows attention to learn to use absolute and relative positions.\n  Timing signals should be added to some precursors of both the query and the\n  memory inputs to attention.\n\n  The use of relative position is possible because sin(a+b) and cos(a+b) can be\n  expressed in terms of b, sin(a) and cos(a).\n\n  x is a Tensor with n \"positional\" dimensions, e.g. one dimension for a\n  sequence or two dimensions for an image\n\n  We use a geometric sequence of timescales starting with\n  min_timescale and ending with max_timescale.  The number of different\n  timescales is equal to channels // (n * 2). For each timescale, we\n  generate the two sinusoidal signals sin(timestep/timescale) and\n  cos(timestep/timescale).  All of these sinusoids are concatenated in\n  the channels dimension.\n\n  Args:\n    x: a Tensor with shape [batch, d1 ... dn, channels]\n    min_timescale: a float\n    max_timescale: a float\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n  num_dims = len(x.get_shape().as_list()) - 2\n  channels = common_layers.shape_list(x)[-1]\n  num_timescales = channels // (num_dims * 2)\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      (tf.to_float(num_timescales) - 1))\n  inv_timescales = min_timescale * tf.exp(\n      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n  for dim in range(num_dims):\n    length = common_layers.shape_list(x)[dim + 1]\n    position = tf.to_float(tf.range(length))\n    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n        inv_timescales, 0)\n    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n    prepad = dim * 2 * num_timescales\n    postpad = channels - (dim + 1) * 2 * num_timescales\n    signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n    for _ in range(1 + dim):\n      signal = tf.expand_dims(signal, 0)\n    for _ in range(num_dims - 1 - dim):\n      signal = tf.expand_dims(signal, -2)\n    x += signal\n  return x\n\n\ndef add_positional_embedding(x, max_length, name=None, positions=None):\n  \"\"\"Adds positional embedding.\n\n  Args:\n    x: Tensor with shape [batch, length, depth].\n    max_length: int representing static maximum size of any dimension.\n    name: str representing name of the embedding tf.Variable.\n    positions: Tensor with shape [batch, length].\n\n  Returns:\n    Tensor of same shape as x.\n  \"\"\"\n  with tf.name_scope(\"add_positional_embedding\"):\n    _, length, depth = common_layers.shape_list(x)\n    var = tf.cast(tf.get_variable(name, [max_length, depth]), x.dtype)\n    if positions is None:\n      pad_length = tf.maximum(0, length - max_length)\n      sliced = tf.cond(\n          tf.less(length, max_length),\n          lambda: tf.slice(var, [0, 0], [length, -1]),\n          lambda: tf.pad(var, [[0, pad_length], [0, 0]]))\n      return x + tf.expand_dims(sliced, 0)\n    else:\n      return x + tf.gather(var, tf.to_int32(positions))\n\n\ndef add_positional_embedding_nd(x, max_length, name=None):\n  \"\"\"Adds n-dimensional positional embedding.\n\n  The embeddings add to all positional dimensions of the tensor.\n\n  Args:\n    x: Tensor with shape [batch, p1 ... pn, depth]. It has n positional\n      dimensions, i.e., 1 for text, 2 for images, 3 for video, etc.\n    max_length: int representing static maximum size of any dimension.\n    name: str representing name of the embedding tf.Variable.\n\n  Returns:\n    Tensor of same shape as x.\n  \"\"\"\n  with tf.name_scope(\"add_positional_embedding_nd\"):\n    x_shape = common_layers.shape_list(x)\n    num_dims = len(x_shape) - 2\n    depth = x_shape[-1]\n    base_shape = [1] * (num_dims + 1) + [depth]\n    base_start = [0] * (num_dims + 2)\n    base_size = [-1] + [1] * num_dims + [depth]\n    for i in range(num_dims):\n      shape = base_shape[:]\n      start = base_start[:]\n      size = base_size[:]\n      shape[i + 1] = max_length\n      size[i + 1] = x_shape[i + 1]\n      var = tf.get_variable(\n          name + \"_%d\" % i,\n          shape,\n          initializer=tf.random_normal_initializer(0, depth**-0.5))\n      var = var * depth**0.5\n      x += tf.slice(var, start, size)\n    return x\n\n\ndef make_edge_vectors(adjacency_matrix, num_edge_types, depth, name=None):\n  \"\"\"Gets edge vectors for the edge types in the adjacency matrix.\n\n  Args:\n    adjacency_matrix: A [batch, num_nodes, num_nodes] tensor of ints.\n    num_edge_types: Number of different edge types\n    depth: Number of channels\n    name: a string\n  Returns:\n    A [batch, num_nodes, num_nodes, depth] vector of tensors\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"edge_vectors\"):\n    att_adj_vectors_shape = [num_edge_types, depth]\n    adjacency_matrix_shape = common_layers.shape_list(adjacency_matrix)\n    adj_vectors = (\n        tf.get_variable(\n            \"adj_vectors\",\n            att_adj_vectors_shape,\n            initializer=tf.random_normal_initializer(0, depth**-0.5)) *\n        (depth**0.5))\n    # Avoiding gathers so that it works on TPUs\n    # adjacency_matrix_one_hot has shape\n    # [batch, num_nodes, num_nodes, num_edge_types]\n\n    adjacency_matrix_one_hot = tf.one_hot(adjacency_matrix, num_edge_types)\n\n    att_adj_vectors = tf.matmul(\n        tf.reshape(tf.to_float(adjacency_matrix_one_hot), [-1, num_edge_types]),\n        adj_vectors)\n    return tf.reshape(att_adj_vectors,\n                      [adjacency_matrix_shape[0], adjacency_matrix_shape[1],\n                       adjacency_matrix_shape[2], depth])\n\n\nclass LshGating(object):\n  \"\"\"Class to split key/queries into separate buckets.\"\"\"\n\n  def __init__(self, depth, nb_hyperplanes, nb_replicat=1, trainable=False):\n    \"\"\"Construct the gating function parameters.\n\n    Compute the gates for a single head.\n\n    Args:\n      depth (int): Dimension of the key/queries to dispatch\n      nb_hyperplanes (int): Nb of vectors use to split the space. Will determine\n        the number of buckets (2^nb_hyperplanes - 1).\n      nb_replicat (int): Redundancy to avoid the edge cases (to be in one bucket\n        the input should be in a majority)\n      trainable (bool): If True, a balance loss is added to force the hyperplane\n        to divide the key/query space evenly\n    \"\"\"\n    self.depth = depth\n    self.nb_hyperplanes = nb_hyperplanes\n    self.nb_buckets = 2**nb_hyperplanes\n    self.nb_replicat = nb_replicat  # Unused for now\n    self.trainable = trainable  # Unused for now\n\n    self.dispatchers = {}\n\n    assert self.nb_replicat == 1  # For now\n\n    with tf.variable_scope(\"lsh_gating\"):\n      # Vectors defining the hyperplanes\n      self.t_vectors = tf.get_variable(\n          \"vector\",\n          shape=(self.depth, self.nb_hyperplanes * self.nb_replicat),\n          dtype=tf.float32,\n          trainable=self.trainable,\n      )\n      # Projection vector from the bit space to similarity score space\n      self.t_group = tf.constant(\n          [self._idx_to_bits(i) for i in range(self.nb_buckets)],\n          dtype=tf.float32,\n          name=\"group\")\n\n  def _idx_to_bits(self, i):\n    \"\"\"Convert an group index to its bit representation.\"\"\"\n    bits = bin(i)[2:].zfill(self.nb_hyperplanes)  # Pad the bits str with 0\n    return [-1.0 if b == \"0\" else 1.0 for b in bits]\n\n  @expert_utils.add_name_scope(\"lsh_gating\")\n  def get_gates(self, x):\n    \"\"\"Return the bucket id of the given tensor.\n\n    Args:\n      x (tf.Tensor): float32 of shape [length, depth]\n\n    Returns:\n      tf.Tensor: One-hot vector int64 of shape [heads, length, nb_buckets]\n        containing the id of the bucket\n    \"\"\"\n\n    # The balance loss don't propagate to the rest of the network\n    x = tf.stop_gradient(x)\n    # [length, depth] * [depth, nb_vectors * replicat]\n    x = tf.matmul(x, self.t_vectors)\n    # [length, nb_vector * replicat]\n    x = tf.sign(x)  # Get on which side of the hyperplane the keys are.\n\n    # x = tf.reshape(x, [-1, nb_replicat, nb_vector])\n    # [length, replicat, nb_vector] * [nb_vector, 2^nb_vector - 1]\n\n    x = tf.matmul(x, self.t_group, transpose_b=True) / self.nb_hyperplanes\n    # We get a similarity score for each of the group between [-1, 1]\n    # [length, (replicat,) 2^nb_vector - 1]\n    # Do an argmax to get the most likely group for each replicat\n    x = tf.argmax(x, axis=-1)\n    # [length(, replicat)]\n    # One-hot for compatibility with the sparse dispatcher\n    x = tf.one_hot(x, self.nb_buckets)\n    # TODO(epot): Use a loss to force an even distribution\n    return x\n\n\n@expert_utils.add_name_scope()\ndef embedding_to_padding(emb):\n  \"\"\"Calculates the padding mask based on which embeddings are all zero.\n\n  We have hacked symbol_modality to return all-zero embeddings for padding.\n\n  Args:\n    emb: a Tensor with shape [..., depth].\n\n  Returns:\n    a float Tensor with shape [...]. Each element is 1 if its corresponding\n    embedding vector is all zero, and is 0 otherwise.\n  \"\"\"\n  emb_sum = tf.reduce_sum(tf.abs(emb), axis=-1)\n  return tf.to_float(tf.equal(emb_sum, 0.0))\n\n\n@expert_utils.add_name_scope()\ndef padding_to_length(padding):\n  \"\"\"Calculate the length of mask based on padding.\n\n  Args:\n    padding: a Tensor with shape [..., length].\n  Returns:\n    a Tensor with shape [...].\n  \"\"\"\n  non_padding = 1.0 - padding\n  return tf.to_int32(tf.reduce_sum(non_padding, axis=-1))\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_local(length, max_backward, max_forward):\n  \"\"\"Create an bias tensor to be added to attention logits.\n\n  A position may attend to positions at most max_distance from it,\n  forward and backwards.\n\n  This does not actually save any computation.\n\n  Args:\n    length: int\n    max_backward: int, maximum distance backward to attend. Negative values\n      indicate unlimited.\n    max_forward: int, maximum distance forward to attend. Negative values\n      indicate unlimited.\n\n  Returns:\n    a `Tensor` with shape [1, 1, length, length].\n  \"\"\"\n  band = common_layers.ones_matrix_band_part(\n      length,\n      length,\n      max_backward,\n      max_forward,\n      out_shape=[1, 1, length, length])\n  return -1e9 * (1.0 - band)\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_lower_triangle(length):\n  \"\"\"Create an bias tensor to be added to attention logits.\n\n  Allows a query to attend to all positions up to and including its own.\n\n  Args:\n   length: a Scalar.\n\n  Returns:\n    a `Tensor` with shape [1, 1, length, length].\n  \"\"\"\n  return attention_bias_local(length, -1, 0)\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_same_segment(query_segment_id, memory_segment_id):\n  \"\"\"Create an bias tensor to be added to attention logits.\n\n  Positions with the same segment_ids can see each other.\n\n  Args:\n    query_segment_id: a float `Tensor` with shape [batch, query_length].\n    memory_segment_id: a float `Tensor` with shape [batch, memory_length].\n\n  Returns:\n    a `Tensor` with shape [batch, 1, query_length, memory_length].\n  \"\"\"\n  ret = (tf.to_float(\n      tf.not_equal(\n          tf.expand_dims(query_segment_id, 2),\n          tf.expand_dims(memory_segment_id, 1))) *\n         large_compatible_negative(memory_segment_id.dtype))\n  return tf.expand_dims(ret, axis=1)\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_ignore_padding(memory_padding):\n  \"\"\"Create an bias tensor to be added to attention logits.\n\n  Args:\n    memory_padding: a float `Tensor` with shape [batch, memory_length].\n\n  Returns:\n    a `Tensor` with shape [batch, 1, 1, memory_length].\n  \"\"\"\n  ret = memory_padding * large_compatible_negative(memory_padding.dtype)\n  return tf.expand_dims(tf.expand_dims(ret, axis=1), axis=1)\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_to_padding(attention_bias,\n                              cast_fn=(lambda x: tf.cast(x, tf.float32))):\n  \"\"\"Inverse of attention_bias_ignore_padding().\n\n  Args:\n    attention_bias: a `Tensor` with shape [batch, 1, 1, memory_length], as\n      returned by attention_bias_ignore_padding().\n    cast_fn: function used to cast to output type.\n\n  Returns:\n    a Tensor with shape [batch, memory_length] with 1.0 in padding positions\n    and 0.0 in non-padding positions. Type is determined by cast_fn.\n  \"\"\"\n  # `attention_bias` is a large negative number in padding positions and 0.0\n  # elsewhere.\n  return tf.squeeze(cast_fn(tf.less(attention_bias, -1)), axis=[1, 2])\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_prepend_inputs_full_attention(padding):\n  \"\"\"Create a bias tensor for prepend_mode=\"prepend_inputs_full_attention\".\n\n  See prepend_inputs in common_hparams.py.\n\n  Produces a bias tensor to be used in self-attention.\n\n  This bias tensor allows for full connectivity in the \"inputs\" part of\n  the sequence and masked connectivity in the targets part.\n\n  Args:\n    padding: a float `Tensor` with shape [batch, length] with\n      ones in positions corresponding to padding.  In each row, a single\n      padding position separates the input part from the target part.\n\n  Returns:\n    a `Tensor` with shape [batch, 1, length, length].\n  \"\"\"\n  # Everything past the first padding position is part of the target.\n  # This Tensor has zeros for the source portion and separator,\n  # and ones for the target portion.\n  in_target = tf.cumsum(padding, axis=1, exclusive=True)\n  # The position within the target, or 0 if part of the source.\n  target_pos = tf.cumsum(in_target, axis=1)\n  # A position with a lesser target_pos cannot see a position with greater\n  # target_pos.\n  illegal_connections = tf.greater(\n      tf.expand_dims(target_pos, 1), tf.expand_dims(target_pos, 2))\n  bias = tf.to_float(illegal_connections) * -1e9\n  bias = tf.expand_dims(bias, 1)\n  return bias\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_proximal(length):\n  \"\"\"Bias for self-attention to encourage attention to close positions.\n\n  Args:\n    length: an integer scalar.\n\n  Returns:\n    a Tensor with shape [1, 1, length, length]\n  \"\"\"\n  r = tf.to_float(tf.range(length))\n  diff = tf.expand_dims(r, 0) - tf.expand_dims(r, 1)\n  return tf.expand_dims(tf.expand_dims(-tf.log1p(tf.abs(diff)), 0), 0)\n\n\n@expert_utils.add_name_scope()\ndef attention_bias_batch(batch_coordinates_q,\n                         batch_coordinates_k=None,\n                         condition_fn=None):\n  \"\"\"Generate a mask to prevent the batch to attend to each others.\n\n  Args:\n    batch_coordinates_q: Int-like Tensor of shape [length_q, 1] containing the\n      coordinates of the batches\n    batch_coordinates_k: Int-like Tensor of shape [length_k, 1] containing the\n      coordinates of the batches. If None, do self-attention.\n    condition_fn: Callable defining the attention mask.\n\n  Returns:\n    Float-like Tensor of shape [length_q, length_k] containing either 0 or\n    -infinity (-1e9).\n  \"\"\"\n  if batch_coordinates_k is None:\n    batch_coordinates_k = batch_coordinates_q\n\n  # Convert to float first because of b/25387198.\n  def to_float(bc):\n    bc = tf.squeeze(bc, 1)\n    bc = tf.to_float(bc)\n    return bc\n\n  # Broadcast to create [length_q, length_k] mask.\n  bc_v = tf.expand_dims(to_float(batch_coordinates_q), 1)\n  bc_h = tf.expand_dims(to_float(batch_coordinates_k), 0)\n  bias_batch = bc_h - bc_v\n  bias_batch = condition_fn(bias_batch)\n  bias_batch *= -1e9\n  return bias_batch\n\n\n# Mask to prevent individual sequences of the same batch to attend to each other\nattention_bias_coordinates = functools.partial(\n    attention_bias_batch,\n    condition_fn=lambda bias: tf.minimum(1.0, tf.abs(bias)),\n)\n\n# Mask similar to upper triangular mask, but allow dispatching\nattention_bias_future = functools.partial(\n    attention_bias_batch,\n    # Elems can attend to themselves (otherwise would use bias_batch + 1.0).\n    # No tf.abs to consider the order,\n    # tf.maximum and tf.minimum to threshold the values.\n    condition_fn=lambda bias: tf.maximum(0.0, tf.minimum(1.0, bias)),\n)\n\n\n@expert_utils.add_name_scope()\ndef split_last_dimension(x, n):\n  \"\"\"Reshape x so that the last dimension becomes two dimensions.\n\n  The first of these two dimensions is n.\n\n  Args:\n    x: a Tensor with shape [..., m]\n    n: an integer.\n\n  Returns:\n    a Tensor with shape [..., n, m/n]\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  m = x_shape[-1]\n  if isinstance(m, int) and isinstance(n, int):\n    assert m % n == 0\n  return tf.reshape(x, x_shape[:-1] + [n, m // n])\n\n\n@expert_utils.add_name_scope()\ndef combine_last_two_dimensions(x):\n  \"\"\"Reshape x so that the last two dimension become one.\n\n  Args:\n    x: a Tensor with shape [..., a, b]\n\n  Returns:\n    a Tensor with shape [..., ab]\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  a, b = x_shape[-2:]\n  return tf.reshape(x, x_shape[:-2] + [a * b])\n\n\n@expert_utils.add_name_scope()\ndef combine_first_two_dimensions(x):\n  \"\"\"Reshape x so that the first two dimension become one.\n\n  Args:\n    x: a Tensor with shape [a, b, ...]\n\n  Returns:\n    a Tensor with shape [ab, ...]\n  \"\"\"\n  ret = tf.reshape(x, tf.concat([[-1], common_layers.shape_list(x)[2:]], 0))\n  old_shape = x.get_shape().dims\n  a, b = old_shape[:2]\n  new_shape = [a * b if a and b else None] + old_shape[2:]\n  ret.set_shape(new_shape)\n  return ret\n\n\n@expert_utils.add_name_scope()\ndef split_heads(x, num_heads):\n  \"\"\"Split channels (dimension 2) into multiple heads (becomes dimension 1).\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    num_heads: an integer\n\n  Returns:\n    a Tensor with shape [batch, num_heads, length, channels / num_heads]\n  \"\"\"\n  return tf.transpose(split_last_dimension(x, num_heads), [0, 2, 1, 3])\n\n\n@expert_utils.add_name_scope()\ndef split_heads_2d(x, num_heads):\n  \"\"\"Split channels (dimension 3) into multiple heads (becomes dimension 1).\n\n  Args:\n    x: a Tensor with shape [batch, height, width, channels]\n    num_heads: an integer\n\n  Returns:\n    a Tensor with shape [batch, num_heads, height, width, channels / num_heads]\n  \"\"\"\n  return tf.transpose(split_last_dimension(x, num_heads), [0, 3, 1, 2, 4])\n\n\ndef split_heads_nd(x, num_heads):\n  \"\"\"Split the depth dimension (last dimension) into multiple heads.\n\n  Args:\n    x: a [batch, d1, ..., dn, depth] tensor\n    num_heads: an integer\n\n  Returns:\n    a [batch, num_heads, d1, ..., dn, depth // num_heads]\n  \"\"\"\n  num_dimensions = len(common_layers.shape_list(x)) - 2\n  return tf.transpose(\n      split_last_dimension(x, num_heads), [0, num_dimensions + 1] +\n      list(range(1, num_dimensions + 1)) + [num_dimensions + 2])\n\n\n@expert_utils.add_name_scope()\ndef combine_heads(x):\n  \"\"\"Inverse of split_heads.\n\n  Args:\n    x: a Tensor with shape [batch, num_heads, length, channels / num_heads]\n\n  Returns:\n    a Tensor with shape [batch, length, channels]\n  \"\"\"\n  return combine_last_two_dimensions(tf.transpose(x, [0, 2, 1, 3]))\n\n\n@expert_utils.add_name_scope()\ndef combine_heads_2d(x):\n  \"\"\"Inverse of split_heads_2d.\n\n  Args:\n    x: a Tensor with shape\n      [batch, num_heads, height, width, channels / num_heads]\n\n  Returns:\n    a Tensor with shape [batch, height, width, channels]\n  \"\"\"\n  return combine_last_two_dimensions(tf.transpose(x, [0, 2, 3, 1, 4]))\n\n\ndef combine_heads_nd(x):\n  \"\"\"Inverse of split_heads_nd.\n\n  Args:\n    x: a [batch, num_heads, d1, ..., dn, depth // num_heads] tensor\n\n  Returns:\n    a [batch, d1, ...., dn, depth] tensor\n  \"\"\"\n  num_dimensions = len(common_layers.shape_list(x)) - 3\n  return combine_last_two_dimensions(\n      tf.transpose(x, [0] + list(range(2, num_dimensions + 2)) +\n                   [1, num_dimensions + 2]))\n\n\ndef attention_image_summary(attn, image_shapes=None):\n  \"\"\"Compute color image summary.\n\n  Args:\n    attn: a Tensor with shape [batch, num_heads, query_length, memory_length]\n    image_shapes: optional tuple of integer scalars.\n      If the query positions and memory positions represent the\n      pixels of flattened images, then pass in their dimensions:\n        (query_rows, query_cols, memory_rows, memory_cols).\n      If the query positions and memory positions represent the\n      pixels x channels of flattened images, then pass in their dimensions:\n        (query_rows, query_cols, query_channels,\n         memory_rows, memory_cols, memory_channels).\n  \"\"\"\n  attn = tf.cast(attn, tf.float32)\n  num_heads = common_layers.shape_list(attn)[1]\n  # [batch, query_length, memory_length, num_heads]\n  image = tf.transpose(attn, [0, 2, 3, 1])\n  image = tf.pow(image, 0.2)  # for high-dynamic-range\n  # Each head will correspond to one of RGB.\n  # pad the heads to be a multiple of 3\n  image = tf.pad(image, [[0, 0], [0, 0], [0, 0], [0, tf.mod(-num_heads, 3)]])\n  image = split_last_dimension(image, 3)\n  image = tf.reduce_max(image, 4)\n  if image_shapes is not None:\n    if len(image_shapes) == 4:\n      q_rows, q_cols, m_rows, m_cols = list(image_shapes)\n      image = tf.reshape(image, [-1, q_rows, q_cols, m_rows, m_cols, 3])\n      image = tf.transpose(image, [0, 1, 3, 2, 4, 5])\n      image = tf.reshape(image, [-1, q_rows * m_rows, q_cols * m_cols, 3])\n    else:\n      assert len(image_shapes) == 6\n      q_rows, q_cols, q_channnels, m_rows, m_cols, m_channels = list(\n          image_shapes)\n      image = tf.reshape(\n          image,\n          [-1, q_rows, q_cols, q_channnels, m_rows, m_cols, m_channels, 3])\n      image = tf.transpose(image, [0, 1, 4, 3, 2, 5, 6, 7])\n      image = tf.reshape(\n          image,\n          [-1, q_rows * m_rows * q_channnels, q_cols * m_cols * m_channels, 3])\n  tf.summary.image(\"attention\", image, max_outputs=1)\n\n\ndef grouped_attention_multihead(query_antecedent,\n                                memory_antecedent,\n                                total_key_depth,\n                                total_value_depth,\n                                output_depth,\n                                num_heads,\n                                num_groups,\n                                memory_target_density=2.0,\n                                multiplicative_overhead=1.25,\n                                additive_overhead=8.0,\n                                mask_right=False,\n                                make_image_summary=True,\n                                name=None):\n  \"\"\"Multi-head dot-product attention with sparsity.\n\n  For each attention head, the queries are partitioned into groups.\n  For each group, only a subset of the key-value pairs are considered.\n\n  The choices of groups are selected based on trained predictors of\n  the total attention given the group inclusion.\n\n  memory_target_density indicates the average how many groups in which\n  a key-value pair should participate.\n\n  We use auxiliary losses to ensure that each group contains roughly\n  the same number of queries and the same number of key-value pairs.\n  If for a given sequence, the actual number of queries/pairs sent to\n  an expert exceeds this target by a factor of more than\n  multiplicative_overhead, then the last ones are dropped.  We use\n  this drop-last policy to avoid bleeding information backwards, which\n  is necessary when using this function with autoregressive\n  prediction.\n\n  Args:\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\n    memory_antecedent: a Tensor with shape [batch, length_m, channels]\n    total_key_depth: an integer\n    total_value_depth: an integer\n    output_depth: an integer\n    num_heads: an integer dividing total_key_depth and total_value_depth\n    num_groups: an integer\n    memory_target_density: a floating point scalar\n    multiplicative_overhead: a floating point scalar\n    additive_overhead: a floating point scalar\n    mask_right: a boolean\n    make_image_summary: a boolean\n    name: an optional string\n\n  Returns:\n    A Tensor with shape [batch, length_q, output_depth]\n\n  Raises:\n    ValueError: if the key depth or value depth are not divisible by the\n      number of attention heads.\n  \"\"\"\n  batch = common_layers.shape_list(query_antecedent)[0]\n  length_q = common_layers.shape_list(query_antecedent)[1]\n  length_kv = common_layers.shape_list(memory_antecedent)[1]\n\n  if total_key_depth % num_heads != 0:\n    raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_key_depth, num_heads))\n  depth_qk = total_key_depth // num_heads\n  if total_value_depth % num_heads != 0:\n    raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_value_depth, num_heads))\n  depth_v = total_value_depth // num_heads\n  with tf.variable_scope(\n      name, default_name=\"multihead_attention_sparse\",\n      values=[query_antecedent, memory_antecedent]):\n    q = common_layers.dense(\n        query_antecedent, total_key_depth, use_bias=False, name=\"q_transform\")\n    kv = common_layers.dense(\n        memory_antecedent,\n        total_key_depth + total_value_depth,\n        use_bias=False,\n        name=\"kv_transform\")\n    q = split_heads(q, num_heads)\n    kv = split_heads(kv, num_heads)\n    # Make predictions about q_total and m_total.\n    # These are used to determine group inclusion.\n    # We will train these by auxiliary losses.  We use stop_gradient here\n    # to keep these losses from back-propagating to the rest of the model.\n    # We add biases that help balance the usage of the experts.\n    q_pred = common_layers.dense(\n        tf.stop_gradient(query_antecedent),\n        num_heads * num_groups,\n        use_bias=False,\n        name=\"q_pred\")\n    q_pred = split_heads(q_pred, num_heads)\n    q_bias = tf.get_variable(\"q_bias\", [1, num_heads, 1, num_groups])\n    q_pred_biased = q_pred + q_bias\n    m_pred = common_layers.dense(\n        tf.stop_gradient(memory_antecedent),\n        num_heads * num_groups,\n        use_bias=False,\n        name=\"m_pred\")\n    m_pred = split_heads(m_pred, num_heads)\n    m_bias = tf.get_variable(\"m_bias\", [1, num_heads, 1, num_groups])\n    m_pred_biased = m_pred + m_bias\n    q *= depth_qk**-0.5\n    # q, kv, q_pred, m_pred are all [batch, heads, length_[q/m], ?]\n    # now reshape them all to [batch * heads, length, ?]\n    q = combine_first_two_dimensions(q)\n    kv = combine_first_two_dimensions(kv)\n    q_pred = combine_first_two_dimensions(q_pred)\n    m_pred = combine_first_two_dimensions(m_pred)\n    q_pred_biased = combine_first_two_dimensions(q_pred_biased)\n    m_pred_biased = combine_first_two_dimensions(m_pred_biased)\n    q_group = tf.argmax(q_pred_biased, axis=2)\n    q_requests = tf.one_hot(q_group, num_groups, axis=-1)\n    m_requests = tf.to_float(tf.greater(m_pred_biased, 0.0))\n    # include first memory position in all groups, to avoid division by zero.\n    m_requests = tf.maximum(\n        m_requests, tf.reshape(tf.one_hot([0], length_kv), [1, length_kv, 1]))\n    q_group_size = tf.reduce_sum(q_requests, 1)\n    m_group_size = tf.reduce_sum(m_requests, 1)\n    q_group_target_size = tf.to_float(length_q) / tf.to_float(num_groups)\n    m_group_target_size = (\n        tf.to_float(length_kv) * memory_target_density /\n        tf.to_float(num_groups))\n    capacity_q = tf.minimum(\n        length_q,\n        tf.to_int32(q_group_target_size * multiplicative_overhead +\n                    additive_overhead))\n    capacity_m = tf.minimum(\n        length_kv,\n        tf.to_int32(m_group_target_size * multiplicative_overhead +\n                    additive_overhead))\n    q_dispatcher = expert_utils.TruncatingDispatcher(q_requests, capacity_q)\n    m_dispatcher = expert_utils.TruncatingDispatcher(m_requests, capacity_m)\n    q_gates = q_dispatcher.gates()\n    m_gates = m_dispatcher.gates()\n    dispatched_q = q_dispatcher.dispatch(q)\n    dispatched_kv = m_dispatcher.dispatch(kv)\n    # dispatched_q: [batch * num_heads, num_groups, capacity_q, depth_qk]\n    # dispatched_kv:\n    #   [batch * num_heads, num_groups, capacity_m, depth_qk + depth_v]\n    k, v = tf.split(dispatched_kv, [depth_qk, depth_v], axis=3)\n    logits = tf.matmul(dispatched_q, k, transpose_b=True)\n    bias = tf.expand_dims((m_dispatcher.nonpadding() - 1.0) * 1e9, 2)\n    if mask_right:\n      q_coordinate = tf.to_float(\n          tf.expand_dims(q_dispatcher.length_coordinate(), 3))\n      m_coordinate = tf.to_float(\n          tf.expand_dims(m_dispatcher.length_coordinate(), 2))\n      bias += tf.to_float(tf.greater(m_coordinate, q_coordinate)) * -1e9\n    logits += bias\n    log_weights = tf.nn.log_softmax(logits)\n    weights = tf.exp(log_weights)\n    # For each query, this is the log of the sum of the unnormalized weights.\n    q_total = tf.stop_gradient(logits[:, :, :, :1] - log_weights[:, :, :, :1])\n    # For each key, this is the sum of the normalized weights.\n    m_total = tf.expand_dims(\n        tf.reduce_sum(tf.stop_gradient(weights), axis=2), -1)\n    o = tf.matmul(weights, v)\n    o = q_dispatcher.combine(o)\n\n    o = tf.reshape(o, [batch, num_heads, length_q, depth_v])\n    o = combine_heads(o)\n    o = common_layers.dense(\n        o, output_depth, use_bias=False, name=\"output_transform\")\n\n    m_total = m_dispatcher.combine(m_total)\n    q_total = q_dispatcher.combine(q_total)\n    q_total = tf.squeeze(q_total, -1)\n    m_total = tf.squeeze(m_total, -1)\n    # Compute summed m predictions for all groups\n    m_pred_used = tf.reduce_sum(tf.exp(m_pred) * m_dispatcher.gates(), axis=2)\n    q_pred_used = tf.reduce_sum(q_pred * q_dispatcher.gates(), axis=2)\n    epsilon = 1e-3\n    m_pred_used = tf.log(m_pred_used + epsilon)\n    m_total = tf.log(m_total + epsilon)\n    m_loss = tf.nn.l2_loss(m_total - m_pred_used)\n    q_loss = tf.nn.l2_loss(\n        (q_total - q_pred_used) * tf.reduce_sum(q_gates, axis=2))\n\n    q_loss /= tf.to_float(batch * length_q)\n    m_loss /= tf.to_float(batch * length_kv)\n\n    # We would like the query groups to be equal sized.  The group\n    # size is discrete, so we need some trick here.  We add a loss\n    # proportional to the product of the group size and the\n    # predictions for that group.  This encourages the predictions to\n    # decrease for groups that are too big.\n    q_group_deviation = (q_group_size / q_group_target_size) - 1.0\n    q_balance_loss = tf.reduce_sum(\n        tf.reduce_mean(q_pred_biased, axis=1) *\n        q_group_deviation) / tf.to_float(batch)\n    m_group_deviation = (m_group_size / m_group_target_size) - 1.0\n    m_balance_loss = tf.reduce_sum(\n        tf.reduce_mean(m_pred_biased, axis=1) *\n        m_group_deviation) / tf.to_float(batch)\n\n    # The losses in this function only propagate back to variables\n    # defined in this function, and the losses outside of this\n    # function only propagate back to variables outside of this\n    # function.  Assuming some kind of adaptive learning algorithm,\n    # it should not matter how much we scale the losses in this function.\n    # Still we scale them down a lot so that they should not show up\n    # much in the overall loss for the model.\n    extra_loss_multiplier = 1e-3\n    extra_loss = q_loss + m_loss + q_balance_loss + m_balance_loss\n    extra_loss *= extra_loss_multiplier\n\n    # Show a bunch of summaries.\n    if common_layers.should_generate_summaries() and make_image_summary:\n      tf.summary.histogram(\"q_group_size\", q_group_size)\n      tf.summary.histogram(\"m_group_size\", m_group_size)\n      tf.summary.scalar(\"q_loss\", q_loss)\n      tf.summary.scalar(\"m_loss\", m_loss)\n      tf.summary.scalar(\"q_balance_loss\", q_balance_loss)\n      tf.summary.scalar(\"m_balance_loss\", m_balance_loss)\n      tf.summary.histogram(\"m_pred_used\", m_pred_used)\n      tf.summary.histogram(\"m_total\", m_total)\n      tf.summary.histogram(\"q_pred_used\", q_pred_used)\n      tf.summary.histogram(\"q_total\", q_total)\n      if make_image_summary:\n        # image summaries are expensive.\n        # So we restrict them to head_num<4, query_position<512, batch_index=0.\n        trunc_heads = min(4, num_heads)\n        trunc_length_q = tf.minimum(length_q, 512)\n        # We recompute the attention for the first example, in an inefficient\n        # way - masking.  This lets us show pretty pictures.\n        # [trunc_heads, length_q, group]\n        q_gates_trunc = q_gates[:trunc_heads, :trunc_length_q, :]\n        # [trunc_heads, length_kv, group]\n        m_gates_trunc = m_gates[:trunc_heads, :, :]\n        grouping_mask = tf.matmul(\n            q_gates_trunc, m_gates_trunc, transpose_b=True)\n        q_trunc = q[:trunc_heads, :trunc_length_q, :]\n        k_trunc = kv[:trunc_heads, :, :depth_qk]\n        logits_trunc = tf.matmul(q_trunc, k_trunc, transpose_b=True)\n        if mask_right:\n          band = common_layers.ones_matrix_band_part(trunc_length_q, length_kv,\n                                                     -1, 0)\n          trunc_bias = tf.expand_dims((1.0 - band) * -1e9, 0)\n          logits_trunc += trunc_bias\n        att_trunc = tf.nn.softmax(logits_trunc)\n        mask_coverage = tf.reduce_sum(grouping_mask * att_trunc) / (\n            tf.to_float(trunc_length_q) * trunc_heads)\n        tf.summary.scalar(\"coverage\", mask_coverage)\n        att_trunc_hdr = tf.pow(att_trunc, 0.2)  # for high-dynamic-range\n        mask_channel = grouping_mask * tf.maximum(att_trunc_hdr, 0.3)\n        image = tf.stack([att_trunc_hdr, mask_channel, mask_channel], axis=3)\n        tf.summary.image(\"att\", image, max_outputs=trunc_heads)\n        # show one group for each head.\n        att_per_group = tf.expand_dims(weights[:trunc_heads, 0, :, :], -1)\n        tf.summary.image(\n            \"att_per_group_%d\",\n            tf.pow(att_per_group, 0.2),\n            max_outputs=trunc_heads)\n    return o, extra_loss\n\n\ndef harden_attention_weights(weights, k, gumbel_noise_weight):\n  \"\"\"Make attention weights non-0 only on the top k ones.\"\"\"\n  if gumbel_noise_weight > 0.:\n    gumbel_noise = -tf.log(-tf.log(tf.random_uniform(tf.shape(weights),\n                                                     minval=1e-5,\n                                                     maxval=1 - 1e-5)))\n    weights += gumbel_noise * gumbel_noise_weight\n\n  # Subtract the top-kth weight and zero-out all lower ones.\n  # Note that currently in case of numerical ties it will retain more\n  # than k elements. In the future, we may want to avoid this.\n  weights -= common_layers.top_kth_iterative(weights, k)\n  weights = tf.nn.relu(weights)\n  # Re-normalize the weights.\n  weights_sum = tf.reduce_sum(weights, axis=-1, keep_dims=True)\n  weights_sum = tf.maximum(weights_sum, 1e-6)  # Avoid division by 0.\n  weights /= weights_sum\n  return weights\n\n\ndef dot_product_attention(q,\n                          k,\n                          v,\n                          bias,\n                          dropout_rate=0.0,\n                          image_shapes=None,\n                          name=None,\n                          make_image_summary=True,\n                          save_weights_to=None,\n                          dropout_broadcast_dims=None,\n                          activation_dtype=None,\n                          weight_dtype=None,\n                          hard_attention_k=0,\n                          gumbel_noise_weight=0.0):\n  \"\"\"Dot-product attention.\n\n  Args:\n    q: Tensor with shape [..., length_q, depth_k].\n    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must\n      match with q.\n    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must\n      match with q.\n    bias: bias Tensor (see attention_bias())\n    dropout_rate: a float.\n    image_shapes: optional tuple of integer scalars.\n      see comments for attention_image_summary()\n    name: an optional string\n    make_image_summary: True if you want an image summary.\n    save_weights_to: an optional dictionary to capture attention weights\n      for visualization; the weights tensor will be appended there under\n      a string key created from the variable scope (including name).\n    dropout_broadcast_dims: an optional list of integers less than rank of q.\n      Specifies in which dimensions to broadcast the dropout decisions.\n    activation_dtype: Used to define function activation dtype when using\n      mixed precision.\n    weight_dtype: The dtype weights are stored in when using mixed precision\n    hard_attention_k: integer, if > 0 triggers hard attention (picking top-k)\n    gumbel_noise_weight: if > 0, apply Gumbel noise with weight\n      `gumbel_noise_weight` before picking top-k. This is a no op if\n      hard_attention_k <= 0.\n\n  Returns:\n    Tensor with shape [..., length_q, depth_v].\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"dot_product_attention\", values=[q, k, v]) as scope:\n    logits = tf.matmul(q, k, transpose_b=True)  # [..., length_q, length_kv]\n    if bias is not None:\n      bias = common_layers.cast_like(bias, logits)\n      logits += bias\n    # If logits are fp16, upcast before softmax\n    logits = maybe_upcast(logits, activation_dtype, weight_dtype)\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    if hard_attention_k > 0:\n      weights = harden_attention_weights(weights, hard_attention_k,\n                                         gumbel_noise_weight)\n    weights = common_layers.cast_like(weights, q)\n    if save_weights_to is not None:\n      save_weights_to[scope.name] = weights\n      save_weights_to[scope.name + \"/logits\"] = logits\n    # Drop out attention links for each head.\n    weights = common_layers.dropout_with_broadcast_dims(\n        weights, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n    if common_layers.should_generate_summaries() and make_image_summary:\n      attention_image_summary(weights, image_shapes)\n    return tf.matmul(weights, v)\n\n\ndef _generate_relative_positions_matrix(length_q, length_k,\n                                        max_relative_position,\n                                        cache=False):\n  \"\"\"Generates matrix of relative positions between inputs.\"\"\"\n  if not cache:\n    if length_q == length_k:\n      range_vec_q = range_vec_k = tf.range(length_q)\n    else:\n      range_vec_k = tf.range(length_k)\n      range_vec_q = range_vec_k[-length_q:]\n    distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n  else:\n    distance_mat = tf.expand_dims(tf.range(-length_k+1, 1, 1), 0)\n  distance_mat_clipped = tf.clip_by_value(distance_mat, -max_relative_position,\n                                          max_relative_position)\n  # Shift values to be >= 0. Each integer still uniquely identifies a relative\n  # position difference.\n  final_mat = distance_mat_clipped + max_relative_position\n  return final_mat\n\n\ndef _generate_relative_positions_embeddings(length_q, length_k, depth,\n                                            max_relative_position, name,\n                                            cache=False):\n  \"\"\"Generates tensor of size [1 if cache else length_q, length_k, depth].\"\"\"\n  with tf.variable_scope(name):\n    relative_positions_matrix = _generate_relative_positions_matrix(\n        length_q, length_k, max_relative_position, cache=cache)\n    vocab_size = max_relative_position * 2 + 1\n    # Generates embedding for each relative position of dimension depth.\n    embeddings_table = tf.get_variable(\"embeddings\", [vocab_size, depth])\n    embeddings = tf.gather(embeddings_table, relative_positions_matrix)\n    return embeddings\n\n\ndef _relative_attention_inner(x, y, z, transpose):\n  \"\"\"Relative position-aware dot-product attention inner calculation.\n\n  This batches matrix multiply calculations to avoid unnecessary broadcasting.\n\n  Args:\n    x: Tensor with shape [batch_size, heads, length or 1, length or depth].\n    y: Tensor with shape [batch_size, heads, length or 1, depth].\n    z: Tensor with shape [length or 1, length, depth].\n    transpose: Whether to transpose inner matrices of y and z. Should be true if\n        last dimension of x is depth, not length.\n\n  Returns:\n    A Tensor with shape [batch_size, heads, length, length or depth].\n  \"\"\"\n  batch_size = tf.shape(x)[0]\n  heads = x.get_shape().as_list()[1]\n  length = tf.shape(x)[2]\n\n  # xy_matmul is [batch_size, heads, length or 1, length or depth]\n  xy_matmul = tf.matmul(x, y, transpose_b=transpose)\n  # x_t is [length or 1, batch_size, heads, length or depth]\n  x_t = tf.transpose(x, [2, 0, 1, 3])\n  # x_t_r is [length or 1, batch_size * heads, length or depth]\n  x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])\n  # x_tz_matmul is [length or 1, batch_size * heads, length or depth]\n  x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose)\n  # x_tz_matmul_r is [length or 1, batch_size, heads, length or depth]\n  x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])\n  # x_tz_matmul_r_t is [batch_size, heads, length or 1, length or depth]\n  x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1, 2, 0, 3])\n  return xy_matmul + x_tz_matmul_r_t\n\n\ndef dot_product_attention_relative(q,\n                                   k,\n                                   v,\n                                   bias,\n                                   max_relative_position,\n                                   dropout_rate=0.0,\n                                   image_shapes=None,\n                                   save_weights_to=None,\n                                   name=None,\n                                   make_image_summary=True,\n                                   cache=False,\n                                   allow_memory=False,\n                                   hard_attention_k=0,\n                                   gumbel_noise_weight=0.0):\n  \"\"\"Calculate relative position-aware dot-product self-attention.\n\n  The attention calculation is augmented with learned representations for the\n  relative position between each element in q and each element in k and v.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth].\n    k: a Tensor with shape [batch, heads, length, depth].\n    v: a Tensor with shape [batch, heads, length, depth].\n    bias: bias Tensor.\n    max_relative_position: an integer specifying the maximum distance between\n        inputs that unique position embeddings should be learned for.\n    dropout_rate: a floating point number.\n    image_shapes: optional tuple of integer scalars.\n    save_weights_to: an optional dictionary to capture attention weights\n      for visualization; the weights tensor will be appended there under\n      a string key created from the variable scope (including name).\n    name: an optional string.\n    make_image_summary: Whether to make an attention image summary.\n    cache: whether use cache mode\n    allow_memory: whether to assume that recurrent memory is in use. If True,\n      the length dimension of k/v/bias may be longer than the queries, and it is\n      assumed that the extra memory entries precede the non-memory entries.\n    hard_attention_k: integer, if > 0 triggers hard attention (picking top-k)\n    gumbel_noise_weight: if > 0, apply Gumbel noise with weight\n      `gumbel_noise_weight` before picking top-k. This is a no op if\n      hard_attention_k <= 0.\n\n  Returns:\n    A Tensor.\n\n  Raises:\n    ValueError: if max_relative_position is not > 0.\n  \"\"\"\n  if not max_relative_position:\n    raise ValueError(\"Max relative position (%s) should be > 0 when using \"\n                     \"relative self attention.\" % (max_relative_position))\n  with tf.variable_scope(\n      name, default_name=\"dot_product_attention_relative\",\n      values=[q, k, v]) as scope:\n\n    # This calculation only works for self attention.\n    # q, k and v must therefore have the same shape, unless memory is enabled.\n    if not cache and not allow_memory:\n      q.get_shape().assert_is_compatible_with(k.get_shape())\n      q.get_shape().assert_is_compatible_with(v.get_shape())\n\n    # Use separate embeddings suitable for keys and values.\n    depth = k.get_shape().as_list()[3]\n    length_k = common_layers.shape_list(k)[2]\n    length_q = common_layers.shape_list(q)[2] if allow_memory else length_k\n    relations_keys = _generate_relative_positions_embeddings(\n        length_q, length_k, depth, max_relative_position,\n        \"relative_positions_keys\", cache=cache)\n    relations_values = _generate_relative_positions_embeddings(\n        length_q, length_k, depth, max_relative_position,\n        \"relative_positions_values\", cache=cache)\n\n    # Compute self attention considering the relative position embeddings.\n    logits = _relative_attention_inner(q, k, relations_keys, True)\n    if bias is not None:\n      logits += bias\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    if hard_attention_k > 0:\n      weights = harden_attention_weights(weights, hard_attention_k,\n                                         gumbel_noise_weight)\n    if save_weights_to is not None:\n      save_weights_to[scope.name] = weights\n      save_weights_to[scope.name + \"/logits\"] = logits\n    weights = tf.nn.dropout(weights, 1.0 - dropout_rate)\n    if (not tf.get_variable_scope().reuse and\n        common_layers.should_generate_summaries() and\n        make_image_summary):\n      attention_image_summary(weights, image_shapes)\n    return _relative_attention_inner(weights, v, relations_values, False)\n\n\ndef _relative_position_to_absolute_position_masked(x):\n  \"\"\"Helper to dot_product_self_attention_relative_v2.\n\n  Rearrange an attention logits or weights Tensor.\n\n  The dimensions of the input represent:\n  [batch, heads, query_position, memory_position - query_position + length - 1]\n\n  The dimensions of the output represent:\n  [batch, heads, query_position, memory_position]\n\n  Only works with masked_attention.  Undefined behavior for regions of the\n  input where memory_position > query_position.\n\n  Args:\n    x: a Tensor with shape [batch, heads, length, length]\n\n  Returns:\n    a Tensor with shape [batch, heads, length, length]\n  \"\"\"\n  batch, heads, length, _ = common_layers.shape_list(x)\n  x = tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 0]])\n  x = tf.reshape(x, [batch, heads, 1 + length, length])\n  x = tf.slice(x, [0, 0, 1, 0], [-1, -1, -1, -1])\n  return x\n\n\ndef _absolute_position_to_relative_position_masked(x):\n  \"\"\"Helper to dot_product_self_attention_relative_v2.\n\n  Rearrange an attention logits or weights Tensor.\n\n  The dimensions of the input represent:\n  [batch, heads, query_position, memory_position]\n\n  The dimensions of the output represent:\n  [batch, heads, query_position, memory_position - query_position + length - 1]\n\n  Only works with masked_attention.  Undefined behavior for regions of the\n  input where memory_position > query_position.\n\n  Args:\n    x: a Tensor with shape [batch, heads, length, length]\n\n  Returns:\n    a Tensor with shape [batch, heads, length, length]\n  \"\"\"\n  batch, heads, length, _ = common_layers.shape_list(x)\n  x = tf.pad(x, [[0, 0], [0, 0], [1, 0], [0, 0]])\n  x = tf.reshape(x, [batch, heads, length, length + 1])\n  x = tf.slice(x, [0, 0, 0, 1], [batch, heads, length, length])\n  return x\n\n\ndef get_relative_embeddings_left(max_relative_position, length, depth,\n                                 num_heads, heads_share_relative_embedding,\n                                 name):\n  \"\"\"Instantiate or retrieve relative embeddings, sliced according to length.\n\n  Use for masked case where the relative attention is only looking left.\n\n  Args:\n    max_relative_position: an Integer for the number of entries in the relative\n      embedding, which corresponds to the max relative distance that is\n      considered.\n    length: an Integer, specifies the length of the input sequence for which\n      this relative embedding is retrieved for.\n    depth: an Integer, specifies the depth for relative embeddings.\n    num_heads: an Integer, specifies the number of heads.\n    heads_share_relative_embedding: a Boolean specifying if the relative\n      embedding is shared across heads.\n    name: a string giving the name of the embedding variables.\n\n  Returns:\n    a Tensor with shape [length, depth]\n  \"\"\"\n  initializer_stddev = depth**-0.5\n  if heads_share_relative_embedding:\n    embedding_shape = (max_relative_position, depth)\n  else:\n    embedding_shape = (num_heads, max_relative_position, depth)\n  relative_embeddings = tf.get_variable(\n      name=name, shape=embedding_shape,\n      initializer=tf.random_normal_initializer(stddev=initializer_stddev))\n  # Pad first before slice to avoid using tf.cond.\n  pad_length = tf.maximum(length - max_relative_position, 0)\n  start_slice_position = tf.maximum(max_relative_position - length, 0)\n  if heads_share_relative_embedding:\n    padded_relative_embeddings = tf.pad(\n        relative_embeddings,\n        [[pad_length, 0], [0, 0]])\n    used_relative_embeddings = tf.slice(\n        padded_relative_embeddings,\n        [start_slice_position, 0], [length, -1])\n  else:\n    padded_relative_embeddings = tf.pad(\n        relative_embeddings,\n        [[0, 0], [pad_length, 0], [0, 0]])\n    used_relative_embeddings = tf.slice(\n        padded_relative_embeddings,\n        [0, start_slice_position, 0], [-1, length, -1])\n  return used_relative_embeddings\n\n\ndef dot_product_self_attention_relative_v2(q,\n                                           k,\n                                           v,\n                                           bias,\n                                           max_relative_position=None,\n                                           dropout_rate=0.0,\n                                           image_shapes=None,\n                                           save_weights_to=None,\n                                           name=None,\n                                           make_image_summary=True,\n                                           dropout_broadcast_dims=None,\n                                           heads_share_relative_embedding=False,\n                                           add_relative_to_values=False):\n  \"\"\"Calculate relative position-aware dot-product self-attention.\n\n  Only works for masked self-attention (no looking forward).\n\n  The attention calculation is augmented with learned representations for the\n  relative position between each element in q and each element in k and v.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth].\n    k: a Tensor with shape [batch, heads, length, depth].\n    v: a Tensor with shape [batch, heads, length, depth].\n    bias: bias Tensor.\n    max_relative_position: an integer indicating the maximum relative distance\n      to look back - changing this invalidates checkpoints\n    dropout_rate: a floating point number.\n    image_shapes: optional tuple of integer scalars.\n    save_weights_to: an optional dictionary to capture attention weights\n      for visualization; the weights tensor will be appended there under\n      a string key created from the variable scope (including name).\n    name: an optional string.\n    make_image_summary: Whether to make an attention image summary.\n    dropout_broadcast_dims:  an optional list of integers less than 4\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n    heads_share_relative_embedding: a boolean indicating wheather to share\n      relative embeddings between attention heads.\n    add_relative_to_values: a boolean for whether to add relative component to\n      values.\n\n  Returns:\n    A Tensor.\n\n  Raises:\n    ValueError: if max_relative_position is not > 0.\n  \"\"\"\n  if not max_relative_position:\n    raise ValueError(\"Max relative position (%s) should be > 0 when using \"\n                     \"relative self attention.\" % (max_relative_position))\n  with tf.variable_scope(\n      name,\n      default_name=\"dot_product_self_attention_relative_v2\",\n      values=[q, k, v]) as scope:\n\n    # This calculation only works for self attention.\n    # q, k and v must therefore have the same shape.\n    # (Except v can have different depth.)\n    q.get_shape().assert_is_compatible_with(k.get_shape())\n    q.get_shape()[:-1].assert_is_compatible_with(v.get_shape()[:-1])\n\n    # Use separate embeddings suitable for keys and values.\n    _, num_heads, length, depth_k = common_layers.shape_list(k)\n\n    # [batch, num_heads, query_length, memory_length]\n    logits = tf.matmul(q, k, transpose_b=True)\n    key_relative_embeddings = get_relative_embeddings_left(\n        max_relative_position, length, depth_k, num_heads,\n        heads_share_relative_embedding, \"key_relative_embeddings\")\n\n    rel_logits = matmul_with_relative_keys(q, key_relative_embeddings,\n                                           heads_share_relative_embedding)\n    rel_logits = _relative_position_to_absolute_position_masked(rel_logits)\n    logits += rel_logits\n    if bias is not None:\n      logits += bias\n\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    if save_weights_to is not None:\n      save_weights_to[scope.name] = weights\n      save_weights_to[scope.name + \"/logits\"] = logits\n    # Dropping out the attention links for each of the heads.\n    weights = common_layers.dropout_with_broadcast_dims(\n        weights, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n    if common_layers.should_generate_summaries() and make_image_summary:\n      attention_image_summary(weights, image_shapes)\n    output = tf.matmul(weights, v)\n    if add_relative_to_values:\n      # [batch, num_heads, query_length, memory_length]\n      relative_weights = _absolute_position_to_relative_position_masked(weights)\n      depth_v = common_layers.shape_list(v)[3]\n      value_relative_embeddings = get_relative_embeddings_left(\n          max_relative_position, length, depth_v, num_heads,\n          heads_share_relative_embedding, \"value_relative_embeddings\")\n      output += matmul_with_relative_values(\n          relative_weights, value_relative_embeddings,\n          heads_share_relative_embedding)\n    return output\n\n\ndef _absolute_position_to_relative_position_unmasked(x):\n  \"\"\"Helper function for dot_product_unmasked_self_attention_relative_v2.\n\n  Rearrange an attention logits or weights Tensor.\n\n  The dimensions of the input represent:\n  [batch, heads, query_position, memory_position]\n\n  The dimensions of the output represent:\n  [batch, heads, query_position, memory_position - query_position + length - 1]\n\n  Only works with unmasked_attention.\n\n  Args:\n    x: a Tensor with shape [batch, heads, length, length]\n\n  Returns:\n    a Tensor with shape [batch, heads, length, 2*length-1]\n  \"\"\"\n  batch, heads, length, _ = common_layers.shape_list(x)\n  # padd along column\n  x = tf.pad(x, [[0, 0], [0, 0], [0, 0], [0, length-1]])\n  x_flat = tf.reshape(x, [batch, heads, length**2 + length*(length -1)])\n  # add 0's in the beginning that will skew the elements after reshape\n  x_flat = tf.pad(x_flat, [[0, 0], [0, 0], [length, 0]])\n  x = tf.reshape(x_flat, [batch, heads, length, 2*length])\n  x = tf.slice(x, [0, 0, 0, 1], [batch, heads, length,\n                                 2*length -1])\n  return x\n\n\ndef get_relative_embeddings_left_right(max_relative_position, length, depth,\n                                       num_heads,\n                                       heads_share_relative_embedding,\n                                       name):\n  \"\"\"Instantiate or retrieve relative embeddings, sliced according to length.\n\n  Use for unmasked case where the relative attention looks both left and right.\n\n  Args:\n    max_relative_position: an Integer for the number of entries in the relative\n      embedding, which corresponds to the max relative distance that is\n      considered.\n    length: an Integer, specifies the length of the input sequence for which\n      this relative embedding is retrieved for.\n    depth: an Integer, specifies the depth for relative embeddings.\n    num_heads: an Integer, specifies the number of heads.\n    heads_share_relative_embedding: a Boolean specifying if the relative\n      embedding is shared across heads.\n    name: a string giving the name of the embedding variables.\n\n  Returns:\n    a Tensor with shape [length, depth]\n  \"\"\"\n  initializer_stddev = depth**-0.5\n  max_relative_position_unmasked = 2 * max_relative_position - 1\n  if heads_share_relative_embedding:\n    embedding_shape = (max_relative_position_unmasked, depth)\n  else:\n    embedding_shape = (num_heads, max_relative_position_unmasked, depth)\n  relative_embeddings = tf.get_variable(\n      name=name, shape=embedding_shape,\n      initializer=tf.random_normal_initializer(stddev=initializer_stddev))\n  # Pad first before slice to avoid using tf.cond.\n  pad_length = tf.maximum(length - max_relative_position, 0)\n  slice_start_position = tf.maximum(max_relative_position-length, 0)\n  if heads_share_relative_embedding:\n    padded_relative_embeddings = tf.pad(\n        relative_embeddings,\n        [[pad_length, pad_length], [0, 0]])\n    used_relative_embeddings = tf.slice(\n        padded_relative_embeddings,\n        [slice_start_position, 0], [2 * length - 1, -1])\n  else:\n    padded_relative_embeddings = tf.pad(\n        relative_embeddings,\n        [[0, 0], [pad_length, pad_length], [0, 0]])\n    used_relative_embeddings = tf.slice(\n        padded_relative_embeddings,\n        [0, slice_start_position, 0], [-1, 2 * length - 1, -1])\n  return used_relative_embeddings\n\n\ndef dot_product_unmasked_self_attention_relative_v2(\n    q, k, v, bias, max_relative_position=None, dropout_rate=0.0,\n    image_shapes=None, save_weights_to=None, name=None, make_image_summary=True,\n    dropout_broadcast_dims=None, heads_share_relative_embedding=False,\n    add_relative_to_values=False):\n  \"\"\"Calculate relative position-aware dot-product self-attention.\n\n  The attention calculation is augmented with learned representations for the\n  relative position between each element in q and each element in k and v.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth].\n    k: a Tensor with shape [batch, heads, length, depth].\n    v: a Tensor with shape [batch, heads, length, depth].\n    bias: bias Tensor.\n    max_relative_position: an integer the max relative embedding considered.\n      Changing this invalidates checkpoints.\n    dropout_rate: a floating point number.\n    image_shapes: optional tuple of integer scalars.\n    save_weights_to: an optional dictionary to capture attention weights\n      for visualization; the weights tensor will be appended there under\n      a string key created from the variable scope (including name).\n    name: an optional string.\n    make_image_summary: Whether to make an attention image summary.\n    dropout_broadcast_dims:  an optional list of integers less than 4\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n    heads_share_relative_embedding: a boolean indicating wheather to share\n      relative embeddings between attention heads.\n    add_relative_to_values: a boolean for whether to add relative component to\n      values.\n\n  Returns:\n    A Tensor.\n\n  Raises:\n    ValueError: if max_relative_position is not > 0.\n  \"\"\"\n  if not max_relative_position:\n    raise ValueError(\"Max relative position (%s) should be > 0 when using \"\n                     \"relative self attention.\" % (max_relative_position))\n\n  with tf.variable_scope(\n      name,\n      default_name=\"dot_product_unmasked_self_attention_relative_v2\",\n      values=[q, k, v]) as scope:\n\n    # This calculation only works for self attention.\n    # q, k and v must therefore have the same shape.\n    q.get_shape().assert_is_compatible_with(k.get_shape())\n    q.get_shape().assert_is_compatible_with(v.get_shape())\n\n    # [batch, num_heads, query_length, memory_length]\n    logits = tf.matmul(q, k, transpose_b=True)\n\n    length = common_layers.shape_list(q)[2]\n    k_shape = common_layers.shape_list(k)\n    num_heads = k_shape[1]\n    depth_k = k_shape[-1]\n\n    key_relative_embeddings = get_relative_embeddings_left_right(\n        max_relative_position, length, depth_k, num_heads,\n        heads_share_relative_embedding,\n        \"key_relative_embeddings\")\n    unmasked_rel_logits = matmul_with_relative_keys(\n        q, key_relative_embeddings, heads_share_relative_embedding)\n    unmasked_rel_logits = _relative_position_to_absolute_position_unmasked(\n        unmasked_rel_logits)\n    logits += unmasked_rel_logits\n\n    if bias is not None:\n      logits += bias\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    if save_weights_to is not None:\n      save_weights_to[scope.name] = weights\n      save_weights_to[scope.name + \"/logits\"] = logits\n    # dropping out the attention links for each of the heads\n    weights = common_layers.dropout_with_broadcast_dims(\n        weights, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n    # relative_weights.set_shape([None, None, None, max_length])\n    if common_layers.should_generate_summaries() and make_image_summary:\n      attention_image_summary(weights, image_shapes)\n    ret = tf.matmul(weights, v)\n    if add_relative_to_values:\n      # Adds the contribution of the weighted relative embeddings to the values.\n      # [batch, num_heads, query_length, 2*memory_length-1]\n      relative_weights = _absolute_position_to_relative_position_unmasked(\n          weights)\n      depth_v = common_layers.shape_list(v)[3]\n      value_relative_embeddings = get_relative_embeddings_left_right(\n          max_relative_position, length, depth_v, num_heads,\n          heads_share_relative_embedding, \"value_relative_embeddings\")\n      ret += matmul_with_relative_values(\n          relative_weights, value_relative_embeddings,\n          heads_share_relative_embedding)\n    return ret\n\n\ndef _matmul_with_relative_keys_2d(x, y, heads_share_relative_embedding):\n  \"\"\"Helper function for dot_product_unmasked_self_attention_relative_2d.\"\"\"\n  if heads_share_relative_embedding:\n    ret = tf.einsum(\"bhxyd,md->bhxym\", x, y)\n  else:\n    ret = tf.einsum(\"bhxyd,hmd->bhxym\", x, y)\n  return ret\n\n\ndef dot_product_unmasked_self_attention_relative_2d(\n    q, k, v, bias, max_relative_position=None, dropout_rate=0.0,\n    image_shapes=None, name=None, make_image_summary=True,\n    dropout_broadcast_dims=None, heads_share_relative_embedding=False,\n    add_relative_to_values=False):\n  \"\"\"Calculate relative position unmasked dot-product self-attention 2d.\n\n\n  The attention calculation is augmented with learned representations for the\n  relative position between each element in q and each element in k and v in\n  height and width dimensions. for query index (i,j) and key index (l, m),\n  the logit is q_i k_j^T + q_i rh_{l-i}^T + q_i rw_{m-j}^T, where rh and ry are\n  the set of relative embeddings in height and width spatial dimensions,\n  respectively.\n\n  Args:\n    q: a Tensor with shape [batch, heads, height, width, depth].\n    k: a Tensor with shape [batch, heads, height, width, depth].\n    v: a Tensor with shape [batch, heads, height, width, depth].\n    bias: bias Tensor.\n    max_relative_position: an integer the max relative embedding considered.\n      Changing this invalidates checkpoints.\n    dropout_rate: a floating point number.\n    image_shapes: optional tuple of integer scalars.\n    name: an optional string.\n    make_image_summary: Whether to make an attention image summary.\n    dropout_broadcast_dims:  an optional list of integers less than 4\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n    heads_share_relative_embedding: a boolean indicating wheather to share\n      relative embeddings between attention heads.\n    add_relative_to_values: a boolean for adding relative embeddings to values.\n\n  Returns:\n    [batch, heads, height, width, depth] tensor, the output of attention.\n    height_key_relative_embeddings: a 3d or 2d tensor, depending on head sharing\n      settings, which are the relative embeddings for height.\n    width_key_relative_embeddings: a 3d or 2d tensor, depending on head sharing\n      settings, which are the relative embeddings for width.\n\n  Raises:\n    ValueError: if max_relative_position is not > 0.\n  \"\"\"\n  if not max_relative_position:\n    raise ValueError(\"Max relative position (%s) should be > 0 when using \"\n                     \"relative self attention.\" % (max_relative_position))\n\n  if add_relative_to_values:\n    raise ValueError(\"Adding relative embeddings to values is not implemented\")\n\n  with tf.variable_scope(\n      name,\n      default_name=\"dot_product_self_attention_relative_v2\",\n      values=[q, k, v]):\n\n    # This calculation only works for self attention.\n    # q, k and v must therefore have the same shape.\n    q.get_shape().assert_is_compatible_with(k.get_shape())\n    q.get_shape()[:-1].assert_is_compatible_with(v.get_shape()[:-1])\n\n    (height, width) = (common_layers.shape_list(q)[2],\n                       common_layers.shape_list(q)[3])\n    k_shape = common_layers.shape_list(k)\n    num_heads = k_shape[1]\n    depth_k = k_shape[-1]\n    depth_v = common_layers.shape_list(v)[-1]\n    # flatten height width\n    flatten_hw = lambda x, d: tf.reshape(x, [-1, num_heads, height*width, d])\n    # [batch, num_heads, query_length, memory_length]\n    logits = tf.matmul(flatten_hw(q, depth_k), flatten_hw(k, depth_k),\n                       transpose_b=True)\n\n    def _compute_2d_relative_logits(\n        query, key_relative_embeddings, height, width,\n        heads_share_relative_embedding, transpose_mask):\n      \"\"\"compute relative logits.\"\"\"\n      unmasked_rel_logits = _matmul_with_relative_keys_2d(\n          query, key_relative_embeddings, heads_share_relative_embedding)\n      # collapse height and heads\n      unmasked_rel_logits = tf.reshape(unmasked_rel_logits,\n                                       [-1, num_heads*height, width,\n                                        2*width-1])\n      unmasked_rel_logits = (\n          _relative_position_to_absolute_position_unmasked(\n              unmasked_rel_logits))\n      # shape it back for tiling\n      unmasked_rel_logits = tf.reshape(\n          unmasked_rel_logits, [-1, num_heads, height, width, width])\n      # tiling it height times\n      unmasked_rel_logits = tf.expand_dims(\n          unmasked_rel_logits, axis=3)\n      unmasked_rel_logits = tf.tile(unmasked_rel_logits,\n                                    [1, 1, 1, height, 1, 1])\n      # bringing it to the right shape for adding to the logits.\n      unmasked_rel_logits = tf.transpose(unmasked_rel_logits, transpose_mask)\n      unmasked_rel_logits = tf.reshape(unmasked_rel_logits,\n                                       [-1, num_heads, height*width,\n                                        height*width])\n      return unmasked_rel_logits\n\n    # Relative logits in width dimension first.\n    width_key_relative_embeddings = get_relative_embeddings_left_right(\n        max_relative_position, width, depth_k, num_heads,\n        heads_share_relative_embedding,\n        \"width_key_relative_embeddings\")\n    # [batch, heads, height, 2*width-1, 2*width-1]\n    width_unmasked_rel_logits = _compute_2d_relative_logits(\n        q, width_key_relative_embeddings, height, width,\n        heads_share_relative_embedding, [0, 1, 2, 4, 3, 5])\n    logits += width_unmasked_rel_logits\n    # Relative logits in height dimension next. For ease, we transpose\n    # height and width and repeat the above steps, and transpose to eventually\n    # put the logits in their right positions.\n    # [batch, heads, height, 2*height-1, 2*width-1]\n    height_key_relative_embeddings = get_relative_embeddings_left_right(\n        max_relative_position, height, depth_k, num_heads,\n        heads_share_relative_embedding,\n        \"height_key_relative_embeddings\")\n\n    height_unmasked_rel_logits = _compute_2d_relative_logits(\n        tf.transpose(q, [0, 1, 3, 2, 4]),\n        height_key_relative_embeddings,\n        width,\n        height,\n        heads_share_relative_embedding, [0, 1, 4, 2, 5, 3])\n    logits += height_unmasked_rel_logits\n    if bias is not None:\n      logits += bias\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    # dropping out the attention links for each of the heads\n    weights = common_layers.dropout_with_broadcast_dims(\n        weights, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n    if common_layers.should_generate_summaries() and make_image_summary:\n      attention_image_summary(weights, image_shapes)\n    ret = tf.matmul(weights, flatten_hw(v, depth_v))\n    # reshape back the same spatial dimensions as q\n    return (\n        tf.reshape(ret, [-1, num_heads, height, width, depth_v]),\n        height_key_relative_embeddings,\n        width_key_relative_embeddings)\n\n\ndef _split_along_width(x_left_right_blocks):\n  \"\"\"Helper function for local 2d attention.\n\n  Takes a tensor of [batch, heads, num_h_blocks, num_w_blocks,\n  height, width, depth] and returns two tensors which contain every alternate\n  position along the width\n\n\n  Args:\n    x_left_right_blocks: A [batch, num_h_blocks, num_w_blocks,\n                            height, width, depth] tensor\n\n  Returns:\n    x_left_blocks, x_right_blocks: two [batch, num_h_blocks,\n                                        (num_w_blocks-2)/2, height, width,\n                                        depth] tensors\n\n  \"\"\"\n  (_, x_num_h_blocks, x_num_outer_w_blocks, x_memory_flange_h,\n   x_memory_flange_w, depth) = common_layers.shape_list(x_left_right_blocks)\n  x_num_w_blocks = (x_num_outer_w_blocks-1)//2\n  # get it ready for splitting the left and right memory blocks\n  x_left_right_blocks = tf.reshape(x_left_right_blocks,\n                                   [-1,\n                                    x_num_h_blocks,\n                                    x_num_outer_w_blocks//2, 2,\n                                    x_memory_flange_h,\n                                    x_memory_flange_w, depth])\n\n  x_left_blocks, x_right_blocks = tf.split(x_left_right_blocks,\n                                           num_or_size_splits=2, axis=3)\n  x_left_blocks = tf.squeeze(x_left_blocks, axis=3)\n  x_right_blocks = tf.squeeze(x_right_blocks, axis=3)\n  x_left_blocks = tf.slice(x_left_blocks, [0, 0, 0, 0, 0, 0],\n                           [-1, -1, x_num_w_blocks, -1, -1, -1])\n  x_right_blocks = tf.slice(x_right_blocks, [0, 0, 1, 0, 0, 0],\n                            [-1, -1, x_num_w_blocks, -1, -1, -1])\n  return x_left_blocks, x_right_blocks\n\n\ndef _get_left_right_blocks(x):\n  \"\"\"Helper function. Assumes that memory_flange is half of query sizes.\n\n  This function splits the tensor of width 'n' into two halves, where the\n  first half gets the width indices 0, 2, 4.. and the second half gets the\n  width indices 3, 5, ... We also fuse two blocks along the h dimension.\n\n  Args:\n    x: a 6-d tensor.\n\n  Returns:\n    x_left_blocks, x_right_blocks: Two 6-d tensors\n  \"\"\"\n  (_, x_num_outer_h_blocks, x_num_outer_w_blocks, x_memory_flange_h,\n   x_memory_flange_w, depth) = common_layers.shape_list(x)\n  x_left_right_blocks = tf.slice(x,\n                                 [0, 1, 0, 0, 0, 0],\n                                 [-1, x_num_outer_h_blocks-2, -1, -1,\n                                  -1, -1])\n  num_blocks_h = (x_num_outer_h_blocks-2)//2\n  x_left_right_blocks = tf.reshape(x_left_right_blocks,\n                                   [-1,\n                                    num_blocks_h,\n                                    2, x_num_outer_w_blocks,\n                                    x_memory_flange_h,\n                                    x_memory_flange_w, depth])\n  x_left_right_blocks = tf.transpose(x_left_right_blocks,\n                                     [0, 1, 3, 2, 4, 5, 6])\n  x_left_right_blocks = tf.reshape(x_left_right_blocks,\n                                   [-1, num_blocks_h,\n                                    x_num_outer_w_blocks, 2*x_memory_flange_h,\n                                    x_memory_flange_w, depth])\n  # get it ready for splitting the left and right memory blocks\n  x_left_blocks, x_right_blocks = _split_along_width(x_left_right_blocks)\n\n  return x_left_blocks, x_right_blocks\n  # return x_left_right_blocks\n\n\ndef _extract_blocks(x, block_h, block_w):\n  \"\"\"Helper function for local 2d attention.\n\n  Args:\n    x: a [batch, height, width, depth] tensor\n    block_h: An integer. block height\n    block_w: An inteter. block width\n\n  Returns:\n    a [batch, num_heads, height/block_h, width/block_w, depth] tensor\n  \"\"\"\n  (_, height, width, depth) = common_layers.shape_list(x)\n  assert height % block_h == 0\n  assert width % block_w == 0\n  x = tf.reshape(x, [-1, height//block_h, block_h,\n                     width//block_w, block_w, depth])\n  return tf.transpose(x, [0, 1, 3, 2, 4, 5])\n\n\ndef get_2d_local_memory(x, query_shape, memory_flange):\n  \"\"\"Stitches together the local 2d memory blocks.\n\n  Args:\n    x: a [batch, height, width, depth tensor]\n    query_shape: 2-d integer list of query shape\n    memory_flange: 2-d integer list of memory flanges\n\n  Returns:\n    x: A [batch, num_h_blocks, num_w_blocks,\n          query_shape[0]+2*memory_flange[0],query_shape[1]+2*memory_flange[1]]\n          tensor.\n  \"\"\"\n  (_, height, width, depth_x) = common_layers.shape_list(x)\n  x_center_blocks = _extract_blocks(x, query_shape[0], query_shape[1])\n  # add extra padding to x so that we can extract the memory region\n  # around the center\n  paddings = [[0, 0], [memory_flange[0], memory_flange[0]],\n              [memory_flange[1], memory_flange[1]], [0, 0]]\n  padded_x = tf.pad(x, paddings)\n  padded_x.set_shape([None, height+2*memory_flange[0],\n                      width+2*memory_flange[1], depth_x])\n  x_outer_memory_blocks = _extract_blocks(padded_x,\n                                          memory_flange[0], memory_flange[1])\n  # We'll extract left and right memory blocks, top and bottom memory blocks,\n  # and then the corner memory blocks\n\n  # Each of these after  will have shape\n  # [batch, num_h_blocks, num_w_blocks, query_shape[0],\n  # memory_flange[1], depth]\n  x_left_blocks, x_right_blocks = _get_left_right_blocks(\n      x_outer_memory_blocks)\n  t_hw_block = lambda x: tf.transpose(x, [0, 2, 1, 4, 3, 5])\n  # now to get top and bottom blocks, we should just transpose the outer\n  # blocks, call the same function and transpose back to get shape\n  # [batch, num_h_blocks, num_w_blocks, memory_flange[0],\n  # query_shape[1], depth]\n  x_top_center_blocks, x_bottom_center_blocks = (\n      map(t_hw_block, _get_left_right_blocks(\n          t_hw_block(x_outer_memory_blocks))))\n\n  # now to get the corner blocks\n  x_left_corner_blocks, x_right_corner_blocks = _split_along_width(\n      x_outer_memory_blocks)\n  # now to extract top and bottom for both k and v\n  # we need to transpose because _split_along_width separates along\n  # the width\n  # each of these should have shape [batch, num_h_blocks,\n  # num_w_blocks, memory_flange[0], memory_flange[1], depth]\n\n  t_hw = lambda x: tf.transpose(x, [0, 2, 1, 3, 4, 5])\n  x_top_left_corner_blocks, x_bottom_left_corner_blocks = (\n      map(t_hw, _split_along_width(t_hw(x_left_corner_blocks))))\n  x_top_right_corner_blocks, x_bottom_right_corner_blocks = (\n      map(t_hw, _split_along_width(t_hw(x_right_corner_blocks))))\n\n  # The memory is top_left     top_center    top_right\n  #               left_center  middle        right_center\n  #               bottom_left  bottom_center bottom_right\n  # Assembling the above row by row\n  # first [x_top_left, x_top, x_top_right]\n  # to get [batch, num_h_blocks, num_w_blocks, memory_flange[0],\n  # query_shape[1]+2*memory_flange[1], depth]\n  # then [x_left, x_center, x_right]\n  # then [x_bottom_left, x_bottom, x_bottom_right]\n  x_top_memory = tf.concat(\n      [x_top_left_corner_blocks,\n       x_top_center_blocks,\n       x_top_right_corner_blocks], axis=4)\n  x_middle_memory = tf.concat(\n      [x_left_blocks, x_center_blocks, x_right_blocks], axis=4)\n  x_bottom_memory = tf.concat(\n      [x_bottom_left_corner_blocks,\n       x_bottom_center_blocks,\n       x_bottom_right_corner_blocks], axis=4)\n\n  # concat along height\n  x = tf.concat([x_top_memory, x_middle_memory, x_bottom_memory], axis=3)\n  return x\n\n\ndef get_2d_local_memory_v2(x, query_shape, memory_flange):\n  \"\"\"Gathering memory blocks around query blocks. flange is half of query .\n\n    Only works if memory flanges are half of query sizes.\n\n  Args:\n    x: a [batch, height, width, depth tensor]\n    query_shape: 2-d integer list of query shape\n    memory_flange: 2-d integer list of memory flanges\n\n  Returns:\n    x: A [batch, num_h_blocks, num_w_blocks,\n          query_shape[0]+2*memory_flange[0],query_shape[1]+2*memory_flange[1]]\n          tensor.\n  \"\"\"\n  (_, height, width, depth_x) = common_layers.shape_list(x)\n  # add extra padding to x so that we can extract the memory region\n  # around the center\n  paddings = [[0, 0], [memory_flange[0], memory_flange[0]],\n              [memory_flange[1], memory_flange[1]], [0, 0]]\n  padded_x = tf.pad(x, paddings)\n  padded_x.set_shape([None, height+2*memory_flange[0],\n                      width+2*memory_flange[1], depth_x])\n  num_h_memory_blocks = height//query_shape[0] + 1\n  num_w_memory_blocks = width//query_shape[1] + 1\n  x_memory_blocks = _extract_blocks(padded_x,\n                                    query_shape[0], query_shape[1])\n  x_width_blocks = tf.split(x_memory_blocks, num_w_memory_blocks,\n                            2)\n  x_left_width = tf.concat(x_width_blocks[:num_w_memory_blocks - 1], axis=2)\n  x_right_width = tf.concat(x_width_blocks[1:], axis=2)\n  x_memory_blocks = tf.concat([x_left_width, x_right_width], axis=4)\n\n  x_height_blocks = tf.split(x_memory_blocks, num_h_memory_blocks, 1)\n  x_top_height = tf.concat(x_height_blocks[:num_h_memory_blocks - 1], axis=1)\n  x_bottom_height = tf.concat(x_height_blocks[1:], axis=1)\n  x = tf.concat([x_top_height, x_bottom_height], axis=3)\n\n  return x\n\n\ndef dot_product_unmasked_attention_local_2d_tpu(\n    q, k, v, bias, max_relative_position=None, query_shape=(8, 8),\n    dropout_rate=0.0, image_shapes=None, name=None, make_image_summary=False,\n    dropout_broadcast_dims=None):\n  \"\"\"Calculate unmasked dot-product local self-attention 2d on tpu.\n\n  Args:\n    q: a Tensor with shape [batch, heads, height, width, depth].\n    k: a Tensor with shape [batch, heads, height, width, depth].\n    v: a Tensor with shape [batch, heads, height, width, depth].\n    bias: bias Tensor.\n    max_relative_position: an integer the max relative embedding considered.\n      Changing this invalidates checkpoints.\n    query_shape: a two tuple indicating query shape\n    dropout_rate: a floating point number.\n    image_shapes: optional tuple of integer scalars.\n    name: an optional string.\n    make_image_summary: Whether to make an attention image summary.\n    dropout_broadcast_dims:  an optional list of integers less than 4\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n\n  Returns:\n    [batch, heads, height, width, depth] tensor, the output of attention.\n\n  \"\"\"\n  if max_relative_position:\n    raise ValueError(\"Relative local 2d attention not implemented\")\n\n  with tf.variable_scope(\n      name,\n      default_name=\"dot_product_unmasked_attention_local_2d_tpu\",\n      values=[q, k, v]):\n\n    # This calculation only works for self attention.\n    # q, k and v must therefore have the same shape.\n    q.get_shape().assert_is_compatible_with(k.get_shape())\n    q.get_shape().assert_is_compatible_with(v.get_shape())\n    orig_q_shape = common_layers.shape_list(q)\n    # Pad query, key, value to ensure multiple of corresponding lengths.\n    memory_flange = [int(query_shape[0]//2), int(query_shape[1]//2)]\n    q = pad_to_multiple_2d(q, query_shape)\n    k = pad_to_multiple_2d(k, query_shape)\n    v = pad_to_multiple_2d(v, query_shape)\n    q_shape = common_layers.shape_list(q)\n    (height, width) = (q_shape[2],\n                       q_shape[3])\n    _, num_heads, height, width, depth_k = common_layers.shape_list(k)\n    depth_v = common_layers.shape_list(v)[-1]\n    num_h_blocks = height//query_shape[0]\n    num_w_blocks = width//query_shape[1]\n    # Extract center queries, keys, and values\n    q = tf.reshape(q, [-1, height, width, depth_k])\n    queries = _extract_blocks(\n        q, query_shape[0], query_shape[1])\n    k = tf.reshape(k, [-1, height, width, depth_k])\n    keys = get_2d_local_memory_v2(\n        k, query_shape, memory_flange)\n    v = tf.reshape(v, [-1, height, width, depth_v])\n    values = get_2d_local_memory_v2(\n        v, query_shape, memory_flange)\n    memory_h = query_shape[0] + 2*memory_flange[0]\n    memory_w = query_shape[1] + 2*memory_flange[1]\n    queries = tf.reshape(queries, [-1, num_heads, num_h_blocks, num_w_blocks,\n                                   query_shape[0]*query_shape[1], depth_k])\n    keys = tf.reshape(keys, [-1, num_heads, num_h_blocks, num_w_blocks,\n                             memory_h*memory_w, depth_k])\n    values = tf.reshape(values, [-1, num_heads, num_h_blocks, num_w_blocks,\n                                 memory_h*memory_w, depth_v])\n    logits = tf.matmul(queries, keys, transpose_b=True)\n    if bias is not None:\n      logits += bias\n\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    # Dropping out the attention links for each of the heads\n    weights = common_layers.dropout_with_broadcast_dims(\n        weights, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n    if common_layers.should_generate_summaries() and make_image_summary:\n      attention_image_summary(weights, image_shapes)\n    ret = tf.matmul(weights, values)\n    # we need to get it back to shape [batch, heads, height, width]\n    ret = tf.reshape(ret, [-1, num_heads, num_h_blocks, num_w_blocks,\n                           query_shape[0], query_shape[1], depth_v])\n    ret = tf.transpose(ret, [0, 1, 2, 4, 3, 5, 6])\n    ret = tf.reshape(ret, [-1, num_heads, num_h_blocks*query_shape[0],\n                           num_w_blocks*query_shape[1], depth_v])\n    # slice if padding was introduced\n    ret = tf.slice(ret, [0, 0, 0, 0, 0], [-1, -1, orig_q_shape[2],\n                                          orig_q_shape[3], -1])\n    return ret\n\n\ndef dot_product_unmasked_attention_local_2d_tpu_simple(\n    x, bias, total_key_depth, total_value_depth, num_heads,\n    query_shape=(8, 8),\n    dropout_rate=0.0, image_shapes=None, make_image_summary=False,\n    dropout_broadcast_dims=None):\n\n  \"\"\"Calculate simple unmasked dot-product local self-attention 2d on tpu.\n\n  The query, key, and value blocks are the same. We do not do a second linear\n  transformation after computing the values\n\n  Args:\n    x: a Tensor with shape [batch, height, width, depth].\n    bias: bias Tensor.\n    total_key_depth: the dimensions of the keys\n    total_value_depth: the dimensions of the values\n    num_heads: number of heads\n    query_shape: a two tuple indicating query shape\n    dropout_rate: a floating point number.\n    image_shapes: optional tuple of integer scalars.\n    make_image_summary: Whether to make an attention image summary.\n    dropout_broadcast_dims:  an optional list of integers less than 4\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n\n  Returns:\n    ret: [batch, height, width, total_value_depth] tensor,\n      the output of attention.\n    q: [batch, height, width, total_key_depth] query tensor\n    k: [batch, height, width, total_key_depth] key tensor\n    v: [batch, height, width, total_value_depth] value tensor\n\n  \"\"\"\n  # This calculation only works for self attention.\n  # q, k and v must therefore have the same shape.\n  orig_x_shape = common_layers.shape_list(x)\n  # Pad query, key, value to ensure multiple of corresponding lengths if\n  # necessary\n  is_padded = False\n  if (orig_x_shape[1]%query_shape[0]) != 0 or (\n      orig_x_shape[2]%query_shape[1]) != 0:\n    x = pad_to_multiple_2d(x, query_shape)\n    is_padded = True\n  _, height, width, depth = common_layers.shape_list(x)\n  assert depth%num_heads == 0\n  num_h_blocks = height//query_shape[0]\n  num_w_blocks = width//query_shape[1]\n  # Extract center queries, keys, and values\n  x_blocks = _extract_blocks(x, query_shape[0], query_shape[1])\n  x_blocks = tf.reshape(x_blocks, [-1, query_shape[0]*query_shape[1], depth])\n  q, k, v = compute_qkv(x_blocks, None, total_key_depth, total_value_depth)\n  hsplit = lambda x: split_heads(x, num_heads)\n  q, k, v = map(hsplit, [q, k, v])\n  logits = tf.matmul(q, k, transpose_b=True)\n  if bias is not None:\n    logits += bias\n  weights = tf.nn.softmax(logits, name=\"attention_weights\")\n  # Dropping out the attention links for each of the heads\n  weights = common_layers.dropout_with_broadcast_dims(\n      weights, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n  if common_layers.should_generate_summaries() and make_image_summary:\n    attention_image_summary(weights, image_shapes)\n  output = tf.matmul(weights, v)\n  output = combine_heads(output)\n  # we need to get it back to shape [batch, height, width]\n  ret = tf.reshape(output, [-1, num_h_blocks, num_w_blocks,\n                            query_shape[0], query_shape[1], total_value_depth])\n\n  ret = tf.transpose(ret, [0, 1, 3, 2, 4, 5])\n  ret = tf.reshape(ret, [-1, num_h_blocks*query_shape[0],\n                         num_w_blocks*query_shape[1], total_value_depth])\n  # slice if padding was introduced\n  if is_padded:\n    ret = tf.slice(ret, [0, 0, 0, 0], [-1, orig_x_shape[1],\n                                       orig_x_shape[2], -1])\n  return ret, q, k, v\n\n\ndef masked_within_block_local_attention_1d(q, k, v, block_length=64, name=None):\n  \"\"\"Attention to the source and a neighborhood to the left within a block.\n\n  The sequence is divided into blocks of length block_length. Attention for a\n  given query position can only see memory positions less than or equal to the\n  query position in the corresponding block.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth_k]\n    k: a Tensor with shape [batch, heads, length, depth_k]\n    v: a Tensor with shape [batch, heads, length, depth_v]\n    block_length: an integer\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, length, depth_v]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"within_local_attention_1d\", values=[q, k, v]):\n    batch, heads, length, depth_k = common_layers.shape_list(q)\n    depth_v = common_layers.shape_list(v)[-1]\n    if isinstance(block_length, tf.Tensor):\n      const = contrib.util().constant_value(block_length)\n      if const is not None:\n        block_length = int(const)\n\n    # Pad query, key, value to ensure multiple of block length.\n    original_length = length\n    padding_size = tf.mod(-length, block_length)\n    length += padding_size\n    padding = [[0, 0], [0, 0], [0, padding_size], [0, 0]]\n    q = tf.pad(q, padding)\n    k = tf.pad(k, padding)\n    v = tf.pad(v, padding)\n\n    # Compute attention for all subsequent query blocks.\n    num_blocks = tf.div(length, block_length)\n    q = tf.reshape(q, [batch, heads, num_blocks, block_length, depth_k])\n    k = tf.reshape(k, [batch, heads, num_blocks, block_length, depth_k])\n    v = tf.reshape(v, [batch, heads, num_blocks, block_length, depth_v])\n    # [batch, heads, num_blocks, block_length, block_length]\n    attention = tf.matmul(q, k, transpose_b=True)\n    attention += tf.reshape(attention_bias_lower_triangle(block_length),\n                            [1, 1, 1, block_length, block_length])\n    attention = tf.nn.softmax(attention)\n    # [batch, heads, num_blocks, block_length, depth_v]\n    output = tf.matmul(attention, v)\n    output = tf.reshape(output, [batch, heads, -1, depth_v])\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0], [-1, -1, original_length, -1])\n    output.set_shape([None if isinstance(dim, tf.Tensor) else dim for dim in\n                      (batch, heads, length, depth_v)])\n    return output\n\n\ndef _relative_position_to_absolute_position_unmasked(x):\n  \"\"\"Converts tensor from relative to aboslute indexing for local attention.\n\n  Args:\n    x: a Tensor of shape [batch (or batch*num_blocks), heads,\n                          length, 2 * length - 1]\n\n  Returns:\n    A Tensor of shape [batch (or batch*num_blocks), heads, length, length]\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  batch = x_shape[0]\n  heads = x_shape[1]\n  length = x_shape[2]\n  # Concat columns of pad to shift from relative to absolute indexing.\n  col_pad = tf.zeros((batch, heads, length, 1))\n  x = tf.concat([x, col_pad], axis=3)\n\n  # Concat extra elements so to add up to shape (len+1, 2*len-1).\n  flat_x = tf.reshape(x, [batch, heads, length * 2 * length])\n  flat_pad = tf.zeros((batch, heads, length-1))\n  flat_x_padded = tf.concat([flat_x, flat_pad], axis=2)\n\n  # Reshape and slice out the padded elements.\n  final_x = tf.reshape(flat_x_padded, [batch, heads, length+1, 2*length-1])\n  final_x = final_x[:, :, :, length-1:]\n  final_x = final_x[:, :, :length, :]\n  return final_x\n\n\ndef masked_local_attention_1d(q,\n                              k,\n                              v,\n                              block_length=128,\n                              make_image_summary=False,\n                              dropout_rate=0.,\n                              name=None):\n  \"\"\"Attention to the source position and a neighborhood to the left of it.\n\n  The sequence is divided into blocks of length block_length. Attention for a\n  given query position can only see memory positions less than or equal to the\n  query position, in the corresponding block and the previous block.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth_k]\n    k: a Tensor with shape [batch, heads, length, depth_k]\n    v: a Tensor with shape [batch, heads, length, depth_v]\n    block_length: an integer\n    make_image_summary: a boolean, whether to make an attention image summary.\n    dropout_rate: Dropout rate for attention dropout\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, length, depth_v]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"local_attention_1d\", values=[q, k, v]):\n    batch, heads, length, depth_k = common_layers.shape_list(q)\n    depth_v = common_layers.shape_list(v)[-1]\n    if isinstance(block_length, tf.Tensor):\n      const = contrib.util().constant_value(block_length)\n      if const is not None:\n        block_length = int(const)\n    # If (length < 2 * block_length), then we use only one block.\n    if isinstance(length, int) and isinstance(block_length, int):\n      block_length = length if length < block_length * 2 else block_length\n    else:\n      block_length = tf.where(\n          tf.less(length, block_length * 2), length, block_length)\n\n    # Pad query, key, value to ensure multiple of block length.\n    original_length = length\n    padding_size = tf.mod(-length, block_length)\n    length += padding_size\n    padding = [[0, 0], [0, 0], [0, padding_size], [0, 0]]\n    q = tf.pad(q, padding)\n    k = tf.pad(k, padding)\n    v = tf.pad(v, padding)\n\n    if isinstance(length, int) and isinstance(block_length, int):\n      num_blocks = length // block_length\n    else:\n      num_blocks = tf.div(length, block_length)\n\n    # Compute attention for the first query block.\n    first_q = tf.slice(q, [0, 0, 0, 0], [-1, -1, block_length, -1])\n    first_k = tf.slice(k, [0, 0, 0, 0], [-1, -1, block_length, -1])\n    first_v = tf.slice(v, [0, 0, 0, 0], [-1, -1, block_length, -1])\n\n    first_output = dot_product_attention(\n        first_q,\n        first_k,\n        first_v,\n        attention_bias_lower_triangle(block_length),\n        dropout_rate=dropout_rate,\n        make_image_summary=make_image_summary,\n        name=\"first_block\")\n\n    # Compute attention for all subsequent query blocks.\n    q = tf.reshape(q, [batch, heads, num_blocks, block_length, depth_k])\n    k = tf.reshape(k, [batch, heads, num_blocks, block_length, depth_k])\n    v = tf.reshape(v, [batch, heads, num_blocks, block_length, depth_v])\n\n    local_k = _make_local_block(k, depth_k, batch, heads, num_blocks,\n                                block_length)\n    local_v = _make_local_block(v, depth_v, batch, heads, num_blocks,\n                                block_length)\n    tail_q = tf.slice(q, [0, 0, 1, 0, 0], [-1, -1, -1, -1, -1])\n    tail_q = tf.reshape(tail_q,\n                        [batch, heads, num_blocks - 1, block_length, depth_k])\n    local_length = common_layers.shape_list(local_k)[3]\n\n    # make sure source_pos <= target_pos\n    good_part = common_layers.ones_matrix_band_part(\n        block_length,\n        local_length,\n        -1,\n        block_length,\n        out_shape=[1, 1, 1, block_length, local_length])\n    bias = (1.0 - good_part) * -1e9\n    # TODO(noam): figure out how to show a summary for the remaining blocks.\n    # The naive way currently causes errors due to empty tensors.\n    # output: [batch, heads, num_blocks-1, block_length, depth_v]\n    tail_output = dot_product_attention(\n        tail_q,\n        local_k,\n        local_v,\n        bias,\n        dropout_rate=dropout_rate,\n        make_image_summary=False,\n        name=\"tail_block\")\n    tail_output = tf.reshape(\n        tail_output, [batch, heads, (num_blocks - 1) * block_length, depth_v])\n    output = tf.concat([first_output, tail_output], axis=2)\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0], [-1, -1, original_length, -1])\n    output = tf.reshape(output, [batch, heads, original_length, depth_v])\n    return output\n\n\ndef _make_local_block(x, depth, batch, heads, num_blocks, block_length):\n  \"\"\"Helper function to create a local version of the keys or values for 1d.\"\"\"\n  prev_block = tf.slice(x, [0, 0, 0, 0, 0],\n                        [-1, -1, num_blocks - 1, -1, -1])\n  cur_block = tf.slice(x, [0, 0, 1, 0, 0], [-1, -1, -1, -1, -1])\n  local_block = tf.concat([prev_block, cur_block], 3)\n  return tf.reshape(local_block,\n                    [batch, heads, num_blocks - 1, block_length * 2, depth])\n\n\ndef masked_relative_local_attention_1d(q,\n                                       k,\n                                       v,\n                                       block_length=128,\n                                       make_image_summary=False,\n                                       dropout_rate=0.,\n                                       heads_share_relative_embedding=False,\n                                       add_relative_to_values=False,\n                                       name=None):\n  \"\"\"Masked local 1d attention with relative positions.\n\n  The sequence is divided into blocks of length block_size.\n  Attention for a given query position can only see memory positions\n  less than or equal to the query position, in the corresponding block\n  and the previous block.\n\n  If mask_right is True, then a target position cannot see greater source\n  positions.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth_k]\n    k: a Tensor with shape [batch, heads, length, depth_k]\n    v: a Tensor with shape [batch, heads, length, depth_v]\n    block_length: an integer\n    make_image_summary: a boolean, whether to make an attention image summary.\n    dropout_rate: Dropout rate for attention dropout\n    heads_share_relative_embedding: a boolean for sharing relative embeddings.\n    add_relative_to_values: a boolean for whether to add relative component to\n        values.\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, length, depth_v]\n\n  Raises:\n    ValueError: wwhen the name for the variable scope is not passed.\n  \"\"\"\n  if not name:\n    raise ValueError(\"Name must be assigned since reuse for variable scope is \"\n                     \"set to tf.AUTO_REUSE, in order to reuse relative \"\n                     \"embeddings of keys and values.\")\n\n  # Reuse flag is set to auto_reuse to reuse relative embeddings of keys and\n  # values across blocks (first and tail blocks).\n  with tf.variable_scope(\n      name, default_name=\"masked_relative_local_attention_1d\",\n      values=[q, k, v], reuse=tf.AUTO_REUSE):\n\n    default_block_length = block_length\n    batch = common_layers.shape_list(q)[0]\n    heads = common_layers.shape_list(q)[1]\n    length = common_layers.shape_list(q)[2]\n    # If (length < 2 * block_length), then we use only one block.\n    if isinstance(length, int) and isinstance(block_length, int):\n      block_length = length if length < block_length * 2 else block_length\n    else:\n      block_length = tf.where(\n          tf.less(length, block_length * 2), length, block_length)\n    depth_k = common_layers.shape_list(k)[3]\n    depth_v = common_layers.shape_list(v)[3]\n    original_length = length\n    padding_size = tf.mod(-length, block_length)\n    length += padding_size\n    padding = [[0, 0], [0, 0], [0, padding_size], [0, 0]]\n    q = tf.pad(q, padding)\n    k = tf.pad(k, padding)\n    v = tf.pad(v, padding)\n\n    num_blocks = length // block_length\n    # compute attention for the first query block.\n    first_q = tf.slice(q, [0, 0, 0, 0], [-1, -1, block_length, -1])\n    first_k = tf.slice(k, [0, 0, 0, 0], [-1, -1, block_length, -1])\n    first_v = tf.slice(v, [0, 0, 0, 0], [-1, -1, block_length, -1])\n    # Relative embeddings will be used later as well.\n    # TODO(avaswani,annahuang): check why 2*bl was breaking for music\n    # Needs to be known at static shape inference time, hence cannot be\n    # 2 * block_length.\n    rel_embed_length = 4 * default_block_length\n    # We only multiply with the needed embeddings as we slice them out.\n    first_rel_embeddings = get_relative_embeddings_left(\n        rel_embed_length, block_length, depth_k, heads,\n        heads_share_relative_embedding, \"relative_embeddings\")\n    first_rel_logits = matmul_with_relative_keys(\n        first_q, first_rel_embeddings, heads_share_relative_embedding)\n    first_logits = tf.matmul(first_q, first_k, transpose_b=True)\n    first_logits += (\n        _relative_position_to_absolute_position_masked(first_rel_logits))\n    # adding a mask\n    first_logits += (\n        common_layers.cast_like(attention_bias_lower_triangle(block_length),\n                                first_logits))\n    first_att = tf.nn.softmax(first_logits,\n                              name=\"first_attention_weights\")\n    # dropping out the attention links for each of the heads\n    first_att = common_layers.dropout_with_broadcast_dims(\n        first_att, 1.0 - dropout_rate,\n        broadcast_dims=None)\n    # only call image summary for the first block\n    if common_layers.should_generate_summaries() and make_image_summary:\n      attention_image_summary(first_att, None)\n    first_output = tf.matmul(first_att, first_v)\n\n    # compute attention for all subsequent query blocks.\n    q = tf.reshape(q, [batch, heads, num_blocks, block_length, depth_k])\n    k = tf.reshape(k, [batch, heads, num_blocks, block_length, depth_k])\n    v = tf.reshape(v, [batch, heads, num_blocks, block_length, depth_v])\n    local_k = _make_local_block(k, depth_k, batch, heads, num_blocks,\n                                block_length)\n    local_v = _make_local_block(v, depth_v, batch, heads, num_blocks,\n                                block_length)\n    tail_q = tf.slice(q, [0, 0, 1, 0, 0], [-1, -1, -1, -1, -1])\n    tail_q = tf.reshape(tail_q,\n                        [batch, heads, num_blocks - 1, block_length, depth_k])\n    local_length = common_layers.shape_list(local_k)[3]\n\n    # collapsing num blocks and batch size so that we can reuse\n    # functions\n    def _reshape_for_relative(x):\n      x_shape = common_layers.shape_list(x)\n      # [batch, num_blocks, heads, length, depth]\n      x = tf.transpose(x, [0, 2, 1, 3, 4])\n      x = tf.reshape(x, [batch*x_shape[2], heads, x_shape[3],\n                         x_shape[4]])\n      return x\n    rel_tail_q = _reshape_for_relative(tail_q)\n    rel_k = _reshape_for_relative(local_k)\n    rel_v = _reshape_for_relative(local_v)\n    rel_embeddings = get_relative_embeddings_left(\n        rel_embed_length, 2 * block_length, depth_k, heads,\n        heads_share_relative_embedding, \"relative_embeddings\")\n    rel_logits = matmul_with_relative_keys(\n        rel_tail_q, rel_embeddings, heads_share_relative_embedding)\n    # Computing relative logits separately for the masked and unmasked parts\n    # because the reshaping logic is different for both\n    masked_rel_logits = tf.slice(rel_logits, [0, 0, 0, block_length],\n                                 [-1, -1, -1, -1])\n    masked_rel_logits = _relative_position_to_absolute_position_masked(\n        masked_rel_logits)\n    unmasked_rel_logits = tf.slice(rel_logits, [0, 0, 0, 0],\n                                   [-1, -1, -1, 2*block_length-1])\n    unmasked_rel_logits = _relative_position_to_absolute_position_unmasked(\n        unmasked_rel_logits)\n    all_rel_logits = tf.concat([unmasked_rel_logits, masked_rel_logits],\n                               axis=3)\n    all_logits = (\n        tf.matmul(rel_tail_q, rel_k, transpose_b=True) + all_rel_logits)\n    # make sure source_pos <= target_pos\n    good_part = common_layers.ones_matrix_band_part(block_length,\n                                                    local_length,\n                                                    -1, block_length)\n    mask = (1.0 - good_part) * -1e9\n    mask = common_layers.cast_like(mask, all_logits)\n    all_logits += tf.reshape(mask, [1, 1, block_length, local_length])\n    weights = tf.nn.softmax(all_logits, name=\"attention_weights\")\n    # [batch (* num_blocks), heads, query_length (=block_length),\n    # key_length (=2*block_length)]\n    weights = common_layers.dropout_with_broadcast_dims(\n        weights, 1.0 - dropout_rate,\n        broadcast_dims=None)\n\n    output = tf.matmul(weights, rel_v)\n    if add_relative_to_values:\n      # Adds the contribution of the weighted relative embeddings to the values.\n      weights_for_unmasked, weights_for_masked = (\n          tf.split(weights, 2, axis=3))\n      rel_weights_unmasked = _absolute_position_to_relative_position_unmasked(\n          weights_for_unmasked)\n      rel_weights_masked = _absolute_position_to_relative_position_masked(\n          weights_for_masked)\n\n      value_rel_embeddings_unmasked = get_relative_embeddings_left(\n          rel_embed_length, 2 * block_length, depth_v,\n          heads, heads_share_relative_embedding,\n          \"value_relative_embeddings\")\n      # The unmasked part starts with index -1 as opposed 0 has take uptil last.\n      if heads_share_relative_embedding:\n        value_rel_embeddings_unmasked = value_rel_embeddings_unmasked[:-1, :]\n      else:\n        value_rel_embeddings_unmasked = value_rel_embeddings_unmasked[:, :-1, :]\n      value_rel_embeddings_masked = get_relative_embeddings_left(\n          rel_embed_length, block_length, depth_v,\n          heads, heads_share_relative_embedding,\n          \"value_relative_embeddings\")\n\n      # [batch (*num_blocks), heads, query length, key length]\n      rel_weights = tf.concat(\n          [rel_weights_unmasked, rel_weights_masked], axis=3)\n      if heads_share_relative_embedding:\n        value_rel_embeddings_concat_axis = 0\n      else:\n        value_rel_embeddings_concat_axis = 1\n      value_rel_embeddings = tf.concat(\n          [value_rel_embeddings_unmasked, value_rel_embeddings_masked],\n          axis=value_rel_embeddings_concat_axis)\n      output_rel = matmul_with_relative_values(\n          rel_weights, value_rel_embeddings, heads_share_relative_embedding)\n      output += output_rel\n\n    # bring to [batch, heads, num_blocks-1, block_length, depth]\n    output = tf.reshape(output,\n                        [batch, num_blocks-1, heads, block_length, depth_v])\n    output = tf.transpose(output, [0, 2, 1, 3, 4])\n\n    output = tf.reshape(\n        output, [batch, heads, (num_blocks - 1) * block_length, depth_v])\n    output = tf.concat([first_output, output], axis=2)\n    output = tf.slice(output, [0, 0, 0, 0], [-1, -1, original_length, -1])\n    output = tf.reshape(output, [batch, heads, original_length, depth_v])\n    return output\n\n\ndef matmul_with_relative_values(x, y, heads_share_relative_embedding):\n  if heads_share_relative_embedding:\n    ret = tf.einsum(\"bhlm,md->bhld\", x, y)\n  else:\n    ret = tf.einsum(\"bhlm,hmd->bhld\", x, y)\n  return ret\n\n\ndef matmul_with_relative_keys(x, y, heads_share_relative_embedding):\n  if heads_share_relative_embedding:\n    ret = tf.einsum(\"bhld,md->bhlm\", x, y)\n  else:\n    ret = tf.einsum(\"bhld,hmd->bhlm\", x, y)\n  return ret\n\n\ndef local_attention_1d(q, k, v, block_length=128, filter_width=100, name=None):\n  \"\"\"Strided block local self-attention.\n\n  The sequence is divided into blocks of length block_length. Attention for a\n  given query position can see all memory positions in the corresponding block\n  and filter_width many positions to the left and right of the block.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth_k]\n    k: a Tensor with shape [batch, heads, length, depth_k]\n    v: a Tensor with shape [batch, heads, length, depth_v]\n    block_length: an integer\n    filter_width: an integer indicating how much to look left and right of the\n      block.\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, length, depth_v]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"local_self_attention_1d\", values=[q, k, v]):\n    # Check that q, k, v have the same shape except in their depth dimension.\n    q.get_shape()[:-1].assert_is_compatible_with(k.get_shape()[:-1])\n    q.get_shape()[:-1].assert_is_compatible_with(v.get_shape()[:-1])\n\n    batch_size, num_heads, original_length, _ = common_layers.shape_list(q)\n\n    # Pad query, key, value to ensure multiple of corresponding lengths.\n    def pad_to_multiple(x, pad_length):\n      x_length = common_layers.shape_list(x)[2]\n      return tf.pad(x, [[0, 0], [0, 0], [0, -x_length % pad_length], [0, 0]])\n\n    def pad_l_and_r(x, pad_length):\n      return tf.pad(x, [[0, 0], [0, 0], [pad_length, pad_length], [0, 0]])\n\n    # Set up query blocks.\n    # [batch, heads, blocks_q, block_length, depth_k]\n    q = pad_to_multiple(q, block_length)\n    q = reshape_by_blocks(q, common_layers.shape_list(q), block_length)\n    total_query_blocks = common_layers.shape_list(q)[2]\n\n    # Set up key and value blocks.\n    # [batch, heads, blocks_k, block_length, depth_k]\n    blocks_per_filter_width = filter_width // block_length\n    remaining_items = filter_width % block_length\n    k = pad_to_multiple(k, block_length)\n    v = pad_to_multiple(v, block_length)\n    k = pad_l_and_r(k, filter_width + block_length - remaining_items)\n    v = pad_l_and_r(v, filter_width + block_length - remaining_items)\n    k = reshape_by_blocks(k, common_layers.shape_list(k), block_length)\n    v = reshape_by_blocks(v, common_layers.shape_list(v), block_length)\n\n    total_kv_blocks = common_layers.shape_list(k)[2]\n\n    slices = []\n    # prepare the left-most and right-most partial blocks if needed\n    if remaining_items:\n      first_partial_block_k = tf.slice(\n          k, [0, 0, 0, block_length - remaining_items, 0],\n          [-1, -1, total_query_blocks, -1, -1])\n      first_partial_block_v = tf.slice(\n          v, [0, 0, 0, block_length - remaining_items, 0],\n          [-1, -1, total_query_blocks, -1, -1])\n      last_partial_block_k = tf.slice(\n          k, [0, 0, total_kv_blocks - total_query_blocks, 0, 0],\n          [-1, -1, -1, remaining_items, -1])\n      last_partial_block_v = tf.slice(\n          v, [0, 0, total_kv_blocks - total_query_blocks, 0, 0],\n          [-1, -1, -1, remaining_items, -1])\n      slices.append((first_partial_block_k, first_partial_block_v))\n      slices.append((last_partial_block_k, last_partial_block_v))\n\n    # Prepare the rest of the blocks\n    first_block_index = 1 if remaining_items else 0\n    attention_blocks = 2 * blocks_per_filter_width + 1\n    for i in range(first_block_index, attention_blocks + first_block_index):\n      block_k = tf.slice(k, [0, 0, i, 0, 0],\n                         [-1, -1, total_query_blocks, -1, -1])\n      block_v = tf.slice(v, [0, 0, i, 0, 0],\n                         [-1, -1, total_query_blocks, -1, -1])\n      slices.append((block_k, block_v))\n    # [batch, heads, blocks_q, block_length + 2 * filter_width, depth_k]\n    k = tf.concat([s[0] for s in slices], axis=3)\n    v = tf.concat([s[1] for s in slices], axis=3)\n\n    attention_bias = tf.expand_dims(embedding_to_padding(k) * -1e9, axis=-2)\n    depth_v = common_layers.shape_list(v)[-1]\n\n    output = dot_product_attention(\n        q,\n        k,\n        v,\n        attention_bias,\n        dropout_rate=0.,\n        name=\"local_1d\",\n        make_image_summary=False)\n    output = tf.reshape(output, [batch_size, num_heads, -1, depth_v])\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0], [-1, -1, original_length, -1])\n    output.set_shape([None if isinstance(dim, tf.Tensor) else dim for dim in\n                      (batch_size, num_heads, original_length, depth_v)])\n    return output\n\n\ndef reshape_by_blocks(x, x_shape, memory_block_size):\n  \"\"\"Reshapes input by splitting its length over blocks of memory_block_size.\n\n  Args:\n    x: a Tensor with shape [batch, heads, length, depth]\n    x_shape: tf.TensorShape of x.\n    memory_block_size: Integer which divides length.\n\n  Returns:\n    Tensor with shape\n    [batch, heads, length // memory_block_size, memory_block_size, depth].\n  \"\"\"\n  x = tf.reshape(x, [\n      x_shape[0], x_shape[1], x_shape[2] // memory_block_size,\n      memory_block_size, x_shape[3]\n  ])\n  return x\n\n\ndef dilated_self_attention_1d(q,\n                              k,\n                              v,\n                              query_block_size=128,\n                              memory_block_size=128,\n                              gap_size=2,\n                              num_memory_blocks=2,\n                              name=None):\n  \"\"\"Dilated self-attention.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth]\n    k: a Tensor with shape [batch, heads, length, depth]\n    v: a Tensor with shape [batch, heads, length, depth]\n    query_block_size: an integer indicating size of query block\n    memory_block_size: an integer indicating the size of a memory block.\n    gap_size: an integer indicating the gap size\n    num_memory_blocks: how many memory blocks to look at to the left and right.\n      Each will be separated by gap_size.\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, length, depth]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"dilated_self_attention_1d\", values=[q, k, v]):\n    v_list_shape = v.get_shape().as_list()\n    assert v_list_shape == k.shape.as_list(), \"K and V depths must be equal\"\n    v_shape = common_layers.shape_list(v)\n    depth_v = v_shape[3]\n    batch_size = v_shape[0]\n    num_heads = v_shape[1]\n    original_length = common_layers.shape_list(q)[2]\n\n    # Pad query, key, value to ensure multiple of corresponding lengths.\n    def pad_to_multiple(x, pad_length):\n      x_length = common_layers.shape_list(x)[2]\n      return tf.pad(x, [[0, 0], [0, 0], [0, -x_length % pad_length], [0, 0]])\n\n    def pad_l_and_r(x, pad_length):\n      return tf.pad(x, [[0, 0], [0, 0], [pad_length, pad_length], [0, 0]])\n\n    q = pad_to_multiple(q, query_block_size)\n    v = pad_to_multiple(v, query_block_size)\n    k = pad_to_multiple(k, query_block_size)\n\n    # Set up query blocks.\n    new_q_shape = common_layers.shape_list(q)\n    q = reshape_by_blocks(q, new_q_shape, query_block_size)\n    self_k_part = reshape_by_blocks(k, new_q_shape, query_block_size)\n    self_v_part = reshape_by_blocks(v, new_q_shape, query_block_size)\n\n    # Set up key and value windows.\n    k_v_padding = (gap_size + memory_block_size) * num_memory_blocks\n    k = pad_l_and_r(k, k_v_padding)\n    v = pad_l_and_r(v, k_v_padding)\n\n    # Get gather indices.\n    index_length = (new_q_shape[2] - query_block_size + memory_block_size)\n    indices = tf.range(0, index_length, delta=1, name=\"index_range\")\n    indices = tf.reshape(indices, [1, -1, 1])  # [1, length, 1] for convs\n    kernel = tf.expand_dims(tf.eye(memory_block_size), axis=1)\n    gather_indices = tf.nn.conv1d(\n        tf.cast(indices, tf.float32),\n        kernel,\n        query_block_size,\n        padding=\"VALID\",\n        name=\"gather_conv\")\n\n    gather_indices = tf.squeeze(tf.cast(gather_indices, tf.int32), axis=0)\n\n    # Get left and right memory blocks for each query.\n    # [length, batch, heads, dim]\n    k_t = tf.transpose(k, [2, 0, 1, 3])\n    v_t = tf.transpose(v, [2, 0, 1, 3])\n    left_k = gather_dilated_memory_blocks(\n        k_t[:-k_v_padding, :, :, :], num_memory_blocks, gap_size,\n        query_block_size, memory_block_size, gather_indices)\n    left_v = gather_dilated_memory_blocks(\n        v_t[:-k_v_padding, :, :, :], num_memory_blocks, gap_size,\n        query_block_size, memory_block_size, gather_indices)\n\n    right_k = gather_dilated_memory_blocks(\n        k_t[k_v_padding:, :, :, :],\n        num_memory_blocks,\n        gap_size,\n        query_block_size,\n        memory_block_size,\n        gather_indices,\n        direction=\"right\")\n    right_v = gather_dilated_memory_blocks(\n        v_t[k_v_padding:, :, :, :],\n        num_memory_blocks,\n        gap_size,\n        query_block_size,\n        memory_block_size,\n        gather_indices,\n        direction=\"right\")\n\n    k_windows = tf.concat([left_k, self_k_part, right_k], axis=3)\n    v_windows = tf.concat([left_v, self_v_part, right_v], axis=3)\n    attention_bias = tf.expand_dims(\n        embedding_to_padding(k_windows) * -1e9, axis=-2)\n\n    output = dot_product_attention(\n        q,\n        k_windows,\n        v_windows,\n        attention_bias,\n        dropout_rate=0.,\n        name=\"dilated_1d\",\n        make_image_summary=False)\n    output = tf.reshape(output, [batch_size, num_heads, -1, depth_v])\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0], [-1, -1, original_length, -1])\n    output.set_shape(v_list_shape)\n    return output\n\n\ndef gather_dilated_memory_blocks(x,\n                                 num_memory_blocks,\n                                 gap_size,\n                                 query_block_size,\n                                 memory_block_size,\n                                 gather_indices,\n                                 direction=\"left\"):\n  \"\"\"Gathers blocks with gaps in between.\n\n  Args:\n    x: Tensor of shape [length, batch, heads, depth]\n    num_memory_blocks: how many memory blocks to look in \"direction\". Each will\n      be separated by gap_size.\n    gap_size: an integer indicating the gap size\n    query_block_size: an integer indicating size of query block\n    memory_block_size: an integer indicating the size of a memory block.\n    gather_indices: The indices to gather from.\n    direction: left or right\n\n  Returns:\n    Tensor of shape [batch, heads, blocks, block_length, depth]\n  \"\"\"\n  gathered_blocks = []\n  # gathering memory blocks\n  for block_id in range(num_memory_blocks):\n    block_end_index = -(query_block_size + gap_size *\n                        (block_id + 1) + memory_block_size * block_id)\n    block_start_index = (\n        (memory_block_size + gap_size) * (num_memory_blocks - (block_id + 1)))\n    if direction != \"left\":\n      [block_end_index,\n       block_start_index] = [-block_start_index, -block_end_index]\n    if block_end_index == 0:\n      x_block = x[block_start_index:]\n    else:\n      x_block = x[block_start_index:block_end_index]\n\n    def gather_dilated_1d_blocks(x, gather_indices):\n      x_new = tf.gather(x, gather_indices)\n      # [batch, heads, blocks, block_length, dim]\n      return tf.transpose(x_new, [2, 3, 0, 1, 4])\n\n    gathered_blocks.append(gather_dilated_1d_blocks(x_block, gather_indices))\n  return tf.concat(gathered_blocks, 3)\n\n\ndef masked_dilated_self_attention_1d(q,\n                                     k,\n                                     v,\n                                     query_block_size=64,\n                                     memory_block_size=64,\n                                     gap_size=2,\n                                     num_memory_blocks=2,\n                                     name=None):\n  \"\"\"Dilated self-attention. TODO(avaswani): Try it and write a paper on it.\n\n  Args:\n    q: a Tensor with shape [batch, heads, length, depth]\n    k: a Tensor with shape [batch, heads, length, depth]\n    v: a Tensor with shape [batch, heads, length, depth]\n    query_block_size: an integer\n    memory_block_size: an integer indicating how much to look left.\n    gap_size: an integer indicating the gap size\n    num_memory_blocks: how many memory blocks to look at to the left. Each will\n      be separated by gap_size.\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, length, depth]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"masked_dilated_self_attention_1d\", values=[q, k, v]):\n    v_list_shape = v.get_shape().as_list()\n    assert v_list_shape == k.shape.as_list(), \"K and V depths must be equal\"\n    v_shape = common_layers.shape_list(v)\n    depth_v = v_shape[3]\n    batch_size = v_shape[0]\n    num_heads = v_shape[1]\n    original_length = common_layers.shape_list(q)[2]\n\n    # Pad query, key, value to ensure multiple of corresponding lengths.\n    def pad_to_multiple(x, pad_length):\n      x_length = common_layers.shape_list(x)[2]\n      return tf.pad(x, [[0, 0], [0, 0], [0, -x_length % pad_length], [0, 0]])\n\n    def pad_l(x, left_pad_length):\n      return tf.pad(x, [[0, 0], [0, 0], [left_pad_length, 0], [0, 0]])\n\n    q = pad_to_multiple(q, query_block_size)\n    v = pad_to_multiple(v, query_block_size)\n    k = pad_to_multiple(k, query_block_size)\n\n    # Set up query blocks.\n    new_q_shape = common_layers.shape_list(q)\n    q = reshape_by_blocks(q, new_q_shape, query_block_size)\n\n    # Set up key and value windows.\n    self_k_part = reshape_by_blocks(k, new_q_shape, query_block_size)\n    self_v_part = reshape_by_blocks(v, new_q_shape, query_block_size)\n    k_v_padding = (gap_size + memory_block_size) * num_memory_blocks\n    k = pad_l(k, k_v_padding)\n    v = pad_l(v, k_v_padding)\n\n    # Get gather indices.\n    index_length = (new_q_shape[2] - query_block_size + memory_block_size)\n\n    indices = tf.range(0, index_length, delta=1, name=\"index_range\")\n    indices = tf.reshape(indices, [1, -1, 1])  # [1, length, 1] for convs\n    kernel = tf.expand_dims(tf.eye(memory_block_size), axis=1)\n    gather_indices = tf.nn.conv1d(\n        tf.cast(indices, tf.float32),\n        kernel,\n        query_block_size,\n        padding=\"VALID\",\n        name=\"gather_conv\")\n    gather_indices = tf.squeeze(tf.cast(gather_indices, tf.int32), axis=0)\n\n    # Get left and right memory blocks for each query.\n    # [length, batch, heads, dim]\n    k_t = tf.transpose(k, [2, 0, 1, 3])\n    v_t = tf.transpose(v, [2, 0, 1, 3])\n\n    k_unmasked_windows = gather_dilated_memory_blocks(\n        k_t, num_memory_blocks, gap_size, query_block_size, memory_block_size,\n        gather_indices)\n    v_unmasked_windows = gather_dilated_memory_blocks(\n        v_t, num_memory_blocks, gap_size, query_block_size, memory_block_size,\n        gather_indices)\n\n    # Combine memory windows.\n    block_q_shape = common_layers.shape_list(q)\n    masked_attention_bias = tf.tile(\n        tf.expand_dims(attention_bias_lower_triangle(query_block_size), axis=0),\n        [block_q_shape[0], block_q_shape[1], block_q_shape[2], 1, 1])\n    padding_attention_bias = tf.expand_dims(\n        embedding_to_padding(k_unmasked_windows) * -1e9, axis=-2)\n    padding_attention_bias = tf.tile(padding_attention_bias,\n                                     [1, 1, 1, query_block_size, 1])\n    attention_bias = tf.concat(\n        [masked_attention_bias, padding_attention_bias], axis=-1)\n    # combine memory windows\n    k_windows = tf.concat([self_k_part, k_unmasked_windows], 3)\n    v_windows = tf.concat([self_v_part, v_unmasked_windows], 3)\n    output = dot_product_attention(\n        q,\n        k_windows,\n        v_windows,\n        attention_bias,\n        dropout_rate=0.,\n        name=\"dilated_1d\",\n        make_image_summary=False)\n    output = tf.reshape(output, [batch_size, num_heads, -1, depth_v])\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0], [-1, -1, original_length, -1])\n    output.set_shape(v_list_shape)\n    return output\n\n\ndef local_attention_2d(q,\n                       k,\n                       v,\n                       query_shape=(8, 16),\n                       memory_flange=(8, 16),\n                       name=None):\n  \"\"\"Strided block local self-attention.\n\n  The 2-D sequence is divided into 2-D blocks of shape query_shape. Attention\n  for a given query position can only see memory positions less than or equal to\n  the query position. The memory positions are the corresponding block with\n  memory_flange many positions to add to the height and width of the block\n  (namely, left, top, and right).\n\n  Args:\n    q: a Tensor with shape [batch, heads, h, w, depth_k]\n    k: a Tensor with shape [batch, heads, h, w, depth_k]\n    v: a Tensor with shape [batch, heads, h, w, depth_v]. In the current\n      implementation, depth_v must be equal to depth_k.\n    query_shape: an tuple indicating the height and width of each query block.\n    memory_flange: an integer indicating how much to look in height and width\n      from each query block.\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, h, w, depth_v]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"local_self_attention_2d\", values=[q, k, v]):\n    v_shape = common_layers.shape_list(v)\n\n    # Pad query, key, value to ensure multiple of corresponding lengths.\n    q = pad_to_multiple_2d(q, query_shape)\n    k = pad_to_multiple_2d(k, query_shape)\n    v = pad_to_multiple_2d(v, query_shape)\n    paddings = [[0, 0], [0, 0], [memory_flange[0], memory_flange[1]],\n                [memory_flange[0], memory_flange[1]], [0, 0]]\n    k = tf.pad(k, paddings)\n    v = tf.pad(v, paddings)\n\n    # Set up query blocks.\n    q_indices = gather_indices_2d(q, query_shape, query_shape)\n    q_new = gather_blocks_2d(q, q_indices)\n\n    # Set up key and value blocks.\n    memory_shape = (query_shape[0] + 2 * memory_flange[0],\n                    query_shape[1] + 2 * memory_flange[1])\n    k_and_v_indices = gather_indices_2d(k, memory_shape, query_shape)\n    k_new = gather_blocks_2d(k, k_and_v_indices)\n    v_new = gather_blocks_2d(v, k_and_v_indices)\n\n    attention_bias = tf.expand_dims(\n        tf.to_float(embedding_to_padding(k_new)) * -1e9, axis=-2)\n    output = dot_product_attention(\n        q_new,\n        k_new,\n        v_new,\n        attention_bias,\n        dropout_rate=0.,\n        name=\"local_2d\",\n        make_image_summary=False)\n    # Put representations back into original shapes.\n    padded_q_shape = common_layers.shape_list(q)\n    output = scatter_blocks_2d(output, q_indices, padded_q_shape)\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0, 0],\n                      [-1, -1, v_shape[2], v_shape[3], -1])\n    return output\n\n\ndef pad_to_multiple_2d(x, block_shape):\n  \"\"\"Making sure x is a multiple of shape.\n\n  Args:\n    x: a [batch, heads, h, w, depth] or [batch, h, w, depth] tensor\n    block_shape: a 2-d list of integer shapes\n\n  Returns:\n    padded_x: a [batch, heads, h, w, depth] or [batch, h, w, depth] tensor\n  \"\"\"\n  old_shape = x.get_shape().dims\n  last = old_shape[-1]\n  if len(old_shape) == 4:\n    height_padding = -common_layers.shape_list(x)[1] % block_shape[0]\n    width_padding = -common_layers.shape_list(x)[2] % block_shape[1]\n    paddings = [[0, 0], [0, height_padding], [0, width_padding], [0, 0]]\n  elif len(old_shape) == 5:\n    height_padding = -common_layers.shape_list(x)[2] % block_shape[0]\n    width_padding = -common_layers.shape_list(x)[3] % block_shape[1]\n    paddings = [[0, 0], [0, 0], [0, height_padding], [0, width_padding], [0, 0]]\n\n  padded_x = tf.pad(x, paddings)\n  padded_shape = padded_x.get_shape().as_list()\n  padded_shape = padded_shape[:-1] + [last]\n  padded_x.set_shape(padded_shape)\n  return padded_x\n\n\ndef reshape_range(tensor, i, j, shape):\n  \"\"\"Reshapes a tensor between dimensions i and j.\"\"\"\n  t_shape = common_layers.shape_list(tensor)\n  target_shape = t_shape[:i] + shape + t_shape[j:]\n  return tf.reshape(tensor, target_shape)\n\n\ndef gather_blocks_2d(x, indices):\n  \"\"\"Gathers flattened blocks from x.\"\"\"\n  x_shape = common_layers.shape_list(x)\n  x = reshape_range(x, 2, 4, [tf.reduce_prod(x_shape[2:4])])\n  # [length, batch, heads, dim]\n  x_t = tf.transpose(x, [2, 0, 1, 3])\n  x_new = tf.gather(x_t, indices)\n  # returns [batch, heads, num_blocks, block_length ** 2, dim]\n  return tf.transpose(x_new, [2, 3, 0, 1, 4])\n\n\ndef scatter_blocks_2d(x, indices, shape):\n  \"\"\"scatters blocks from x into shape with indices.\"\"\"\n  x_shape = common_layers.shape_list(x)\n  # [length, batch, heads, dim]\n  x_t = tf.transpose(\n      tf.reshape(x, [x_shape[0], x_shape[1], -1, x_shape[-1]]), [2, 0, 1, 3])\n  x_t_shape = common_layers.shape_list(x_t)\n  indices = tf.reshape(indices, [-1, 1])\n  scattered_x = tf.scatter_nd(indices, x_t, x_t_shape)\n  scattered_x = tf.transpose(scattered_x, [1, 2, 0, 3])\n  return tf.reshape(scattered_x, shape)\n\n\ndef gather_indices_2d(x, block_shape, block_stride):\n  \"\"\"Getting gather indices.\"\"\"\n  # making an identity matrix kernel\n  kernel = tf.eye(block_shape[0] * block_shape[1])\n  kernel = reshape_range(kernel, 0, 1, [block_shape[0], block_shape[1], 1])\n  # making indices [1, h, w, 1] to appy convs\n  x_shape = common_layers.shape_list(x)\n  indices = tf.range(x_shape[2] * x_shape[3])\n  indices = tf.reshape(indices, [1, x_shape[2], x_shape[3], 1])\n  indices = tf.nn.conv2d(\n      tf.cast(indices, tf.float32),\n      kernel,\n      strides=[1, block_stride[0], block_stride[1], 1],\n      padding=\"VALID\")\n  # making indices [num_blocks, dim] to gather\n  dims = common_layers.shape_list(indices)[:3]\n  if all([isinstance(dim, int) for dim in dims]):\n    num_blocks = functools.reduce(operator.mul, dims, 1)\n  else:\n    num_blocks = tf.reduce_prod(dims)\n  indices = tf.reshape(indices, [num_blocks, -1])\n  return tf.cast(indices, tf.int32)\n\n\ndef make_2d_block_raster_mask(query_shape, memory_flange):\n  \"\"\"Creates a mask for 2d block raster scan.\n\n  The query mask can look to the left, top left, top, and top right, but\n  not to the right. Inside the query, we have the standard raster scan\n  masking.\n  Args:\n    query_shape: A tuple of ints (query_height, query_width)\n    memory_flange: A tuple of ints\n      (memory_flange_height, memory_flange_width)\n\n  Returns:\n    A tensor of shape query_size, memory_size\n  \"\"\"\n  # mask inside the query block\n  query_triangle = common_layers.ones_matrix_band_part(\n      np.prod(query_shape), np.prod(query_shape), -1, 0)\n  split_query_masks = tf.split(query_triangle, query_shape[0], axis=1)\n  # adding mask for left and right\n  mask_pieces = [\n      tf.concat(  # pylint: disable=g-complex-comprehension\n          [tf.ones([np.prod(query_shape), memory_flange[1]]),\n           split_query_masks[i],\n           tf.zeros([np.prod(query_shape), memory_flange[1]])],\n          axis=1) for i in range(query_shape[0])\n  ]\n  # adding mask for top\n  final_mask = tf.concat(\n      [\n          tf.ones([\n              np.prod(query_shape),\n              (query_shape[1] + 2 * memory_flange[1]) * memory_flange[0]\n          ]),\n          tf.concat(mask_pieces, axis=1)\n      ],\n      axis=1)\n  # 0.0 is visible location, 1.0 is masked.\n  return 1. - final_mask\n\n\ndef get_memory_region(x, query_block_shape, memory_flange, q_indices):\n  \"\"\"Get the memory regions that surround a 2d query.\n\n    The memory regions will be the left and top right.\n\n  Args:\n    x: A tensor with shape [batch, heads, height, width, depth]\n    query_block_shape: a 2-d tuple of integers\n    memory_flange: a 2-d tuple of integers\n    q_indices: a tensor of indices for each of the center blocks.\n      [num_blocks, block_length]\n  Returns:\n    x_flange: A tensor of shape [batch, heads, #blocks, block_length, depth]\n  \"\"\"\n  # Padding x to be multiple of query_shape and then\n  # extracting the memory blocks from the same regions as the query blocks\n  x_query_padded = pad_to_multiple_2d(x, query_block_shape)\n  x_center = gather_blocks_2d(x_query_padded, q_indices)\n  # Then padding the flange region\n  paddings = [[0, 0], [0, 0], [memory_flange[0], 0],\n              [memory_flange[1], memory_flange[1]], [0, 0]]\n  x_memory_padded = tf.pad(x_query_padded, paddings)\n  left_x = None\n  top_x = None\n  # Extracting the memory regions around the query block. left_x_region extends\n  # to the left and the top_x_region is the combination of top left, top, and\n  # top right of the query block\n  # if no left region\n  if memory_flange[1] > 0:\n    left_x_region = x_memory_padded[:, :, memory_flange[\n        0]:, :-(query_block_shape[1] + memory_flange[1]), :]\n    left_memory_shape = (query_block_shape[0], memory_flange[1])\n    left_indices = gather_indices_2d(left_x_region, left_memory_shape,\n                                     query_block_shape)\n    left_x = gather_blocks_2d(left_x_region, left_indices)\n  # if no top region\n  if memory_flange[0] > 0:\n    top_x_region = x_memory_padded[:, :, :-query_block_shape[0], :, :]\n\n    top_memory_shape = (memory_flange[0],\n                        query_block_shape[1] + 2 * memory_flange[1])\n\n    top_indices = gather_indices_2d(top_x_region, top_memory_shape,\n                                    query_block_shape)\n\n    top_x = gather_blocks_2d(top_x_region, top_indices)\n  x_flange = None\n  if top_x is not None and left_x is not None:\n    x_flange = tf.concat([top_x, left_x], axis=3)\n  else:\n    x_flange = top_x if top_x is not None else left_x\n  return x_flange, x_center\n\n\ndef get_shifted_center_blocks(x, indices):\n  \"\"\"Get right shifted blocks for masked local attention 2d.\n\n  Args:\n    x: A tensor with shape [batch, heads, height, width, depth]\n    indices: The indices to gather blocks\n\n  Returns:\n    x_shifted: a tensor of extracted blocks, each block right shifted along\n      length.\n  \"\"\"\n  center_x = gather_blocks_2d(x, indices)\n\n  # Shift right along the length dimension\n  def shift_right_2d_blocks(x):\n    \"\"\"Shift the second to last dimension of x right by one.\"\"\"\n    shifted_targets = (\n        tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 0], [0, 0]])[:, :, :, :-1, :])\n    return shifted_targets\n\n  x_shifted = shift_right_2d_blocks(center_x)\n  return x_shifted\n\n\ndef right_shift_blockwise(x, query_shape, name=None):\n  \"\"\"Right shifts once in every block.\n\n  Args:\n    x: a tensor of shape [batch, height, width, depth]\n    query_shape: A 2d tuple of ints\n    name: a string\n\n  Returns:\n    output: a tensor of the same shape as x\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"right_shift_blockwise\", values=[x]):\n    x_list_shape = x.get_shape().as_list()\n    x_shape = common_layers.shape_list(x)\n    # Add a dummy dimension for heads.\n    x = tf.expand_dims(x, axis=1)\n    x = pad_to_multiple_2d(x, query_shape)\n    padded_x_shape = common_layers.shape_list(x)\n    # Set up q blocks.\n    x_indices = gather_indices_2d(x, query_shape, query_shape)\n    x_new = get_shifted_center_blocks(x, x_indices)\n\n    # Put representations back into original shapes.\n    output = scatter_blocks_2d(x_new, x_indices, padded_x_shape)\n    # Remove the dummy head dimension.\n    output = tf.squeeze(output, axis=1)\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0], [-1, x_shape[1], x_shape[2], -1])\n    output.set_shape(x_list_shape)\n    return output\n\n\ndef right_shift_blockwise_nd(x, block_shape):\n  \"\"\"Right shift once in every block.\n\n  Args:\n    x: a [batch, d1, d2, ..., dn, depth] tensor\n    block_shape: a tuple (q1, q2, ..., qn) representing the block shape\n\n  Returns:\n    a [batch, d1, d2, ..., dn, depth] tensor, right shifted.\n  \"\"\"\n  blocked_x = break_into_blocks_nd(x, block_shape)\n  blocked_x_shape = common_layers.shape_list(blocked_x)\n  blocked_x = tf.reshape(blocked_x,\n                         [blocked_x_shape[0], -1, blocked_x_shape[-1]])\n  padded_x = tf.pad(blocked_x, [[0, 0], [1, 0], [0, 0]])\n  x = tf.slice(padded_x, [0, 0, 0],\n               [-1, np.prod(blocked_x_shape[1:-1], dtype=np.int32), -1])\n  x = tf.reshape(x, blocked_x_shape)\n  return put_back_blocks_nd(x, block_shape)\n\n\ndef masked_local_attention_2d(q,\n                              k,\n                              v,\n                              query_shape=(8, 16),\n                              memory_flange=(8, 16),\n                              name=None):\n  \"\"\"Strided block local self-attention.\n\n  Each position in a query block can attend to all the generated queries in\n  the query block, which are generated in raster scan, and positions that are\n  generated to the left and top. The shapes are specified by query shape and\n  memory flange. Note that if you're using this function, you do not need to\n  right shift. Right shifting happens inside this function separately for each\n  block.\n\n  Args:\n    q: a Tensor with shape [batch, heads, h, w, depth_k]\n    k: a Tensor with shape [batch, heads, h, w, depth_k]\n    v: a Tensor with shape [batch, heads, h, w, depth_v]. In the current\n      implementation, depth_v must be equal to depth_k.\n    query_shape: an tuple indicating the height and width of each query block.\n      query_shape = block_shape\n    memory_flange: an integer indicating how much to look in height and width\n      from each query block.\n      memory shape = query_shape + (block_flange[0], 2*block_flange[1])\n    name: an optional string\n\n  Returns:\n    a Tensor of shape [batch, heads, h, w, depth_v]\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"local_masked_self_attention_2d\", values=[q, k, v]):\n    v_shape = common_layers.shape_list(v)\n\n    # Pad query to ensure multiple of corresponding lengths.\n    q = pad_to_multiple_2d(q, query_shape)\n\n    # Set up query blocks.\n    q_indices = gather_indices_2d(q, query_shape, query_shape)\n    q_new = gather_blocks_2d(q, q_indices)\n\n    # Set up key and value blocks.\n    k_flange, k_center = get_memory_region(k, query_shape, memory_flange,\n                                           q_indices)\n    v_flange, v_center = get_memory_region(v, query_shape, memory_flange,\n                                           q_indices)\n    if k_flange is not None:\n      k_new = tf.concat([k_flange, k_center], axis=3)\n      v_new = tf.concat([v_flange, v_center], axis=3)\n    else:\n      k_new = k_center\n      v_new = v_center\n\n    # Set up the masks.\n    query_elements = np.prod(query_shape)\n    padding_mask = None\n    if k_flange is not None:\n      padding_mask = tf.expand_dims(\n          embedding_to_padding(k_flange) * -1e9, axis=-2)\n      padding_mask = tf.tile(padding_mask, [1, 1, 1, query_elements, 1])\n\n    center_attention_bias = attention_bias_lower_triangle(\n        np.prod(query_elements))\n    center_attention_bias = tf.reshape(\n        center_attention_bias, [1, 1, 1, query_elements, query_elements])\n    v_center_shape = common_layers.shape_list(v_center)\n    center_attention_bias = tf.tile(\n        center_attention_bias,\n        [v_center_shape[0], v_center_shape[1], v_center_shape[2], 1, 1])\n    if padding_mask is not None:\n      # Combine the mask for padding and visible region.\n      attention_bias = tf.concat([padding_mask, center_attention_bias], axis=4)\n    else:\n      attention_bias = center_attention_bias\n\n    output = dot_product_attention(\n        q_new,\n        k_new,\n        v_new,\n        attention_bias,\n        dropout_rate=0.,\n        name=\"masked_local_2d\",\n        make_image_summary=False)\n    # Put representations back into original shapes.\n    padded_q_shape = common_layers.shape_list(q)\n    output = scatter_blocks_2d(output, q_indices, padded_q_shape)\n\n    # Remove the padding if introduced.\n    output = tf.slice(output, [0, 0, 0, 0, 0],\n                      [-1, -1, v_shape[2], v_shape[3], -1])\n    return output\n\n\ndef masked_local_attention_nd(q,\n                              k,\n                              v,\n                              query_shape,\n                              memory_flange,\n                              decode_step=None,\n                              name=None):\n  \"\"\"Masked local attention nd.\n\n  Each position in q can attend to positions in memory that are positioned less\n  than or equal to query position according to raster scan ordering and are in\n  the same memory block. A memory block is n-dimensional and each dimension 'i'\n  is of size q[i] + 2 * m[i] except for the first dimension which is of size\n  q[0] + m[0]. NOTE: This computation assumes memory_flange is divisible by\n  query_shape in every dimension.\n\n  Args:\n    q: a [batch, heads, d1, d2, ..., dn, depth_k] tensor or a [batch, heads, 1,\n      1, ..., 1, depth_k] tensor in decoding mode.\n    k: a [batch, heads, d1, d2, ..., dn, depth_k] tensor\n    v: a [batch, heads, d1, d2, ..., dn, depth_v] tensor\n    query_shape: a tuple (q1, q2, ..., qn) indicating the shape of query blocks.\n    memory_flange: a tuple (m1, m2, ..., mn) indicating the number of extra\n      positions in the attention memory. memory_shape=[q1 + m1, d2 + 2 * m2,\n      ..., dn + 2 * mn]\n    decode_step: an integer in fast decoding mode.\n    name: an optional string\n\n  Returns:\n    a [batch, head, d1, d2, ..., dn, depth_v] tensor or\n      [batch, head, 1, 1, ..., 1, depth_v] if decode_step is not None.\n  \"\"\"\n  assert all([m % b == 0 for m, b in zip(memory_flange, query_shape)])\n  with tf.variable_scope(\n      name, default_name=\"masked_local_attention_nd\", values=[q, k, v]):\n    # This computation only applies to self attention, so assert q, k and v have\n    # the same dimensions.\n    if decode_step is None:\n      q.get_shape().assert_is_compatible_with(k.get_shape())\n      q.get_shape()[:-1].assert_is_compatible_with(v.get_shape()[:-1])\n    else:\n      k.get_shape().assert_is_compatible_with(v.get_shape())\n\n    # move heads to batch dimension. This is needed to reduce number of\n    # dimensions as much as possible, since most ops support only up to 7\n    # dimensions.\n    q_shape = common_layers.shape_list(q)\n    k_shape = common_layers.shape_list(k)\n    v_shape = common_layers.shape_list(v)\n    q = tf.reshape(q, [-1] + q_shape[2:])\n    k = tf.reshape(k, [-1] + k_shape[2:])\n    v = tf.reshape(v, [-1] + v_shape[2:])\n\n    # Pad query, key, value to ensure multiple of corresponding lengths.\n    if decode_step is None:\n      # don't pad query in fast decoding mode. We only need to calculate self\n      # attention for one position.\n      q = pad_to_multiple_nd(q, query_shape)\n    k = pad_to_multiple_nd(k, query_shape)\n    v = pad_to_multiple_nd(v, query_shape)\n\n    # extract query and memory blocks\n    if decode_step is None:\n      q = break_into_blocks_nd(q, query_shape)\n    else:\n      # in fast decoding, q has 1 block with 1 item in it\n      # q shape will be [batch] + [1] * n + [1, depth] which is equivalent of\n      # [batch, b1, b2, ..., bn, items_in_block, depth] where there is 1 block\n      # and 1 item in that block\n      q = tf.reshape(q, [-1] + [1] * (len(q_shape) - 3) + [q_shape[-1]])\n    k = break_into_memory_blocks_nd(k, query_shape, memory_flange, masked=True)\n    v = break_into_memory_blocks_nd(v, query_shape, memory_flange, masked=True)\n\n    # extract just one block of k and v in fast decoding mode.\n    if decode_step is not None:\n      k = select_block_for_decode_step(k, decode_step, query_shape)\n      v = select_block_for_decode_step(v, decode_step, query_shape)\n\n    # flatten q, k and v to [batch, num_blocks, items_in_block, depth]\n    q, blocks_per_dim = flatten_blocks_nd(q)\n    k, _ = flatten_blocks_nd(k)\n    v, _ = flatten_blocks_nd(v)\n\n    # make attention bias for causal attention.\n    causal_attn_bias = causal_attention_bias_nd(\n        query_shape, memory_flange, decode_step=decode_step)\n    padding_attn_bias = tf.expand_dims(\n        embedding_to_padding(v[:1, :, :, :]) * -1e9, axis=-2)\n\n    if decode_step is None:\n      num_blocks = common_layers.shape_list(v)[1]\n      causal_attn_bias = tf.tile(causal_attn_bias, [1, num_blocks, 1, 1])\n      padding_attn_bias = tf.tile(\n          padding_attn_bias,\n          [1, 1, np.prod(query_shape, dtype=np.int32), 1])\n    attn_bias = tf.minimum(causal_attn_bias, padding_attn_bias)\n\n    # Calculate dot product attention\n    output = dot_product_attention(\n        q,\n        k,\n        v,\n        attn_bias,\n        dropout_rate=0.,\n        name=name or \"masked_local_nd\",\n        make_image_summary=False)\n\n    # restructure the output from blocks ordering to the original ordering\n    output = unflatten_blocks_nd(output, blocks_per_dim)\n    if decode_step is None:\n      # In fast decoding, output only contains one element, this is not needed.\n      output = put_back_blocks_nd(output, query_shape)\n\n    # bring back the heads dimension\n    output_shape = common_layers.shape_list(output)\n    output = tf.reshape(output, q_shape[:2] + output_shape[1:])\n    if decode_step is None:\n      # No padding is introduced in fast decoding, no need to do this.\n      output_shape = common_layers.shape_list(output)\n      output = tf.slice(output, [0] * len(output_shape),\n                        [-1, -1] + q_shape[2:-1] + [-1])\n    return output\n\n\ndef select_block_for_decode_step(blocked_x, decode_step, query_shape):\n  \"\"\"Selects one block from `x` that contains position `decode_step`.\n\n  NOTE: This method only works for blocked inputs. It selects one block around\n  `decode_step` position in blocked raster scan order.\n\n  Args:\n    blocked_x: a [batch, blocks_per_d1, ..., blocks_per_dn, b1 * ...* bn, depth]\n      tensor\n    decode_step: an integer\n    query_shape: a tuple (q1, q2, ..., qn) representing query shape\n\n  Returns:\n     a [batch, [1] * n, b1 * ... * bn, depth] tensor\n  \"\"\"\n  blocked_x_shape = common_layers.shape_list(blocked_x)\n  # calculate the shape of the normal x\n  x_shape = [b * q for b, q in zip(blocked_x_shape[1:-2], query_shape)]\n  # Get the position of `decode_step` element in the unblocked x.\n  index = decode_step_to_index(decode_step, query_shape, x_shape)\n  # Convert it to the blocked positions.\n  blocked_index = [i // q for i, q in zip(index, query_shape)]\n  # TPU needs size to be non negative for the case when begin is not\n  # compile-time constants.\n  return tf.slice(blocked_x, [0] + blocked_index + [0, 0],\n                  [blocked_x_shape[0]] + [1] * len(blocked_index) +\n                  blocked_x_shape[-2:])\n\n\ndef flatten_blocks_nd(x):\n  \"\"\"Flattens blocks of the input tensor.\n\n  Args:\n    x: a [batch, b1, ..., bn, items_in_block, depth] tensor\n\n  Returns:\n    a flattened tensor of shape [batch, b1 * ...* bm, items_in_block, depth]\n    a list of [b1, ..., bn] which is used for unflattening.\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  num_blocks = np.prod(x_shape[1:-2], dtype=np.int32)\n  return tf.reshape(x, [-1, num_blocks] + x_shape[-2:]), x_shape[1:-2]\n\n\ndef unflatten_blocks_nd(x, blocks_per_dimension):\n  \"\"\"Converts a flattened tensor into a normal blocked tensor.\n\n  Args:\n    x: a [batch, d1 * ... dn, items_in_block, depth] tensor\n    blocks_per_dimension: a n-d list of integers for number of blocks in each\n      dimension.\n\n  Returns:\n    a [batch, d1, d2, ..., dn, items_in_block, depth] tensor\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  assert x_shape[1] == np.prod(blocks_per_dimension, dtype=np.int32)\n  return tf.reshape(x, [-1] + list(blocks_per_dimension) + x_shape[-2:])\n\n\ndef break_into_memory_blocks_nd(x, query_shape, memory_flange, masked=False):\n  \"\"\"Break a tensor into memory blocks around query blocks.\n\n  This requires memory_flange to be divisible by query_shape in every dimension.\n\n  Args:\n    x: a [batch, d1, d2, ..., dn, depth] tensor\n    query_shape: a n-d list of integers representing query shape\n    memory_flange: an n-d list of integers representing memory flange.\n    masked: a boolean for masked vs unmasked attention.\n\n  Returns:\n    a [batch, blocks_per_d1, ..., blocks_per_dn, b1 * ...* bn, depth] where bi\n      is the memory block size in dimension i which is equal to q[i] + 2m[i] or\n      q[i] + m[i] if masked attention and i = 1.\n  \"\"\"\n  assert all([m % b == 0 for b, m in zip(query_shape, memory_flange)])\n\n  original_x_shape = common_layers.shape_list(x)\n  # calculate the total number of query blocks in each dimension\n  blocks_in_memory_flange = [m // b for b, m in zip(query_shape, memory_flange)]\n  num_query_blocks = [\n      l // q for l, q in zip(original_x_shape[1:-1], query_shape)\n  ]\n  # pad x to have enough items on the corners to form the  memory blocks.\n  if masked:\n    # Only pad the beginning of first dimension in masked mode.\n    x = tf.pad(x, [[0, 0], [memory_flange[0], 0]] +\n               [[p, p] for p in memory_flange[1:]] + [[0, 0]])\n  else:\n    x = tf.pad(x, [[0, 0]] + [[p, p] for p in memory_flange] + [[0, 0]])\n\n  query_blocks = break_into_blocks_nd(x, query_shape)\n  # stitch query blocks together to form memory blocks of the desired size.\n  start_indices_per_dimension = []\n  for dimension, blocks in enumerate(blocks_in_memory_flange):\n    if masked and dimension == 0:\n      # num blocks for first dimension in masked mode is blocks + 1\n      size = blocks + 1\n    else:\n      size = 2 * blocks + 1\n    start_indices_per_dimension.append(range(size))\n\n  slices = []\n  for start_indices in itertools.product(*start_indices_per_dimension):\n    start = [0] + list(start_indices) + [0, 0]\n    size = [-1] + num_query_blocks + [-1, -1]\n    s = tf.slice(query_blocks, start, size)\n    slices.append(s)\n  # concat slices in their query block dimension to form the full memory blocks\n  return tf.concat(slices, axis=-2)\n\n\ndef break_into_blocks_nd(x, block_shape):\n  \"\"\"Break input tensor into blocks of `block_shape`.\n\n  Args:\n    x: a [batch, d1, d2, ..., dn, depth] tensor\n    block_shape: a n-d list of integers representing block shape\n\n  Returns:\n    a [batch, d1//block1, ..., dn//blockn, block1 *... * blockn, depth] tensor\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  assert all([l % b == 0 for l, b in zip(x_shape[1:], block_shape)])\n  blocks_per_dimension = [l // b for l, b in zip(x_shape[1:], block_shape)]\n  # reshape to [-1, d1 // block1, block1, ..., dn // blockn, blockn, depth]\n  reshape_to = list(\n      itertools.chain.from_iterable(zip(blocks_per_dimension, block_shape)))\n  x = tf.reshape(x, [-1] + reshape_to + x_shape[-1:])\n  # transpose dimensions to bring the n-d blocks in consecutive dimensions.\n  block_dimensions_index = [2 * (i + 1) for i in range(len(block_shape))]\n  x = tf.transpose(x, [0] + [i - 1 for i in block_dimensions_index] +\n                   block_dimensions_index + [2 * len(block_shape) + 1])\n  return tf.reshape(x, [-1] + blocks_per_dimension +\n                    [np.prod(block_shape, dtype=np.int32)] + x_shape[-1:])\n\n\ndef put_back_blocks_nd(x, block_shape):\n  \"\"\"Restructure input tensor from blocks to normal ordering.\n\n  Args:\n    x: a [batch, b1, ..., bn, items_in_block, depth] tensor\n    block_shape: a n-d list of integers representing block shape.\n\n  Returns:\n    a [batch, d1, ..., dn, depth] where blocks are put back to form the\n      original tensor.\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  assert x_shape[-2] == np.prod(block_shape)\n  x = tf.reshape(x, x_shape[:-2] + list(block_shape) + x_shape[-1:])\n  block_dimension_index = [i + 1 for i in range(len(block_shape))]\n  block_shape_index = [b + len(block_shape) for b in block_dimension_index]\n  interleaved_dimensions = list(\n      itertools.chain.from_iterable(\n          zip(block_dimension_index, block_shape_index)))\n  x = tf.transpose(x, [0] + interleaved_dimensions + [2 * len(block_shape) + 1])\n  x_shape = common_layers.shape_list(x)\n  x = tf.reshape(x, [-1] + [\n      x_shape[2 * i + 1] * x_shape[2 * i + 2] for i in range(len(block_shape))\n  ] + x_shape[-1:])\n  return x\n\n\ndef pad_to_multiple_nd(x, block_shape):\n  \"\"\"Making sure x is a multiple of shape.\n\n  Args:\n    x: a [batch, d1, d2, ..., dn, depth] tensor\n    block_shape: a n-d list of integers representing block shape\n\n  Returns:\n    padded x where each dimension is a multiple of corresponding block length.\n  \"\"\"\n  shape = common_layers.shape_list(x)\n  paddings = [-l % b for l, b in zip(shape[1:-1], block_shape)]\n  return tf.pad(x, [[0, 0]] + [[0, p] for p in paddings] + [[0, 0]])\n\n\ndef causal_attention_bias_nd(query_shape, memory_flange, decode_step=None):\n  \"\"\"Creates causal attention bias for local nd attention.\n\n  This assumes memory_flange is divisible by query_shape in every dimension.\n\n  Args:\n    query_shape: a n-d list of integers representing query shape\n    memory_flange: a n-d list of integers representing memory flange\n    decode_step: an integer\n\n  Returns:\n    a [1, 1, query_items, memory_items] tensor for masked attention bias or\n    a [1, 1, 1, memory_items] tensor if decode_step is not None.\n  \"\"\"\n  assert all([m % q == 0 for q, m in zip(query_shape, memory_flange)])\n  blocks_per_memory_flange = [\n      m // q for q, m in zip(query_shape, memory_flange)\n  ]\n  # previous blocks will be half the number of all blocks if we select blocks\n  # to the left and right of center block in every dimension.\n  prev_blocks = np.prod([2 * b + 1 for b in blocks_per_memory_flange],\n                        dtype=np.int32) // 2\n  all_blocks = np.prod(\n      [blocks_per_memory_flange[0] + 1] +\n      [2 * b + 1 for b in blocks_per_memory_flange[1:]],\n      dtype=np.int32)\n  future_blocks = all_blocks - prev_blocks - 1\n  # add unmasked biases for all prev blocks and a lower triangle for the center\n  # block and all masked for future blocks.\n  items_in_block = np.prod(query_shape, dtype=np.int32)\n  items_in_query = items_in_block if decode_step is None else 1\n  prev_blocks_attn = tf.zeros(\n      [1, 1, items_in_query, prev_blocks * items_in_block])\n\n  # add mask for the center block\n  if decode_step is None:\n    center_block_attn = attention_bias_lower_triangle(items_in_block)\n  else:\n    step_in_block = decode_step % items_in_block\n    cond = tf.reshape(\n        tf.less_equal(tf.range(items_in_block, dtype=tf.int32), step_in_block),\n        [1, 1, items_in_query, items_in_block])\n    center_block_attn = tf.where(\n        cond, tf.zeros([1, 1, items_in_query, items_in_block]),\n        -1e9 * tf.ones([1, 1, items_in_query, items_in_block]))\n\n  # add mask for all future blocks\n  future_blocks_attn = -1e9 * tf.ones(\n      [1, 1, items_in_query, future_blocks * items_in_block])\n  return tf.concat([prev_blocks_attn, center_block_attn, future_blocks_attn],\n                   axis=3)\n\n\ndef compute_attention_component(antecedent,\n                                total_depth,\n                                filter_width=1,\n                                padding=\"VALID\",\n                                name=\"c\",\n                                vars_3d_num_heads=0,\n                                layer_collection=None):\n  \"\"\"Computes attention component (query, key or value).\n\n  Args:\n    antecedent: a Tensor with shape [batch, length, channels]\n    total_depth: an integer\n    filter_width: An integer specifying how wide you want the attention\n      component to be.\n    padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\n    name: a string specifying scope name.\n    vars_3d_num_heads: an optional integer (if we want to use 3d variables)\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\n      KFAC optimizer. Default is None.\n\n  Returns:\n    c : [batch, length, depth] tensor\n  \"\"\"\n  if layer_collection is not None:\n    if filter_width != 1 or vars_3d_num_heads != 0:\n      raise ValueError(\n          \"KFAC implementation only supports filter_width=1 (actual: {}) and \"\n          \"vars_3d_num_heads=0 (actual: {}).\".format(\n              filter_width, vars_3d_num_heads))\n  if vars_3d_num_heads is not None and vars_3d_num_heads > 0:\n    assert filter_width == 1\n    input_depth = antecedent.get_shape().as_list()[-1]\n    depth_per_head = total_depth // vars_3d_num_heads\n    initializer_stddev = input_depth ** -0.5\n    if \"q\" in name:\n      initializer_stddev *= depth_per_head ** -0.5\n    var = tf.get_variable(\n        name, [input_depth,\n               vars_3d_num_heads,\n               total_depth // vars_3d_num_heads],\n        initializer=tf.random_normal_initializer(stddev=initializer_stddev))\n    var = tf.cast(var, antecedent.dtype)\n    var = tf.reshape(var, [input_depth, total_depth])\n    return tf.tensordot(antecedent, var, axes=1)\n  if filter_width == 1:\n    return common_layers.dense(\n        antecedent, total_depth, use_bias=False, name=name,\n        layer_collection=layer_collection)\n  else:\n    return common_layers.conv1d(\n        antecedent, total_depth, filter_width, padding=padding, name=name)\n\n\ndef compute_qkv(query_antecedent,\n                memory_antecedent,\n                total_key_depth,\n                total_value_depth,\n                q_filter_width=1,\n                kv_filter_width=1,\n                q_padding=\"VALID\",\n                kv_padding=\"VALID\",\n                vars_3d_num_heads=0,\n                layer_collection=None):\n  \"\"\"Computes query, key and value.\n\n  Args:\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\n    memory_antecedent: a Tensor with shape [batch, length_m, channels]\n    total_key_depth: an integer\n    total_value_depth: an integer\n    q_filter_width: An integer specifying how wide you want the query to be.\n    kv_filter_width: An integer specifying how wide you want the keys and values\n    to be.\n    q_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\n    kv_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\n    vars_3d_num_heads: an optional (if we want to use 3d variables)\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\n      KFAC optimizer. Default is None.\n\n  Returns:\n    q, k, v : [batch, length, depth] tensors\n  \"\"\"\n  if memory_antecedent is None:\n    memory_antecedent = query_antecedent\n  q = compute_attention_component(\n      query_antecedent,\n      total_key_depth,\n      q_filter_width,\n      q_padding,\n      \"q\",\n      vars_3d_num_heads=vars_3d_num_heads,\n      layer_collection=layer_collection)\n  k = compute_attention_component(\n      memory_antecedent,\n      total_key_depth,\n      kv_filter_width,\n      kv_padding,\n      \"k\",\n      vars_3d_num_heads=vars_3d_num_heads,\n      layer_collection=layer_collection)\n  v = compute_attention_component(\n      memory_antecedent,\n      total_value_depth,\n      kv_filter_width,\n      kv_padding,\n      \"v\",\n      vars_3d_num_heads=vars_3d_num_heads,\n      layer_collection=layer_collection)\n  return q, k, v\n\n\ndef multihead_attention(query_antecedent,\n                        memory_antecedent,\n                        bias,\n                        total_key_depth,\n                        total_value_depth,\n                        output_depth,\n                        num_heads,\n                        dropout_rate,\n                        attention_type=\"dot_product\",\n                        max_relative_position=None,\n                        heads_share_relative_embedding=False,\n                        add_relative_to_values=False,\n                        image_shapes=None,\n                        block_length=128,\n                        block_width=128,\n                        q_filter_width=1,\n                        kv_filter_width=1,\n                        q_padding=\"VALID\",\n                        kv_padding=\"VALID\",\n                        cache=None,\n                        gap_size=0,\n                        num_memory_blocks=2,\n                        name=\"multihead_attention\",\n                        save_weights_to=None,\n                        make_image_summary=True,\n                        dropout_broadcast_dims=None,\n                        vars_3d=False,\n                        layer_collection=None,\n                        recurrent_memory=None,\n                        chunk_number=None,\n                        hard_attention_k=0,\n                        gumbel_noise_weight=0.0,\n                        max_area_width=1,\n                        max_area_height=1,\n                        memory_height=1,\n                        area_key_mode=\"mean\",\n                        area_value_mode=\"sum\",\n                        training=True,\n                        **kwargs):\n  \"\"\"Multihead scaled-dot-product attention with input/output transformations.\n\n  Args:\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\n    memory_antecedent: a Tensor with shape [batch, length_m, channels] or None\n    bias: bias Tensor (see attention_bias())\n    total_key_depth: an integer\n    total_value_depth: an integer\n    output_depth: an integer\n    num_heads: an integer dividing total_key_depth and total_value_depth\n    dropout_rate: a floating point number\n    attention_type: a string, either \"dot_product\", \"dot_product_relative\",\n                    \"local_mask_right\", \"local_unmasked\", \"masked_dilated_1d\",\n                    \"unmasked_dilated_1d\", graph, or any attention function\n                    with the signature (query, key, value, **kwargs)\n    max_relative_position: Maximum distance between inputs to generate\n                           unique relation embeddings for. Only relevant\n                           when using \"dot_product_relative\" attention.\n    heads_share_relative_embedding: boolean to share relative embeddings\n    add_relative_to_values: a boolean for whether to add relative component to\n                            values.\n    image_shapes: optional tuple of integer scalars.\n                  see comments for attention_image_summary()\n    block_length: an integer - relevant for \"local_mask_right\"\n    block_width: an integer - relevant for \"local_unmasked\"\n    q_filter_width: An integer specifying how wide you want the query to be.\n    kv_filter_width: An integer specifying how wide you want the keys and values\n                     to be.\n    q_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\n               kv_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is \"VALID\":\n               no padding.\n    cache: dict containing Tensors which are the results of previous\n           attentions, used for fast decoding. Expects the dict to contrain two\n           keys ('k' and 'v'), for the initial call the values for these keys\n           should be empty Tensors of the appropriate shape.\n               'k' [batch_size, 0, key_channels]\n               'v' [batch_size, 0, value_channels]\n    gap_size: Integer option for dilated attention to indicate spacing between\n              memory blocks.\n    num_memory_blocks: Integer option to indicate how many memory blocks to look\n                       at.\n    name: an optional string.\n    save_weights_to: an optional dictionary to capture attention weights\n      for vizualization; the weights tensor will be appended there under\n      a string key created from the variable scope (including name).\n    make_image_summary: Whether to make an attention image summary.\n    dropout_broadcast_dims:  an optional list of integers less than 4\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n    vars_3d: use 3-dimensional variables for input/output transformations\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\n      KFAC optimizer. Default is None.\n    recurrent_memory: An optional transformer_memory.RecurrentMemory, which\n      retains state across chunks. Default is None.\n    chunk_number: an optional integer Tensor with shape [batch] used to operate\n      the recurrent_memory.\n    hard_attention_k: integer, if > 0 triggers hard attention (picking top-k).\n    gumbel_noise_weight: if > 0, apply Gumbel noise with weight\n      `gumbel_noise_weight` before picking top-k. This is a no op if\n      hard_attention_k <= 0.\n    max_area_width: the max width allowed for an area.\n    max_area_height: the max height allowed for an area.\n    memory_height: the height of the memory.\n    area_key_mode: the mode for computing area keys, which can be \"mean\",\n      \"concat\", \"sum\", \"sample_concat\", and \"sample_sum\".\n    area_value_mode: the mode for computing area values, which can be either\n      \"mean\", or \"sum\".\n    training: indicating if it is in the training mode.\n    **kwargs (dict): Parameters for the attention function.\n\n  Caching:\n    WARNING: For decoder self-attention, i.e. when memory_antecedent == None,\n    the caching assumes that the bias contains future masking.\n\n    The caching works by saving all the previous key and value values so that\n    you are able to send just the last query location to this attention\n    function. I.e. if the cache dict is provided it assumes the query is of the\n    shape [batch_size, 1, hidden_dim] rather than the full memory.\n\n  Returns:\n    The result of the attention transformation. The output shape is\n        [batch_size, length_q, hidden_dim]\n    unless the cache dict is provided in which case only the last memory\n    position is calculated and the output shape is [batch_size, 1, hidden_dim]\n    Optionally returns an additional loss parameters (ex: load balance loss for\n    the experts) returned by the attention_type function.\n\n  Raises:\n    ValueError: if the key depth or value depth are not divisible by the\n      number of attention heads.\n  \"\"\"\n  if total_key_depth % num_heads != 0:\n    raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_key_depth, num_heads))\n  if total_value_depth % num_heads != 0:\n    raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_value_depth, num_heads))\n  vars_3d_num_heads = num_heads if vars_3d else 0\n\n  if layer_collection is not None:\n    if cache is not None:\n      raise ValueError(\"KFAC implementation only supports cache is None.\")\n    if vars_3d:\n      raise ValueError(\"KFAC implementation does not support 3d vars.\")\n\n  if recurrent_memory is not None:\n    if memory_antecedent is not None:\n      raise ValueError(\"Recurrent memory requires memory_antecedent is None.\")\n    if cache is not None:\n      raise ValueError(\"Cache is not supported when using recurrent memory.\")\n    if vars_3d:\n      raise ValueError(\"3d vars are not supported when using recurrent memory.\")\n    if layer_collection is not None:\n      raise ValueError(\"KFAC is not supported when using recurrent memory.\")\n    if chunk_number is None:\n      raise ValueError(\"chunk_number is required when using recurrent memory.\")\n\n  with tf.variable_scope(name, default_name=\"multihead_attention\",\n                         values=[query_antecedent, memory_antecedent]):\n\n    if recurrent_memory is not None:\n      (\n          recurrent_memory_transaction,\n          query_antecedent, memory_antecedent, bias,\n      ) = recurrent_memory.pre_attention(\n          chunk_number,\n          query_antecedent, memory_antecedent, bias,\n      )\n\n    if cache is None or memory_antecedent is None:\n      q, k, v = compute_qkv(query_antecedent, memory_antecedent,\n                            total_key_depth, total_value_depth, q_filter_width,\n                            kv_filter_width, q_padding, kv_padding,\n                            vars_3d_num_heads=vars_3d_num_heads,\n                            layer_collection=layer_collection)\n    if cache is not None:\n      if attention_type not in [\"dot_product\", \"dot_product_relative\"]:\n        # TODO(petershaw): Support caching when using relative position\n        # representations, i.e. \"dot_product_relative\" attention.\n        raise NotImplementedError(\n            \"Caching is not guaranteed to work with attention types other than\"\n            \" dot_product.\")\n      if bias is None:\n        raise ValueError(\"Bias required for caching. See function docstring \"\n                         \"for details.\")\n\n      if memory_antecedent is not None:\n        # Encoder-Decoder Attention Cache\n        q = compute_attention_component(query_antecedent, total_key_depth,\n                                        q_filter_width, q_padding, \"q\",\n                                        vars_3d_num_heads=vars_3d_num_heads)\n        k = cache[\"k_encdec\"]\n        v = cache[\"v_encdec\"]\n      else:\n        k = split_heads(k, num_heads)\n        v = split_heads(v, num_heads)\n        decode_loop_step = kwargs.get(\"decode_loop_step\")\n        if decode_loop_step is None:\n          k = cache[\"k\"] = tf.concat([cache[\"k\"], k], axis=2)\n          v = cache[\"v\"] = tf.concat([cache[\"v\"], v], axis=2)\n        else:\n          # Inplace update is required for inference on TPU.\n          # Inplace_ops only supports inplace_update on the first dimension.\n          # The performance of current implementation is better than updating\n          # the tensor by adding the result of matmul(one_hot,\n          # update_in_current_step)\n          tmp_k = tf.transpose(cache[\"k\"], perm=[2, 0, 1, 3])\n          tmp_k = inplace_ops.alias_inplace_update(\n              tmp_k, decode_loop_step, tf.squeeze(k, axis=2))\n          k = cache[\"k\"] = tf.transpose(tmp_k, perm=[1, 2, 0, 3])\n          tmp_v = tf.transpose(cache[\"v\"], perm=[2, 0, 1, 3])\n          tmp_v = inplace_ops.alias_inplace_update(\n              tmp_v, decode_loop_step, tf.squeeze(v, axis=2))\n          v = cache[\"v\"] = tf.transpose(tmp_v, perm=[1, 2, 0, 3])\n\n    q = split_heads(q, num_heads)\n    if cache is None:\n      k = split_heads(k, num_heads)\n      v = split_heads(v, num_heads)\n\n    key_depth_per_head = total_key_depth // num_heads\n    if not vars_3d:\n      q *= key_depth_per_head**-0.5\n\n    additional_returned_value = None\n    if callable(attention_type):  # Generic way to extend multihead_attention\n      x = attention_type(q, k, v, **kwargs)\n      if isinstance(x, tuple):\n        x, additional_returned_value = x  # Unpack\n    elif attention_type == \"dot_product\":\n      if max_area_width > 1 or max_area_height > 1:\n        x = area_attention.dot_product_area_attention(\n            q, k, v, bias, dropout_rate, image_shapes,\n            save_weights_to=save_weights_to,\n            dropout_broadcast_dims=dropout_broadcast_dims,\n            max_area_width=max_area_width,\n            max_area_height=max_area_height,\n            memory_height=memory_height,\n            area_key_mode=area_key_mode,\n            area_value_mode=area_value_mode,\n            training=training)\n      else:\n        x = dot_product_attention(\n            q, k, v, bias, dropout_rate, image_shapes,\n            save_weights_to=save_weights_to,\n            make_image_summary=make_image_summary,\n            dropout_broadcast_dims=dropout_broadcast_dims,\n            activation_dtype=kwargs.get(\"activation_dtype\"),\n            hard_attention_k=hard_attention_k,\n            gumbel_noise_weight=gumbel_noise_weight)\n    elif attention_type == \"dot_product_relative\":\n      x = dot_product_attention_relative(\n          q,\n          k,\n          v,\n          bias,\n          max_relative_position,\n          dropout_rate,\n          image_shapes,\n          save_weights_to=save_weights_to,\n          make_image_summary=make_image_summary,\n          cache=cache is not None,\n          allow_memory=recurrent_memory is not None,\n          hard_attention_k=hard_attention_k,\n          gumbel_noise_weight=gumbel_noise_weight)\n    elif attention_type == \"dot_product_unmasked_relative_v2\":\n      x = dot_product_unmasked_self_attention_relative_v2(\n          q,\n          k,\n          v,\n          bias,\n          max_relative_position,\n          dropout_rate,\n          image_shapes,\n          save_weights_to=save_weights_to,\n          make_image_summary=make_image_summary,\n          dropout_broadcast_dims=dropout_broadcast_dims,\n          heads_share_relative_embedding=heads_share_relative_embedding,\n          add_relative_to_values=add_relative_to_values)\n    elif attention_type == \"dot_product_relative_v2\":\n      x = dot_product_self_attention_relative_v2(\n          q,\n          k,\n          v,\n          bias,\n          max_relative_position,\n          dropout_rate,\n          image_shapes,\n          save_weights_to=save_weights_to,\n          make_image_summary=make_image_summary,\n          dropout_broadcast_dims=dropout_broadcast_dims,\n          heads_share_relative_embedding=heads_share_relative_embedding,\n          add_relative_to_values=add_relative_to_values)\n    elif attention_type == \"local_within_block_mask_right\":\n      x = masked_within_block_local_attention_1d(\n          q, k, v, block_length=block_length)\n    elif attention_type == \"local_relative_mask_right\":\n      x = masked_relative_local_attention_1d(\n          q,\n          k,\n          v,\n          block_length=block_length,\n          make_image_summary=make_image_summary,\n          dropout_rate=dropout_rate,\n          heads_share_relative_embedding=heads_share_relative_embedding,\n          add_relative_to_values=add_relative_to_values,\n          name=\"masked_relative_local_attention_1d\")\n    elif attention_type == \"local_mask_right\":\n      x = masked_local_attention_1d(\n          q,\n          k,\n          v,\n          block_length=block_length,\n          make_image_summary=make_image_summary)\n    elif attention_type == \"local_unmasked\":\n      x = local_attention_1d(\n          q, k, v, block_length=block_length, filter_width=block_width)\n    elif attention_type == \"masked_dilated_1d\":\n      x = masked_dilated_self_attention_1d(q, k, v, block_length, block_width,\n                                           gap_size, num_memory_blocks)\n    else:\n      assert attention_type == \"unmasked_dilated_1d\"\n      x = dilated_self_attention_1d(q, k, v, block_length, block_width,\n                                    gap_size, num_memory_blocks)\n    x = combine_heads(x)\n\n    # Set last dim specifically.\n    x.set_shape(x.shape.as_list()[:-1] + [total_value_depth])\n\n    if vars_3d:\n      o_var = tf.get_variable(\n          \"o\", [num_heads, total_value_depth // num_heads, output_depth])\n      o_var = tf.cast(o_var, x.dtype)\n      o_var = tf.reshape(o_var, [total_value_depth, output_depth])\n      x = tf.tensordot(x, o_var, axes=1)\n    else:\n      x = common_layers.dense(\n          x, output_depth, use_bias=False, name=\"output_transform\",\n          layer_collection=layer_collection)\n\n    if recurrent_memory is not None:\n      x = recurrent_memory.post_attention(recurrent_memory_transaction, x)\n    if additional_returned_value is not None:\n      return x, additional_returned_value\n    return x\n\n\ndef multihead_attention_2d(query_antecedent,\n                           memory_antecedent,\n                           total_key_depth,\n                           total_value_depth,\n                           output_depth,\n                           num_heads,\n                           attention_type=\"local_attention_2d\",\n                           query_shape=(8, 16),\n                           memory_flange=(8, 16),\n                           name=None):\n  \"\"\"2d Multihead scaled-dot-product attention with inp/output transformations.\n\n  Args:\n    query_antecedent: a Tensor with shape [batch, h, w, depth_k]\n    memory_antecedent: a Tensor with shape [batch, h, w, depth_k]\n    total_key_depth: an integer\n    total_value_depth: an integer\n    output_depth: an integer\n    num_heads: an integer dividing total_key_depth and total_value_depth\n    attention_type: String, type of attention function to use.\n    query_shape: an tuple indicating the height and width of each query block.\n    memory_flange: an integer indicating how much to look in height and width\n    name: an optional string\n\n  Returns:\n    A Tensor of shape [batch, h, w, output_depth]\n\n  Raises:\n    ValueError: if the key depth or value depth are not divisible by the\n      number of attention heads.\n  \"\"\"\n  if total_key_depth % num_heads != 0:\n    raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_key_depth, num_heads))\n  if total_value_depth % num_heads != 0:\n    raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_value_depth, num_heads))\n  with tf.variable_scope(\n      name,\n      default_name=\"multihead_attention_2d\",\n      values=[query_antecedent, memory_antecedent]):\n    q, k, v = compute_qkv(query_antecedent, memory_antecedent, total_key_depth,\n                          total_value_depth)\n    # after splitting, shape is [batch, heads, h, w, depth]\n    q = split_heads_2d(q, num_heads)\n    k = split_heads_2d(k, num_heads)\n    v = split_heads_2d(v, num_heads)\n    key_depth_per_head = total_key_depth // num_heads\n    q *= key_depth_per_head**-0.5\n    if attention_type == \"local_attention_2d\":\n      x = local_attention_2d(\n          q, k, v, query_shape=query_shape, memory_flange=memory_flange)\n    elif attention_type == \"masked_local_attention_2d\":\n      assert attention_type == \"masked_local_attention_2d\"\n      x = masked_local_attention_2d(\n          q, k, v, query_shape=query_shape, memory_flange=memory_flange)\n    else:\n      assert attention_type == \"unmasked_local_attention_2d_tpu\"\n      x = dot_product_unmasked_attention_local_2d_tpu(\n          q, k, v, None, max_relative_position=None, query_shape=query_shape)\n    x = combine_heads_2d(x)\n    x = common_layers.dense(\n        x, output_depth, use_bias=False, name=\"output_transform\")\n    return x\n\n\ndef multihead_attention_nd(query_antecedent,\n                           memory_antecedent,\n                           total_key_depth,\n                           total_value_depth,\n                           output_depth,\n                           num_heads,\n                           query_shape,\n                           memory_flange,\n                           masked=False,\n                           cache=None,\n                           decode_step=None,\n                           name=None):\n  \"\"\"n-d Multihead scaled-dot-product attention with in/output transformations.\n\n  Args:\n    query_antecedent: a Tensor with shape [batch, d1, ..., dn, depth_q] or\n      [batch, 1, ..., 1, depth_q] if in fast decoding mode.\n    memory_antecedent: a Tensor with shape [batch, d1, ..., dn, depth_m] or None\n      for self attention.\n    total_key_depth: an integer\n    total_value_depth: an integer\n    output_depth: an integer\n    num_heads: an integer dividing total_key_depth and total_value_depth\n    query_shape: an tuple indicating the dimensions of each query block.\n    memory_flange: an integer indicating how much to look around a query block\n      in each dimension\n    masked: a boolean to specify whether to do masked or unmasked attention.\n    cache: a dict like: {\n      'k': [batch, num_heads, d1, ..., dn, depth_k // num_heads],\n      'v': [batch, num_heads, d1, ..., dn, depth_v // num_heads]} Caller should\n        initially pass zero tensors for `decode_step` == 0. This method will\n        update cache and caller should pass the same cache in consecutive calls.\n        This works for both GPU and TPU inference. Caller should pass the latest\n        query via `query_antecedent`. `memory_antecedent` should be None in this\n        case, since auto-regressive decoding only applies to self attention.\n    decode_step: integer to pass in decoding mode. `cache` and `decode_step`\n      should both be set in decoding mode. Caller can also pass an empty `cache`\n      without `decode_step`, for this method to initialize the cache for future\n      calls with `decode_step` > 0.\n    name: an optional string\n\n  Returns:\n    A Tensor of shape [batch, d1, ..., dn, output_depth] or\n    [batch, 1, ..., 1, output_depth] if decode_step is set.\n\n  Raises:\n    ValueError: if the key depth or value depth are not divisible by the\n      number of attention heads.\n  \"\"\"\n  if total_key_depth % num_heads != 0:\n    raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_key_depth, num_heads))\n  if total_value_depth % num_heads != 0:\n    raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n                     \"attention heads (%d).\" % (total_value_depth, num_heads))\n  # Validate decoding input params are sensible.\n  if decode_step is not None:\n    assert \"k\" in cache and \"v\" in cache\n  if cache is not None:\n    assert memory_antecedent is None\n\n  with tf.variable_scope(\n      name,\n      default_name=\"multihead_attention_nd\",\n      values=[query_antecedent, memory_antecedent]):\n    if decode_step is not None:\n      latest_antecedent = query_antecedent\n      q, latest_k, latest_v = compute_qkv(latest_antecedent, None,\n                                          total_key_depth, total_value_depth)\n      latest_k = split_heads_nd(latest_k, num_heads)\n      latest_v = split_heads_nd(latest_v, num_heads)\n      # put latest k and v into their correct position in cache.\n      k = cache[\"k\"]\n      v = cache[\"v\"]\n      k = put_item_in_decode_step(k, latest_k, decode_step, query_shape)\n      v = put_item_in_decode_step(v, latest_v, decode_step, query_shape)\n      cache[\"k\"] = k\n      cache[\"v\"] = v\n\n    else:\n      q, k, v = compute_qkv(query_antecedent, memory_antecedent,\n                            total_key_depth, total_value_depth)\n      k = split_heads_nd(k, num_heads)\n      v = split_heads_nd(v, num_heads)\n      if cache is not None:\n        cache[\"k\"] = k\n        cache[\"v\"] = v\n    # after splitting, shape is [batch, heads, d1, ..., dn, depth]\n    q = split_heads_nd(q, num_heads)\n    key_depth_per_head = total_key_depth // num_heads\n    q *= key_depth_per_head**-0.5\n    if masked:\n      x = masked_local_attention_nd(\n          q,\n          k,\n          v,\n          query_shape=query_shape,\n          memory_flange=memory_flange,\n          decode_step=decode_step)\n    else:\n      raise NotImplementedError(\n          \"Unmaked multihead attention nd is not implemented\")\n    x = combine_heads_nd(x)\n    x = common_layers.dense(\n        x, output_depth, use_bias=False, name=\"output_transform\")\n    return x\n\n\ndef decode_step_to_index(decode_step, query_shape, tensor_shape):\n  \"\"\"Maps decode step to n-d index according to blocked raster scan order.\n\n  Args:\n    decode_step: an integer\n    query_shape: a tuple (q1, q2, ..., qn) representing the query shape\n    tensor_shape: a tuple (d1, d2, ..., dn) representing the tensor shape, minus\n      the batch and depth dimensions.\n\n  Returns:\n    a tuple (i1, i2, ..., in) representing the index of the element at\n    `decode_step` w.r.t. blocked raster scan order.\n  \"\"\"\n  assert len(query_shape) == len(tensor_shape)\n  blocks_per_dimension = [t // q for t, q in zip(tensor_shape, query_shape)]\n  items_in_block = np.prod(query_shape, dtype=np.int32)\n  step_block = decode_step // items_in_block\n  step_within_block = decode_step % items_in_block\n\n  block_index = []\n  for q in blocks_per_dimension[::-1]:\n    block_index.insert(0, step_block % q)\n    step_block //= q\n\n  within_block_index = []\n  for q in query_shape[::-1]:\n    within_block_index.insert(0, step_within_block % q)\n    step_within_block //= q\n\n  final_index = [\n      w + b * q for w, b, q in zip(within_block_index, block_index, query_shape)\n  ]\n  return tuple(final_index)\n\n\ndef get_item_at_decode_step(x, decode_step, query_shape):\n  \"\"\"Extracts a single item from an n-d tensor at `decode_step` position.\n\n  Args:\n    x: a [batch, d1, d2, ..., dn, depth] tensor\n    decode_step: an integer\n    query_shape: a tuple (q1, q2, ..., qn) representing the query shape\n\n  Returns:\n    a [batch, 1, 1, ..., 1, depth] tensor that is a single element from `x` at\n    `decode_step` w.r.t. blocked raster scan order.\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  index = decode_step_to_index(decode_step, query_shape, x_shape[1:-1])\n  # TPU needs size to be non negative for the case when begins are not\n  # compile-time constants.\n  return tf.slice(x, [0] + list(index) + [0],\n                  [x_shape[0]] + [1] * len(index) + [x_shape[-1]])\n\n\ndef put_item_in_decode_step(x, item, decode_step, query_shape):\n  \"\"\"Puts a single item into an n-d tensor at `decode_step` position.\n\n  Args:\n    x: a [batch, heads, d1, d2, ..., dn, depth] tensor\n    item: a [batch, heads, 1, 1, ..., 1, depth] tensor\n    decode_step: an integer\n    query_shape: a tuple (q1, q2, ..., qn) representing the query shape\n\n  Returns:\n    a [batch, heads, d1, d2, ..., dn, depth] tensor with value at `decode_step`\n    w.r.t. blocked raster scan order is updated to be `item`.\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  index = decode_step_to_index(decode_step, query_shape, x_shape[2:-1])\n  # inplace_update only works on the first dimension, we need to flatten and\n  # move batch to be the second dimension.\n  flattened_x = tf.reshape(\n      x, [-1, x_shape[1], np.prod(x_shape[2:-1]), x_shape[-1]])\n  # transpose to [positions, batch, heads, depth]\n  flattened_x = tf.transpose(flattened_x, [2, 0, 1, 3])\n\n  flattened_index = 0\n  factor = 1\n  for d, idx in zip(x_shape[-2:1:-1], index[::-1]):\n    flattened_index += idx * factor\n    factor *= d\n\n  item_shape = common_layers.shape_list(item)\n  item = tf.reshape(item, item_shape[:2] + item_shape[-1:])\n  updated_x = inplace_ops.alias_inplace_update(flattened_x, flattened_index,\n                                               item)\n  # unflatten the results\n  updated_x = tf.transpose(updated_x, [1, 2, 0, 3])\n  return tf.reshape(updated_x, [-1, x_shape[1]] + x_shape[2:])\n\n\ndef ffn_self_attention_layer(x,\n                             filter_depth,\n                             output_depth,\n                             num_parts,\n                             dropout_rate,\n                             share_kv=False,\n                             name=None):\n  \"\"\"Self-attention feedforward layer.\n\n  We use self-attention to do feedforward computations. We apply this function\n  positionwise where for each position, we linearly transform the output to have\n  depth filter_depth, and break up the result depth-wise into num_parts\n  contiguous parts. The parts self-attend, we concatenate the results\n  depth-wise, and we linearly transform to a depth of output_depth. The goal is\n  to get multiplicative interactions between components of a representation.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    filter_depth: an integer\n    output_depth: an integer\n    num_parts: an integer dividing filter depth\n    dropout_rate: a floating point number\n    share_kv: Share the key value transform\n    name: an optional string\n\n  Returns:\n    A Tensor with shape [batch, length, output_depth].\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"feedforward_self_attention\", values=[x]):\n    x_shape = common_layers.shape_list(x)\n    part_depth = filter_depth // num_parts\n    if not share_kv:\n      combined = common_layers.dense(\n          x, filter_depth * 3, use_bias=False, name=\"qkv_transform\")\n      combined = tf.expand_dims(combined, axis=2)\n      q, k, v = tf.split(combined, 3, axis=3)\n    else:\n      q = tf.expand_dims(\n          common_layers.dense(\n              x, filter_depth, use_bias=False, name=\"q_transform\"),\n          axis=2)\n      kv_combined = tf.expand_dims(\n          common_layers.dense(\n              tf.concat([x, x], axis=1),\n              filter_depth,\n              use_bias=False,\n              name=\"kv_transform\"),\n          axis=2)\n      k, v = tf.split(kv_combined, [x_shape[1], x_shape[1]], axis=1)\n\n    batch_q = tf.reshape(q, [-1, 1, num_parts, part_depth])\n    batch_k = tf.reshape(k, [-1, 1, num_parts, part_depth])\n    batch_v = tf.reshape(v, [-1, 1, num_parts, part_depth])\n\n    batch_q *= part_depth**-0.5\n    # non-masked bias\n    bias = None\n    x = dot_product_attention(batch_q, batch_k, batch_v, bias, dropout_rate)\n    x = tf.reshape(x, [x_shape[0], x_shape[1], filter_depth])\n    x = common_layers.dense(\n        x, output_depth, use_bias=False, name=\"output_transform\")\n    return x\n\n\ndef parameter_attention(x,\n                        total_key_depth,\n                        total_value_depth,\n                        output_depth,\n                        memory_rows,\n                        num_heads,\n                        dropout_rate,\n                        name=None):\n  \"\"\"Attention over parameters.\n\n  We use the same multi-headed attention as in the other layers, but the memory\n  keys and values are model parameters. There are no linear transformation on\n  the keys or values.\n\n  We are also a bit more careful about memory usage, since the number of\n  memory positions may be very large.\n\n  Args:\n    x: a Tensor with shape [batch, length_q, channels]\n    total_key_depth: an integer\n    total_value_depth: an integer\n    output_depth: an integer\n    memory_rows: an integer\n    num_heads: an integer dividing total_key_depth and total_value_depth\n    dropout_rate: a floating point number\n    name: an optional string\n\n  Returns:\n    A Tensor with shape [batch, length_q, output_depth].\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"parameter_attention\", values=[x]):\n    head_size_k = total_key_depth // num_heads\n    head_size_v = total_value_depth // num_heads\n    var_shape_k = [num_heads, memory_rows, head_size_k]\n    var_shape_v = [num_heads, memory_rows, head_size_v]\n    k = tf.get_variable(\n        \"k\",\n        var_shape_k,\n        initializer=tf.random_normal_initializer(\n            0, output_depth**-0.5 * (num_heads**0.5)))\n    v = tf.get_variable(\n        \"v\",\n        var_shape_v,\n        initializer=tf.random_normal_initializer(\n            0, output_depth**-0.5 * (output_depth**0.5)))\n    batch_size = common_layers.shape_list(x)[0]\n    length = common_layers.shape_list(x)[1]\n    q = common_layers.dense(\n        x, total_key_depth, use_bias=False, name=\"q_transform\")\n    if dropout_rate:\n      # This is a cheaper form of attention dropout where we use to use\n      # the same dropout decisions across batch elements and query positions,\n      # but different decisions across heads and memory positions.\n      v = tf.nn.dropout(\n          v, 1.0 - dropout_rate, noise_shape=[num_heads, memory_rows, 1])\n    # query is [batch, length, hidden_size]\n    # reshape and transpose it to [heads, batch * length, head_size]\n    q = tf.reshape(q, [batch_size, length, num_heads, head_size_k])\n    q = tf.transpose(q, [2, 0, 1, 3])\n    q = tf.reshape(q, [num_heads, batch_size * length, head_size_k])\n    weights = tf.matmul(q, k, transpose_b=True)\n    weights = tf.nn.softmax(weights)\n    y = tf.matmul(weights, v)\n    y = tf.reshape(y, [num_heads, batch_size, length, head_size_v])\n    y = tf.transpose(y, [1, 2, 0, 3])\n    y = tf.reshape(y, [batch_size, length, total_value_depth])\n    y.set_shape([None, None, total_value_depth])\n    y = common_layers.dense(\n        y, output_depth, use_bias=False, name=\"output_transform\")\n    return y\n\n\n@expert_utils.add_name_scope()\ndef coordinate_tensor(shape, axis):\n  \"\"\"Return a tensor with given shape containing coordinate along given axis.\n\n  Args:\n    shape: a Tensor representing the shape of the output Tensor\n    axis: an integer\n\n  Returns:\n    A tensor with shape shape and type tf.int32, where each elements its\n    coordinate along the given axis.\n  \"\"\"\n  if axis < 0:\n    axis = tf.size(shape) + axis  # Convert to positive for the one_hot indice\n\n  r = tf.range(shape[axis])\n  r_shape = tf.one_hot(\n      axis, tf.size(shape), on_value=-1, off_value=1, dtype=tf.int32)\n  return tf.zeros(shape, dtype=tf.int32) + tf.reshape(r, r_shape)\n\n\ndef self_attention_expert(x,\n                          batch_coordinate,\n                          mask_right=True,\n                          split_batch=False,\n                          attention_num_head=1,\n                          attention_kq_size=None,\n                          attention_v_size=None):\n  \"\"\"Implementing attention that runs inside each expert.\n\n  Args:\n    x: A tensor of shape[batch, depth]. Contains representations from\n      different positions, which are lexicographically ordered.\n    batch_coordinate: A tensor of shape [batch, 1] containing the batch\n      coordinate of each element in x. This is needed to make sure that\n      positions from different sequences don't attend to each other.\n    mask_right: A bool. If true, we will not attend to positions on the right,\n      just as decoder self attention.\n    split_batch (bool): If True, each sequence of the batch is processed\n      individually on a loop. If False, the sequences are processed all at\n      once and a mask is applied to isolate the sequences from each others\n    attention_num_head (int): number of attention heads\n    attention_kq_size (int): dimension used for the attention key, and query\n    attention_v_size (int): dimension used for the attention value\n\n  Returns:\n    out: A tensor of shape [batch, depth].\n  example use:\n  expert_utils.local_moe(\n     ...\n     expert_fn=functools.partial(self_attention_expert, mask_right=)\n     )\n  \"\"\"\n\n  depth = x.get_shape().as_list()[-1]\n  length = common_layers.shape_list(batch_coordinate)[0]\n\n  # Print a warning message if one of the expert isn't used (useful at\n  # inference where summaries aren't used and the gating function don't add\n  # noise)\n  global _expert_count  # Hack to make each expert have a unique id\n  _expert_count += 1\n  length = tf.cond(\n      tf.equal(length, 0),\n      lambda: tf.Print(  # pylint: disable=g-long-lambda\n          length, [length], \"Expert {} empty: \".format(_expert_count)),\n      lambda: length,\n  )\n\n  tf.summary.scalar(\"batch_size\", length, family=\"experts_stats_batch_size\")\n\n  attention_kq_size = attention_kq_size or depth\n  attention_v_size = attention_v_size or depth\n\n  def length_not_null(x, batch_coordinate):\n    \"\"\"Branch of the graph only evaluated when length isn't null.\"\"\"\n\n    # Mask between the sequences (not used if map_ids is used)\n    bias_batch = attention_bias_coordinates(batch_coordinate)\n\n    def add_or_set_if(prev_bias, new_bias, condition):\n      \"\"\"Add the bias together while considering the None case.\"\"\"\n      if not condition:\n        return prev_bias\n      if prev_bias is None:\n        return new_bias\n      return prev_bias + new_bias\n\n    def mask_and_call_attention(x):\n      \"\"\"Function applied once for each sequence of the batch.\"\"\"\n\n      # Mask to prevent sequences of attending to the future\n      length = common_layers.shape_list(x)[1]  # x has shape [1, length,...]\n      bias_past = tf.reshape(\n          attention_bias_lower_triangle(length), [length, length])\n      # bias has shape [length, length]\n\n      bias = None\n      bias = add_or_set_if(bias, bias_past, mask_right)\n      bias = add_or_set_if(bias, bias_batch, not split_batch)\n      bias = tf.reshape(bias, [1, 1, length, length])\n\n      return multihead_attention(\n          x,\n          None,\n          bias,\n          total_key_depth=attention_kq_size,\n          total_value_depth=attention_v_size,\n          output_depth=depth,\n          num_heads=attention_num_head,\n          dropout_rate=0.0)\n\n    if split_batch:\n      out = expert_utils.map_ids(x, batch_coordinate, mask_and_call_attention)\n    else:\n      x = tf.reshape(x, [1, length, depth])\n      out = mask_and_call_attention(x)\n      out = tf.squeeze(out, 0)\n    return out\n\n  # If the length is empty, just forward an empty tensor (avoid having to\n  # evaluate multihead_attention with tensor having dim equal to zeros)\n  out = tf.cond(\n      tf.equal(length, 0),\n      lambda: tf.zeros(shape=[0, depth], dtype=tf.float32, name=\"empty_out\"),\n      lambda: length_not_null(x, batch_coordinate),\n  )\n  return out\n\n\ndef local_expert_attention(x,\n                           k,\n                           loss_coef,\n                           attention_num_experts,\n                           train=True,\n                           batch_coordinate=None,\n                           **kwargs):\n  \"\"\"Attention using a mixture of experts.\n\n    Positions sent to the same expert can attend to each other.\n    The mixture of experts is \"local\" in that it is replicated on each\n    datashard.\n\n    local_moe flatten all batches so to avoid problems with padding (ex: all\n    padding going to the same expert, self attention attending to non null\n    padding tokens,...), the padding should be removed before.\n\n  Args:\n    x: a Tensor with shape [batch, length, depth] or [1, batch*length, depth]\n    k: The number of experts to dispatch each example to\n    loss_coef: a scalar. A multiplier for the expert loss\n    attention_num_experts: The number of experts to use\n    train: a boolean for the current mode\n    batch_coordinate (tf.Tensor): int32 tensor of shape [1, batch*length, 1]\n      containing the batch ids. If None, deduced from first dim of x.\n    **kwargs: Arguments to forward to self_attention_expert\n\n  Returns:\n    y: a Tensor with shape [batch, length, depth]\n    loss: a Scalar\n  \"\"\"\n  if batch_coordinate is None:\n    batch_coordinate = tf.expand_dims(\n        coordinate_tensor(common_layers.shape_list(x)[:-1], axis=0), axis=-1)\n  with tf.variable_scope(\"local_expert_attention\"):\n    additional_dispatch_params = {\"batch_coordinate\": batch_coordinate}\n    return expert_utils.local_moe(\n        x,\n        train,\n        functools.partial(self_attention_expert, **kwargs),\n        attention_num_experts,\n        k=k,\n        loss_coef=loss_coef,\n        pass_x=True,\n        pass_gates=False,\n        additional_dispatch_params=additional_dispatch_params,\n    )\n\n\n@expert_utils.add_name_scope()\ndef expert_dot_product(q, k, v, info_q, info_k):\n  \"\"\"Perform dot product on a subset of the sequence.\n\n  Can add a mask to the attention to prevent sequences to attend to each other\n  and to prevent attention to the future.\n\n  Args:\n    q (tf.Tensor): Queries of shape [length_expert_q, depth_k]\n    k (tf.Tensor): Keys of shape [length_expert_k, depth_k]\n    v (tf.Tensor): Values of shape [length_expert_k, depth_v]\n    info_q (BatchInfo): Batch info for queries. If None, no mask is added\n    info_k (BatchInfo): Batch info for keys\n\n  Returns:\n    tf.Tensor: dot product attention output ([length_expert_q, depth_v])\n  \"\"\"\n\n  length_q = common_layers.shape_list(q)[0]\n  length_k = common_layers.shape_list(k)[0]\n  depth_v = v.get_shape().as_list()[-1]\n\n  # Create the mask\n  bias = attention_bias_coordinates(info_q.coordinates, info_k.coordinates)\n  if info_k.order is not None:\n    bias += attention_bias_future(info_q.order, info_k.order)\n\n  # Restore batch and head dimension\n  q, k, v = [tf.expand_dims(tf.expand_dims(t, 0), 0) for t in (q, k, v)]\n\n  def is_zero():\n    zeros = tf.zeros(shape=[1, 1, length_q, depth_v], dtype=tf.float32)\n    zeros = tf.Print(zeros, [length_k, length_q], \"length_k/length_q: \")\n    return zeros\n\n  def is_not_zero():\n    return dot_product_attention(\n        q,\n        k,\n        v,\n        bias=bias,\n        # No image summary to avoid \"Retval[0] does not have value\" (because\n        # inside a condition)\n        make_image_summary=False,\n    )\n\n  # TODO(epot): Should make sure a query gets at least one key. Because the\n  # different sequences of a batch are merged, it's possible that a\n  # query from a sequence only receive memory from another sequence, so\n  # with the mask, the query will perform a softmax on -infinity values.\n  # A hack could be to add at least one sequence of each batch on each group so\n  # the query can attend to at least one element.\n  # Softmax(Q.K)*V\n  v_out = tf.cond(\n      tf.logical_or(tf.equal(length_q, 0), tf.equal(length_k, 0)),\n      is_zero,\n      is_not_zero,\n  )\n\n  # Remove batch and head dimension\n  v_out = tf.squeeze(v_out, axis=0)\n  v_out = tf.squeeze(v_out, axis=0)\n  return v_out\n\n\n@expert_utils.add_name_scope()\ndef dot_product_single_head(q, k, v, gates_q, gates_k, bi):\n  \"\"\"Perform a dot product attention on a single sequence on a single head.\n\n  This function dispatch the q, k, v and loop over the buckets to compute the\n  attention dot product on each subsequences.\n\n  Args:\n    q (tf.Tensor): [length_q, depth_q]\n    k (tf.Tensor): [length_k, depth_q]\n    v (tf.Tensor): [length_k, depth_v]\n    gates_q (tf.Tensor): One-hot vector of shape [length_q, nb_buckets]\n    gates_k (tf.Tensor): One-hot vector of shape [length_k, nb_buckets]\n    bi (BatchInfo): Contains the batch coordinates and sequence order\n\n  Returns:\n    tf.Tensor: [length_q, depth_v]\n  \"\"\"\n\n  nb_buckets = gates_q.get_shape().as_list()[-1]\n\n  q_dispatcher = expert_utils.SparseDispatcher(nb_buckets, gates_q)\n  k_dispatcher = expert_utils.SparseDispatcher(nb_buckets, gates_k)\n\n  def eventually_dispatch(dispatcher, value):\n    if value is not None:\n      return dispatcher.dispatch(value)\n    return [None] * nb_buckets\n\n  # Iterate over every dispatched group\n  list_v_out = []\n  for (\n      q_i,\n      k_i,\n      v_i,\n      qbc,\n      qbo,\n      kbc,\n      kbo,\n  ) in zip(\n      # Dispatch queries, keys and values\n      q_dispatcher.dispatch(q),\n      k_dispatcher.dispatch(k),\n      k_dispatcher.dispatch(v),\n      # Also dispatch the sequence positions and batch coordinates\n      eventually_dispatch(q_dispatcher, bi.coordinates),\n      eventually_dispatch(q_dispatcher, bi.order),\n      eventually_dispatch(k_dispatcher, bi.coordinates),\n      eventually_dispatch(k_dispatcher, bi.order),\n  ):\n    list_v_out.append(\n        expert_dot_product(\n            q_i,\n            k_i,\n            v_i,\n            info_q=BatchInfo(coordinates=qbc, order=qbo),\n            info_k=BatchInfo(coordinates=kbc, order=kbo)))\n\n  # Combine all buckets together to restore the original length\n  return q_dispatcher.combine(list_v_out)\n\n\ndef map_fn_switch(fn, elems, use_map_fn=True, **kwargs):\n  \"\"\"Construct the graph with either tf.map_fn or a python for loop.\n\n  This function is mainly for for benchmarking purpose.\n\n  tf.map_fn is dynamic but is much slower than creating a static graph with\n  for loop. However, having a for loop make the graph much longer to build\n  and can consume too much RAM on distributed setting.\n\n  Args:\n    fn (fct): same that tf.map_fn but for now can only return a single tensor\n      value (instead of a tuple of tensor for the general case)\n    elems (tuple): same that tf.map_fn\n    use_map_fn (bool): If True, tf.map_fn is used, if False, for _ in _: is used\n      instead\n    **kwargs: Additional tf.map_fn arguments (ignored if use_map_fn is False)\n\n  Returns:\n    tf.Tensor: the output of tf.map_fn\n  \"\"\"\n  if use_map_fn:\n    return tf.map_fn(fn, elems, **kwargs)\n  elems_unpacked = (tf.unstack(e) for e in elems)\n  out_unpacked = [fn(e) for e in zip(*elems_unpacked)]\n  out = tf.stack(out_unpacked)\n  return out\n\n\n@expert_utils.add_name_scope()\ndef sparse_dot_product_attention(q, k, v, bi, use_map_fn, experts_params):\n  \"\"\"Sparse multihead self attention.\n\n  Perform an approximation of the full multihead attention by dispatching\n  the tokens using their keys/values. Thus the attention matrix are only\n  computed each times on a subset of the tokens.\n\n  Notes:\n   * The function don't perform scaling here (multihead_attention does\n  the /sqrt(depth)).\n   * The padding should have been removed (so batch size should be 1 but length\n   contains the elements from all different batches)\n   * Right now, only self attention is supported so length_q and length_kv\n   should be identical and the function will add triangular mask.\n   * If bi.order is not None, The bias is added inside this function to\n   prevent attention to the future.\n\n  Args:\n    q (tf.Tensor): Queries of shape [batch, heads, length_q, depth_k]\n    k (tf.Tensor): Keys of shape [batch, heads, length_q, depth_k]\n    v (tf.Tensor): Values of shape [batch, heads, length_kv, depth_v]\n    bi (BatchInfo): Contains the batch coordinates and sequence order\n    use_map_fn (bool): Use either tf.map_fn of python for loop to compute the\n      heads separately\n    experts_params (dict): Additional params for the local expert\n\n  Returns:\n    tf.Tensor: Approximation of Softmax(Q.K) * V, of shape\n      [batch, heads, length_q, depth_v]\n  \"\"\"\n  batch_size, nb_heads, _, depth = common_layers.shape_list(q)\n\n  @expert_utils.add_name_scope()\n  def flatten_first_dims(x):\n    \"\"\"Reshape such that x is [num_heads, -1, depth].\"\"\"\n    # Case 1: Either constant batch size of size 1 or batch already flattened\n    if x.get_shape().as_list()[0] == 1:\n      return tf.squeeze(x, axis=0)\n\n    # Case 2: Flatten batch dimension\n    x = tf.transpose(x, perm=[1, 0, 2, 3])\n    x = tf.reshape(x, [nb_heads, -1, depth])\n    return x\n\n  def flatten_batch(x):\n    if x is None:\n      return x\n    return expert_utils.flatten_all_but_last(x)\n\n  q = flatten_first_dims(q)\n  k = flatten_first_dims(k)\n  v = flatten_first_dims(v)\n  bi = BatchInfo(\n      coordinates=flatten_batch(bi.coordinates),\n      order=flatten_batch(bi.order),\n  )\n\n  # Unstack heads\n  list_q = tf.unstack(q)  # list[tf.Tensor(shape=[batch * length, depth])]\n  list_k = tf.unstack(k)\n  list_v = tf.unstack(v)\n\n  list_gates_q = []\n  list_gates_k = []\n\n  total_loss = 0.0\n  # There might be a more optimized way to compute all heads at once\n  for single_q, single_k, _ in zip(list_q, list_k, list_v):\n    # Each head get its own dispatcher\n    lhs_gating = LshGating(\n        depth=single_q.get_shape().as_list()[-1], **experts_params)\n\n    list_gates_q.append(lhs_gating.get_gates(single_q))\n    list_gates_k.append(lhs_gating.get_gates(single_k))\n\n  gates_q = tf.stack(list_gates_q)\n  gates_k = tf.stack(list_gates_k)\n\n  # Process each head separately.\n  v_out = map_fn_switch(\n      lambda args: dot_product_single_head(bi=bi, *args),\n      elems=(q, k, v, gates_q, gates_k),\n      dtype=(tf.float32),\n      parallel_iterations=2,\n      use_map_fn=use_map_fn,\n  )\n\n  # Restore original shape as expected by multihead_attention\n  if isinstance(batch_size, int) and batch_size == 1:\n    v_out = tf.expand_dims(v_out, axis=0)  # Restore batch_size = 1\n  else:\n    v_out = tf.reshape(v_out, [nb_heads, batch_size, -1, depth])\n    v_out = tf.transpose(v_out, [1, 0, 2, 3])\n  return v_out, total_loss / nb_heads\n\n\n@expert_utils.add_name_scope()\ndef dot_product_batched_head(q, k, v, gates_q, gates_k, mask_right=False):\n  \"\"\"Perform a dot product attention on a single sequence on a single head.\n\n  This function dispatch the q, k, v and loop over the buckets to compute the\n  attention dot product on each subsequences.\n\n  Args:\n    q (tf.Tensor): [batch*heads, length_q, depth_q]\n    k (tf.Tensor): [batch*heads, length_k, depth_q]\n    v (tf.Tensor): [batch*heads, length_k, depth_v]\n    gates_q (tf.Tensor): One-hot of shape [batch*heads, length_q, nb_buckets]\n    gates_k (tf.Tensor): One-hot of shape [batch*heads, length_k, nb_buckets]\n    mask_right (bool): Add a bias to prevent attention to the future\n\n  Returns:\n    tf.Tensor: [length_q, depth_v]\n  \"\"\"\n  nb_buckets = common_layers.shape_list(gates_q)[-1]\n\n  @expert_utils.add_name_scope()\n  def get_dispatcher(gates):\n    \"\"\"Construct dispatcher for gates.\"\"\"\n    length = common_layers.shape_list(gates)[1]\n    # Count the number of ones per batch (and keep the max value)\n    nb_elems_to_dispatch = tf.reduce_sum(gates, axis=[1, 2])\n    nb_elems_to_dispatch = tf.reduce_max(nb_elems_to_dispatch)\n    nb_elems_to_dispatch = tf.to_int32(nb_elems_to_dispatch)\n    capacity = nb_elems_to_dispatch // nb_buckets * 2  # Capacity is hardcoded\n    capacity = tf.minimum(length, capacity)\n    tf.summary.scalar(\"dispatch_capacity\", capacity, family=\"lsh\")\n    return expert_utils.TruncatingDispatcher(gates, capacity)\n\n  def add_summary_capacity(x, prefix):\n    # Monitor if capacity overflow\n    x = x[0, ...]  # Take first batch/head\n    x = tf.reduce_sum(x, axis=0)\n    tf.summary.scalar(prefix + \"_min\", tf.reduce_min(x), family=\"lsh\")\n    tf.summary.scalar(prefix + \"_max\", tf.reduce_max(x), family=\"lsh\")\n    tf.summary.histogram(prefix + \"capacity_distribution\", x, family=\"lsh\")\n    for i in range(3):  # Show the first 3 buckets\n      tf.summary.scalar(\"{}_{}\".format(prefix, i), x[i], family=\"lsh\")\n\n  add_summary_capacity(gates_q, \"q\")\n  add_summary_capacity(gates_k, \"k\")\n\n  q_dispatcher = get_dispatcher(gates_q)\n  k_dispatcher = get_dispatcher(gates_k)\n\n  q = q_dispatcher.dispatch(q)\n  k = k_dispatcher.dispatch(k)\n  v = k_dispatcher.dispatch(v)\n\n  # Bias of shape [batch*heads, nb_buckets, 1, capacity] broadcasted to every\n  # queries\n  bias = tf.expand_dims((k_dispatcher.nonpadding() - 1.0) * 1e9, 2)\n  if mask_right:\n    q_coordinate = tf.to_float(\n        tf.expand_dims(q_dispatcher.length_coordinate(), 3))\n    k_coordinate = tf.to_float(\n        tf.expand_dims(k_dispatcher.length_coordinate(), 2))\n    bias += tf.to_float(tf.greater(k_coordinate, q_coordinate)) * -1e9\n  # The sequence padding is not masked but is ignored on the next layers\n\n  # q, k, v now have shape [batch*heads, nb_bucket, capacity, depth]\n  # The buckets can be seen as different heads\n  v_out = dot_product_attention(q, k, v, bias=bias)\n\n  # Combine all buckets together to restore the original length\n  return q_dispatcher.combine(v_out)\n\n\n@expert_utils.add_name_scope()\ndef sparse_dot_product_attention_truncated(\n    q,\n    k,\n    v,\n    bi,  # Unused\n    experts_params,\n    use_map_fn=False,  # Unused\n    mask_right=False,\n):  # pylint: disable=unused-argument\n  \"\"\"Sparse multihead self attention.\n\n  Perform an approximation of the full multihead attention by dispatching\n  the tokens using their keys/values. Thus the attention matrix are only\n  computed each times on a subset of the tokens.\n\n  Notes:\n   * The function don't perform scaling here (multihead_attention does\n  the /sqrt(depth)).\n   * The padding should have been removed (so batch size should be 1 but length\n   contains the elements from all different batches)\n   * Right now, only self attention is supported so length_q and length_kv\n   should be identical and the function will add triangular mask.\n   * If bi.order is not None, The bias is added inside this function to\n   prevent attention to the future.\n\n  Args:\n    q (tf.Tensor): Queries of shape [batch, heads, length_q, depth_k]\n    k (tf.Tensor): Keys of shape [batch, heads, length_q, depth_k]\n    v (tf.Tensor): Values of shape [batch, heads, length_kv, depth_v]\n    bi (BatchInfo): Contains the batch coordinates and sequence order\n    experts_params (dict): Additional params for the local expert\n    use_map_fn (bool): Use either tf.map_fn of python for loop to compute the\n      heads separately\n    mask_right (bool):\n  Returns:\n    tf.Tensor: Approximation of Softmax(Q.K) * V, of shape\n      [batch, heads, length_q, depth_v]\n  \"\"\"\n  # Currently depth is the same for for q and v\n  batch_size, nb_heads, _, depth = common_layers.shape_list(q)\n\n  total_loss = 0.0\n\n  # Each head get its own dispatcher\n  list_lsh = [LshGating(depth=depth, **experts_params) for _ in range(nb_heads)]\n\n  @expert_utils.add_name_scope()\n  def get_gates_head(x, add_first=False):\n    \"\"\"Return the gates for each heads of the current x.\n\n    Args:\n      x (tf.Tensor): of shape [batch, heads, length, depth]\n      add_first (bool): if True, add the first element on each bucket\n\n    Returns:\n      tf.Tensor: gates of shape [batch, heads, length, num_buckets]\n    \"\"\"\n    length = common_layers.shape_list(x)[2]\n\n    # Invert heads/batch\n    x = tf.transpose(x, perm=[1, 0, 2, 3])\n    x = tf.reshape(x, [nb_heads, batch_size * length, depth])\n\n    list_x = tf.unstack(x)  # list[tf.Tensor(shape=[batch * length, depth])]\n\n    # Unstack heads\n    list_gates = []\n    # There might be a more optimized way to compute all heads at once\n    for lsh, single_x in zip(list_lsh, list_x):\n      # Each head get its own dispatcher\n      gates = lsh.get_gates(single_x)\n      nb_buckets = gates.get_shape().as_list()[-1]\n      # Reshape to [batch, length, depth] but should consider sequence\n      # padding in that case (also dispatch the padding)\n      gates = tf.reshape(gates, [batch_size, length, nb_buckets])\n      list_gates.append(gates)\n\n    gates = tf.stack(list_gates)\n\n    # Restore original shape\n    gates = tf.reshape(gates, [nb_heads, batch_size, length, nb_buckets])\n    gates = tf.transpose(gates, [1, 0, 2, 3])\n\n    # Dispatch the first element to every gates to avoid empty buckets\n    if add_first:\n      gates = tf.maximum(gates,\n                         tf.reshape(tf.one_hot([0], length), [1, 1, length, 1]))\n\n    return gates\n\n  gates_q = get_gates_head(q)\n  gates_k = get_gates_head(k, add_first=True)\n\n  # [batch, heads, length, depth] => [batch*heads, length, depth]\n  q, k, v, gates_q, gates_k = [\n      combine_first_two_dimensions(t) for t in (q, k, v, gates_q, gates_k)\n  ]\n\n  v_out = dot_product_batched_head(q, k, v, gates_q, gates_k, mask_right)\n\n  # Restore original dimension\n  v_out = tf.reshape(v_out, [batch_size, nb_heads, -1, depth])\n\n  return v_out, total_loss / nb_heads\n\n\n@expert_utils.add_var_scope()\ndef deconv_elems_1d(x, factor, out_depth=None):\n  \"\"\"Increase the length and change the dimensionality.\n\n  Expand/project each positions of dim depth of the input into\n  factor*tokens of dim out_depth\n\n  Args:\n    x (tf.Tensor): shape [batch_size, length, depth]\n    factor (int): Multiplicative factor of each tokens.\n    out_depth (int): Output depth (if None, keep depth constant)\n\n  Returns:\n    tf.Tensor: shape [batch_size, length*factor, out_depth]\n  \"\"\"\n  out_depth = out_depth or x.get_shape().as_list()[-1]\n  x = tf.expand_dims(x, 1)  # [batch_size, 1, length, depth]\n  x = layers().Conv2DTranspose(\n      filters=out_depth,\n      kernel_size=(1, factor),\n      strides=(1, factor),\n      padding=\"valid\",\n      data_format=\"channels_last\",\n  )(x)  # [batch_size, 1, length*factor, out_depth]\n  x = tf.squeeze(x, 1)  # [batch_size, length*factor, depth]\n  return x\n\n\n@expert_utils.add_var_scope()\ndef conv_elems_1d(x, factor, out_depth=None):\n  \"\"\"Decrease the length and change the dimensionality.\n\n  Merge/restore/compress factors positions of dim depth of the input into\n  a single position of dim out_depth.\n  This is basically just a strided convolution without overlap\n  between each strides. The original length has to be divided by factor.\n\n  Args:\n    x (tf.Tensor): shape [batch_size, length, depth]\n    factor (int): Length compression factor.\n    out_depth (int): Output depth\n\n  Returns:\n    tf.Tensor: shape [batch_size, length//factor, out_depth]\n  \"\"\"\n  out_depth = out_depth or x.get_shape().as_list()[-1]\n  # with tf.control_dependencies(  # Dynamic assertion\n  #     [tf.assert_equal(tf.shape(x)[1] % factor, 0)]):\n  x = tf.expand_dims(x, 1)  # [batch_size, 1, length, depth]\n  x = layers().Conv2D(\n      filters=out_depth,\n      kernel_size=(1, factor),\n      strides=(1, factor),\n      padding=\"valid\",\n      data_format=\"channels_last\",\n  )(x)  # [batch_size, 1, length//factor, out_depth]\n  x = tf.squeeze(x, 1)  # [batch_size, length//factor, depth]\n  return x\n\n\n@expert_utils.add_var_scope()\ndef local_reduction_attention(x, block_length, multihead_params):\n  \"\"\"Reduce the length dimension using self attention.\n\n  Args:\n    x (tf.Tensor): float32 of shape [batch, length, depth]\n    block_length (int): Block length for local attention (Compression factor)\n    multihead_params (dict): parameters for multihead attention\n\n  Returns:\n    tf.Tensor: Compressed tensor of shape [batch, length // factor, depth]\n  \"\"\"\n\n  @expert_utils.add_name_scope()\n  def dot_product_self_local_attention_flattened(q, k, v):\n    \"\"\"Strided block local self-attention.\n\n    No overlap between the blocks.\n\n    Args:\n      q (tf.Tensor): shape [batch, heads, length, depth_k]\n      k (tf.Tensor): shape [batch, heads, length, depth_k]\n      v (tf.Tensor): shape [batch, heads, length, depth_v]\n\n    Returns:\n      tf.Tensor: shape [batch, heads, length, depth_v]\n    \"\"\"\n    _, num_head, _, depth = q.get_shape().as_list()\n\n    # Extract the blocks\n    def pad_and_reshape(x):\n      \"\"\"Split the length dim into [num_block, block_length].\"\"\"\n      length_x = common_layers.shape_list(x)[2]\n      # Add some padding, but won't matter as the last block will never be\n      # attended by the query (after compression)\n      x = tf.pad(x, [[0, 0], [0, 0], [0, -length_x % block_length], [0, 0]])\n      x = tf.reshape(\n          x,\n          [\n              common_layers.shape_list(x)[0],  # Batch\n              num_head,  # Head\n              common_layers.shape_list(x)[2] // block_length,  # Num blocks\n              block_length,  # Block length\n              depth,  # Depth\n          ])\n      return x\n\n    q, k, v = [pad_and_reshape(t) for t in (q, k, v)]\n\n    # Perform attention on the flattened dot product\n    logits = tf.matmul(q, k, transpose_b=True)\n    logits = tf.reshape(\n        logits,\n        [\n            common_layers.shape_list(logits)[0],  # Batch\n            num_head,  # Head\n            common_layers.shape_list(logits)[2],  # Num blocks\n            block_length**2,  # Flatten last dimension\n        ])\n    weights = tf.nn.softmax(logits)\n    weights = tf.reshape(\n        weights,\n        [\n            common_layers.shape_list(weights)[0],  # Batch\n            num_head,  # Head\n            common_layers.shape_list(weights)[2],  # Num blocks\n            block_length,\n            block_length,  # Restore the block length dimension\n        ])\n    weights = tf.reduce_sum(weights, axis=3, keep_dims=True)  # Compress block\n    v_out = tf.matmul(weights, v)  # [1, block_length] @ [block_length, depth]\n    v_out = tf.squeeze(v_out, axis=3)\n    return v_out\n\n  return multihead_attention(\n      x,\n      None,\n      bias=None,\n      output_depth=x.get_shape().as_list()[-1],\n      attention_type=dot_product_self_local_attention_flattened,\n      **multihead_params)\n\n\n@expert_utils.add_var_scope()\ndef multihead_self_attention_reduced(\n    x,\n    memory_antecedent=None,\n    bias=None,\n    factor=None,\n    multihead_params=None,\n    nonlinearity=\"none\",\n    reduction_type=\"conv\",\n    add_mask=True,\n):\n  \"\"\"Reduce the length dimension by compressing with conv.\n\n  Args:\n    x (tf.Tensor): float32 of shape [batch, length, depth]\n    memory_antecedent (tf.Tensor): Unsupported for now\n    bias (tf.Tensor): Ignored\n    factor (int): compression factor for the memory sequence\n    multihead_params (dict): parameters for multihead attention\n    nonlinearity (str): Add some non-linearity after the memory block\n    reduction_type (str): type of compression\n    add_mask (bool): If True, add the bias to prevent attention to the future\n\n  Returns:\n    (tf.Tensor): float32 of shape [batch, length, depth]\n\n  Raises:\n    ValueError: If reduction_type or nonlinearity is invalid\n  \"\"\"\n  if not factor or not multihead_params:\n    raise ValueError(\"factor and multihead_params should be set\")\n  if memory_antecedent is not None:\n    raise NotImplementedError(\n        \"multihead_self_attention_reduced only works with self-attention\")\n\n  depth = x.get_shape().as_list()[-1]\n\n  # Could try to have some overlap between the blocks but that would\n  # create conv artifacts, would make it difficult to not attend to the future\n  # within one group and the padding should be handled specially.\n\n  # Reduce the memory dimension\n  if reduction_type == \"attention\":\n    memory_x = local_reduction_attention(x, factor, multihead_params)\n  elif reduction_type == \"conv\":\n    # With valid padding, the last block won't be computed (not attended anyway)\n    memory_x = conv_elems_1d(x, factor)\n  else:\n    raise ValueError(\"Unknown reduction type {}\".format(reduction_type))\n\n  if nonlinearity == \"silu\":\n    memory_x *= tf.nn.sigmoid(memory_x)\n  elif nonlinearity != \"none\":\n    raise ValueError(\"Unknown non linearity {}\".format(nonlinearity))\n\n  memory_x = tf.concat(\n      # Add the first elem to make it attendable by everyone (otherwise the\n      # first block cannot attend to anything)\n      [x[:, :1, :], memory_x],\n      axis=1,\n  )\n\n  # Construct the bias\n  @expert_utils.add_name_scope()\n  def construct_bias_vectors(t, axis):\n    length = tf.to_float(common_layers.shape_list(t)[1])\n    length_coordinates = tf.range(length, dtype=tf.float32)\n    length_coordinates = tf.expand_dims(length_coordinates, axis=axis)\n    # [1, length_k] or [length_q, 1]\n    return length_coordinates\n\n  if add_mask:  # Create mask to prevent attention to the future\n    bias = tf.to_float(\n        tf.greater(\n            # Because we add the first elem to the memory block and it can be\n            # attended by anyone,we don't need to add +1 anymore to prevent self\n            # attention Use * factor to make sure the last tokens  of a block\n            # cannot attend the block\n            construct_bias_vectors(memory_x, 0) * factor,\n            # +epsilon to avoid float equality\n            construct_bias_vectors(x, 1) + 1e-3,\n        )) * -1e9\n    bias = tf.expand_dims(bias, axis=0)\n    bias = tf.expand_dims(bias, axis=0)  # [1, 1, length_k, length_q]\n  else:\n    bias = None\n\n  return multihead_attention(\n      query_antecedent=x,\n      memory_antecedent=memory_x,\n      bias=bias,\n      output_depth=depth,\n      **multihead_params)\n\n\ndef scaled_dot_product_attention_simple(q, k, v, bias, name=None):\n  \"\"\"Scaled dot-product attention. One head. One spatial dimension.\n\n  Args:\n    q: a Tensor with shape [batch, length_q, depth_k]\n    k: a Tensor with shape [batch, length_kv, depth_k]\n    v: a Tensor with shape [batch, length_kv, depth_v]\n    bias: optional Tensor broadcastable to [batch, length_q, length_kv]\n    name: an optional string\n\n  Returns:\n    A Tensor.\n  \"\"\"\n  with tf.variable_scope(\n      name, default_name=\"scaled_dot_product_attention_simple\"):\n    scalar = tf.rsqrt(tf.to_float(common_layers.shape_list(q)[2]))\n    logits = tf.matmul(q * scalar, k, transpose_b=True)\n    if bias is not None:\n      logits += bias\n    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n    if common_layers.should_generate_summaries():\n      tf.summary.image(\n          \"attention\", tf.expand_dims(tf.pow(weights, 0.2), 3), max_outputs=1)\n    return tf.matmul(weights, v)\n\n\n_function_cache = {}\n\n\ndef multihead_self_attention_memory_efficient(x,\n                                              bias,\n                                              num_heads,\n                                              head_size=None,\n                                              epsilon=1e-6,\n                                              forget=True,\n                                              test_vars=None,\n                                              name=None):\n  \"\"\"Multihead scaled-dot-product self-attention.\n\n  Includes layer norm.\n\n  Returns multihead-self-attention(layer_norm(x))\n\n  Computes one attention head at a time to avoid exhausting memory.\n\n  If forget=True, then forget all forwards activations and recompute on\n  the backwards pass.\n\n  Args:\n    x: a Tensor with shape [batch, length, input_size]\n    bias: an attention bias tensor broadcastable to [batch, 1, length, length]\n    num_heads: an integer\n    head_size: an optional integer - defaults to input_size/num_heads\n    epsilon: a float, for layer norm\n    forget: a boolean - forget forwards activations and recompute on backprop\n    test_vars: optional tuple of variables for testing purposes\n    name: an optional string\n\n  Returns:\n    A Tensor.\n  \"\"\"\n  io_size = x.get_shape().as_list()[-1]\n  if head_size is None:\n    assert io_size % num_heads == 0\n    head_size = io_size / num_heads\n\n  def forward_internal(x, wqkv, wo, attention_bias, norm_scale, norm_bias):\n    \"\"\"Forward function.\"\"\"\n    n = common_layers.layer_norm_compute(x, epsilon, norm_scale, norm_bias)\n    wqkv_split = tf.unstack(wqkv, num=num_heads)\n    wo_split = tf.unstack(wo, num=num_heads)\n    y = 0\n    for h in range(num_heads):\n      with tf.control_dependencies([y] if h > 0 else []):\n        combined = tf.nn.conv1d(n, wqkv_split[h], 1, \"SAME\")\n        q, k, v = tf.split(combined, 3, axis=2)\n        o = scaled_dot_product_attention_simple(q, k, v, attention_bias)\n        y += tf.nn.conv1d(o, wo_split[h], 1, \"SAME\")\n    return y\n\n  key = (\n      \"multihead_self_attention_memory_efficient %s %s\" % (num_heads, epsilon))\n  if not forget:\n    forward_fn = forward_internal\n  elif key in _function_cache:\n    forward_fn = _function_cache[key]\n  else:\n\n    @function.Defun(compiled=True)\n    def grad_fn(x, wqkv, wo, attention_bias, norm_scale, norm_bias, dy):\n      \"\"\"Custom gradient function.\"\"\"\n      with tf.control_dependencies([dy]):\n        n = common_layers.layer_norm_compute(x, epsilon, norm_scale, norm_bias)\n        wqkv_split = tf.unstack(wqkv, num=num_heads)\n        wo_split = tf.unstack(wo, num=num_heads)\n        deps = []\n        dwqkvs = []\n        dwos = []\n        dn = 0\n        for h in range(num_heads):\n          with tf.control_dependencies(deps):\n            combined = tf.nn.conv1d(n, wqkv_split[h], 1, \"SAME\")\n            q, k, v = tf.split(combined, 3, axis=2)\n            o = scaled_dot_product_attention_simple(q, k, v, attention_bias)\n            partial_y = tf.nn.conv1d(o, wo_split[h], 1, \"SAME\")\n            pdn, dwqkvh, dwoh = tf.gradients(\n                ys=[partial_y],\n                xs=[n, wqkv_split[h], wo_split[h]],\n                grad_ys=[dy])\n            dn += pdn\n            dwqkvs.append(dwqkvh)\n            dwos.append(dwoh)\n            deps = [dn, dwqkvh, dwoh]\n        dwqkv = tf.stack(dwqkvs)\n        dwo = tf.stack(dwos)\n        with tf.control_dependencies(deps):\n          dx, dnorm_scale, dnorm_bias = tf.gradients(\n              ys=[n], xs=[x, norm_scale, norm_bias], grad_ys=[dn])\n        return (dx, dwqkv, dwo, tf.zeros_like(attention_bias), dnorm_scale,\n                dnorm_bias)\n\n    @function.Defun(\n        grad_func=grad_fn, compiled=True, separate_compiled_gradients=True)\n    def forward_fn(x, wqkv, wo, attention_bias, norm_scale, norm_bias):\n      return forward_internal(x, wqkv, wo, attention_bias, norm_scale,\n                              norm_bias)\n\n    _function_cache[key] = forward_fn\n\n  if bias is not None:\n    bias = tf.squeeze(bias, 1)\n  with tf.variable_scope(name, default_name=\"multihead_attention\", values=[x]):\n    # TODO(noam): it would be nice to save memory by casting x to float16\n    # here, but this causes problems with the gradients.  Figure out if there\n    # is a way to leave the gradients as float32.\n    if test_vars is not None:\n      wqkv, wo, norm_scale, norm_bias = list(test_vars)\n    else:\n      wqkv = tf.get_variable(\n          \"wqkv\", [num_heads, 1, io_size, 3 * head_size],\n          initializer=tf.random_normal_initializer(stddev=io_size**-0.5))\n      wo = tf.get_variable(\n          \"wo\", [num_heads, 1, head_size, io_size],\n          initializer=tf.random_normal_initializer(\n              stddev=(head_size * num_heads)**-0.5))\n      norm_scale, norm_bias = common_layers.layer_norm_vars(io_size)\n    y = forward_fn(x, wqkv, wo, bias, norm_scale, norm_bias)\n    y.set_shape(x.get_shape())\n    return y\n\n\nmultihead_attention_sparse_dot_prod = functools.partial(\n    multihead_attention, attention_type=sparse_dot_product_attention)\n\nmultihead_attention_sparse_truncated = functools.partial(\n    multihead_attention, attention_type=sparse_dot_product_attention_truncated)\n"
  }
}